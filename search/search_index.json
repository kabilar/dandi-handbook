{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the DANDI Archive Documentation","text":"<p>The Web interface to the DANDI archive is located at https://dandiarchive.org. This documentation explains how to interact with the archive.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<p>If you want to know more about the DANDI project, its goals, and the problems it tries to solve, check out the Introduction.</p> <p>To start using the archive, head over to Using DANDI in the User Guide section.</p> <p>If are a developer and want to know how the project is organized, check out the Project Structure page in the Developer Guide section.</p>"},{"location":"#where-to-get-help","title":"Where to Get Help","text":"<p>You can communicate with the DANDI team in a variety of ways, depending on your needs:</p> <ul> <li>You can ask questions, report bugs, or  request features at our helpdesk.</li> <li>For interacting with the global neuroscience community, post on https://neurostars.org and use the tag dandi.</li> <li>You can use the DANDI Slack workspace, which we will invite you to after approving your registration on    DANDI using GitHub (this registration is required to upload data or to use the DANDI    JupyterHub). See here for details on how to    register.</li> <li>Email us: info@dandiarchive.org</li> </ul>"},{"location":"#contributing-and-feedback","title":"Contributing and Feedback","text":"<p>We are looking for people to give us feedback on this documentation. If anything is unclear, open an issue on our repository. You can also get in touch on  our Slack channel, which is available to those who have registered an account on the archive.</p> <p>If you want to get started right away and contribute directly to this documentation, see the About This Documentation section.</p>"},{"location":"#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"01_introduction/","title":"Introduction","text":""},{"location":"01_introduction/#what-is-dandi","title":"What is DANDI?","text":"<p>DANDI is:</p> <ul> <li>An open data archive to submit neurophysiology data for electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments.</li> <li>A persistent, versioned, and growing collection of standardized datasets.</li> <li>A place to house data to collaborate across research sites.</li> <li>Supported by the BRAIN Initiative and the AWS Public dataset programs.</li> </ul> <p>DANDI provides significant benefits:</p> <ul> <li>A FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized neurophysiology and associated data.</li> <li>Rich metadata to support search across data.</li> <li>Consistent and transparent data standards to simplify data reuse and software development. We use the Neurodata Without Borders,  Brain Imaging Data Structure, Neuroimaging Data Model (NIDM), and other BRAIN Initiative standards to organize and search the data. See Data Standards for more information.</li> <li>The data can be accessed programmatically allowing for software to work directly with data in the cloud.</li> <li>The infrastructure is built on a software stack of open source products, thus enriching the ecosystem.</li> </ul>"},{"location":"01_introduction/#properties-of-dandi","title":"Properties of DANDI","text":"<p>Data identifiers: The archive provides persistent identifiers for versioned datasets and assets, thus improving reproducibility of neurophysiology research.</p> <p>Data storage: Cloud-based platform on AWS. Data are available from a public S3 bucket. Data from embargoed datasets are available from a private bucket to owners only.</p> <p>Type of data The archive accepts cellular neurophysiology data including electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments and other associated data (e.g. participant information, MRI or other modalities).</p> <p>Accepted Standards and Data File Formats: NWB (HDF5), BIDS (NIfTI, JSON, PNG, TIF, OME.TIF, OME.BTF, OME.ZARR)  (see Data Standards for more details)</p>"},{"location":"01_introduction/#neurophysiology-informatics-challenges-and-dandi-solutions","title":"Neurophysiology Informatics Challenges and DANDI Solutions","text":"Challenges Solutions Most raw data stays in laboratories. DANDI provides a public archive for dissemination of raw and derived data. Non-standardized datasets lead to significant resource needs to understand and adapt code to these datasets. DANDI standardizes all data using NWB and BIDS standards. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. The DANDI ecosystem provides tools for converting data from different instruments into NWB and BIDS. There are many domain general places to house data (e.g. Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant scientific metadata. DANDI is focused on neurophysiology data and related metadata. Datasets are growing larger, requiring compute services to be closer to data. DANDI provides Dandihub, a JupyterHub instance close to the data. Neurotechnology is evolving and requires changes to metadata and data storage. DANDI works with community members to improve data standards and formats. Consolidating and creating robust algorithms (e.g. spike sorting) requires varied data sources. DANDI provides access to many different datasets."},{"location":"100_about_this_doc/","title":"About This Documentation","text":"<p>This documentation is a work in progress and we welcome all input: if something is missing or unclear, let us know by opening an issue on our helpdesk.</p>"},{"location":"100_about_this_doc/#serving-the-docs-locally","title":"Serving the Docs Locally","text":"<p>This project uses the MkDocs tool with the Material theme and extra plugins to generate the website.</p> <p>To test locally, you will need to install the Python dependencies. To do that, type the following commands:</p> <pre><code>git clone https://github.com/dandi/handbook.git\ncd handbook\npip install -r requirements.txt\n</code></pre> <p>If you are working on your fork, simply replace <code>https://github.com/dandi/handbook.git</code> with <code>git clone git@github.com/&lt;username&gt;/handbook.git</code> , where <code>&lt;username&gt;</code> is your GitHub username.</p> <p>Once done, you need to run MkDocs. Simply type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Finally, open <code>http://127.0.0.1:8000/</code> in your browser, and you should see the default home page of the documentation being displayed.</p>"},{"location":"10_using_dandi/","title":"Using DANDI","text":"<p>DANDI allows you to work with stored neurophysiology data in multiple ways. You can search, view, and download files, all without registering for a DANDI account. As a registered user, you can also create these collections of data  along with metadata and publish them to the DANDI platform. </p>"},{"location":"10_using_dandi/#dandisets","title":"Dandisets","text":"<p>DANDI stores cellular neurophysiology data in Dandisets.</p> <p>A Dandiset is a collection of assets (files and their metadata) and metadata about the collection.</p> <ul> <li>A Dandiset is organized in a structured manner to help users and software tools interact with it.</li> <li>Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g. https://identifiers.org/DANDI:000004). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset.</li> </ul>"},{"location":"10_using_dandi/#quick-start","title":"Quick Start","text":"<p>If you are new to DANDI, all you need is an Internet connection to use the DANDI Web application to view  and download files from a  public  Dandiset.  Registration is not required.</p> <p>To view a specific public Dandiset and download one of its files:</p> <ol> <li> <p>At the top of the DANDI Web application, click PUBLIC DANDISETS to see all Dandisets currently available in the     archive. You can sort them by name, identifier, or date of modification.</p> </li> <li> <p>Search for a specific Dandiset by contributor name, modality, or species.</p> </li> <li> <p>Click a Dandiset to open its landing page and view important information such as contact information,     description, license, access information and keywords, and simple statistics.</p> </li> <li> <p>From the right side of the Dandiset landing page, click FILES to see a list of all folders and files for that     Dandiset. Click the download icon  to download a     specific file.  Note: To download an entire Dandiset, you will need to follow the instructions in the     Download section to install and use the DANDI Python client tool.</p> </li> </ol>"},{"location":"10_using_dandi/#next-steps","title":"Next steps","text":"<p>Although anyone on the Internet  can view and download public Dandisets, registered users can also create Dandisets, upload data, and publish  the Dandiset to generate a DOI for it. See the sections that follow for more detailed information about the DANDI project, as well as instructions on how  to work with public Dandisets or to create and publish you own as a registered user. </p>"},{"location":"10_using_dandi/#dandiset-actions","title":"Dandiset Actions","text":"<p>The DANDI project contains the DANDI Web application, the DANDI Python client tool, and the DANDI JupyterHub  instance. These tools can be used to perform actions on Dandisets. </p> <p></p> <p>You can learn more about the Dandiset actions in separate sections:</p> <ul> <li>View</li> <li>Download</li> <li>Upload</li> <li>Publish</li> </ul>"},{"location":"10_using_dandi/#tools-to-interact-with-dandi","title":"Tools to interact with DANDI","text":""},{"location":"10_using_dandi/#dandi-web-application","title":"DANDI Web application","text":"<p>The DANDI Web application allows you to:</p> <ul> <li>Search across all public Dandisets</li> <li>Download data from public Dandisets</li> <li>Create a new Dandiset and provide metadata</li> <li>Publish your Dandiset</li> </ul>"},{"location":"10_using_dandi/#dandi-python-client","title":"DANDI Python client","text":"<p>The DANDI Python client allows you to:</p> <ul> <li>Download Dandisets and individual subject folders or files</li> <li>Organize your data locally before upload</li> <li>Upload Dandisets</li> </ul> <p>Before you can use the DANDI Python client, you have to install the package with <code>pip install dandi</code> in a Python 3.8+ environment.</p> <p>You should check the Dandi Debugging section in case of any problems.</p>"},{"location":"10_using_dandi/#dandihub-analysis-platform","title":"Dandihub analysis platform","text":"<p>Dandihub provides a JupyterHub instance in the cloud to interact with the data stored in DANDI.</p> <p>To use the hub, you will need to register for an account using the DANDI Web application.  Note that <code>Dandihub</code> is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"10_using_dandi/#technical-limitations","title":"Technical limitations","text":"<ul> <li>File name/path: There is a limit of 512 characters for the full path length within a dandiset.</li> <li>Volume and size: There is a limit of 5TB per file. We currently   accept any size of standardized datasets, as long as you can upload them over   an HTTPS connection. However, we ask you contact us if you plan to upload more than 10TB of data.</li> </ul>"},{"location":"10_using_dandi/#citing-dandi","title":"Citing DANDI","text":"<p>You can add the following statement to the methods section of your manuscript.</p> <p>Data and associated metadata were uploaded to the DANDI archive [RRID:SCR_017571] using    the Python command line tool (https://doi.org/10.5281/zenodo.3692138). The data were first    converted into the NWB format (https://doi.org/10.1101/2021.03.13.435173) and  organized    into a BIDS-like (https://doi.org/10.1038/sdata.2016.44) structure.</p> <p>You can refer to DANDI using any of the following options:</p> <ul> <li> <p>Using an RRID RRID:SCR_017571. </p> </li> <li> <p>Using the DANDI CLI reference: https://doi.org/10.5281/zenodo.3692138</p> </li> </ul>"},{"location":"11_view/","title":"Viewing Dandisets","text":""},{"location":"11_view/#browse-dandisets","title":"Browse Dandisets","text":"<p>When you go to the DANDI Web application, you can click on <code>PUBLIC DANDISET</code> to access all Dandisets currently available  in the archive, and you can sort them by name, identifier, size, or date of modification.</p> <p></p>"},{"location":"11_view/#search-dandisets","title":"Search Dandisets","text":"<p>In addition, you can search across the Dandisets for any text part of the Dandiset metadata record.  The text may be about contributor names, modalities, or species.  For example,  <code>\"house mouse\"</code> will return a subset of all Dandisets, while <code>\"mouse house\"</code> will likely not return any. When unquoted each word is used as an <code>OR</code>.</p> <p></p> <p>When you click on one of the Dandisets, you can see that the searching phrase can appear in the description, keywords, or in the assets summary.</p> <p></p>"},{"location":"11_view/#dandisets-metadata","title":"Dandisets Metadata","text":"<p>The landing page of each Dandiset contains important information including  metadata provided by the owners such as contact information, description, license, access information and keywords,   simple statistics for a Dandiset such as size of the Dandiset and number of files, or  a summary of the Dandiset including information about species, techniques, and standards.</p> <p></p> <p>If you scroll down, you will also find: - Assets Summary - Funding Information - Related Resources</p> <p>While most of the metadata is summarized on the landing page, some additional information can be  found by clicking <code>Metadata</code> on the right-side panel. For Dandiset owners, this button also allows  adding relevant metadata to populate the landing page.</p> <p></p>"},{"location":"11_view/#file-view","title":"File View","text":"<p>The right side panel allows you also to access a file browser to navigate the list of folders and files in a Dandiset.</p> <p></p> <p>Any file in the Dandiset has a download icon You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. In addition, there is an info icon that leads to full asset metadata. Some files also have a link to external  services that can open the file. Note: that these services often have size limits and hence are activated only for appropriately sized files.</p>"},{"location":"11_view/#my-dandisets","title":"My Dandisets","text":"<p>If you log in as a registered user, you will also see <code>My Dandisets</code> tab:</p> <p></p> <p>By clicking the tab, you can access all the Dandisets you own. For these Dandisets, you can edit and update  metadata through the Dandiset actions section, and add or remove other owners or data.</p>"},{"location":"12_download/","title":"Downloading Data and Dandisets","text":"<p>You can download the content of a Dandiset using the DANDI Web application (such a specific file) or entire  Dandisets using the DANDI Python CLI.</p>"},{"location":"12_download/#using-the-dandi-web-application","title":"Using the DANDI Web Application","text":"<p>Once you have the Dandiset you are interested in (see more in the Dandiset View section), you can download the content of the Dandiset. On the landing page of each Dandiset, you can find <code>Download</code> button on the right-hand panel. After clicking the  button, you will see the specific command you can use with DANDI Python CLI (as well as the information on how to download the CLI).</p> <p></p>"},{"location":"12_download/#download-specific-files","title":"Download specific files","text":"<p>The right-side panel of the Dandiset landing page allows you also to access the list of folders and files.</p> <p></p> <p>Each file in the Dandiset has a download icon next to it, clicking the icon will start the download process.</p>"},{"location":"12_download/#using-the-python-cli-client","title":"Using the Python CLI Client","text":"<p>The DANDI Python client gives you more options, such as downloading entire  Dandisets.</p> <p>Before You Begin: You need to have Python 3.8+ and install the DANDI Python Client using <code>pip install dandi</code>. If you have an issue using the Python CLI, see the Dandi Debugging section.</p>"},{"location":"12_download/#download-a-dandiset","title":"Download a Dandiset","text":"<p>To download an entire Dandiset, you can use the same command as suggested by DANDI web application, e.g.: </p> <p><code>dandi download DANDI:000023</code></p>"},{"location":"12_download/#download-data-for-a-specific-subject-from-a-dandiset","title":"Download data for a specific subject from a Dandiset","text":"<p>You can download data for specific subjects.  Names of the subjects can be found on DANDI web application or by running a command with the DANDI CLI: <code>dandi ls -r  DANDI:000023</code>. Once you have the subject ID, you can download the data, e.g.:</p> <p><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/_draft_/assets/?path=sub-811677083</code></p> <p>You should replace <code>_draft_</code> with a specific version you are interested in (e.g. <code>0.210914.1900</code> in the case of this Dandiset).</p> <p>You can also use the link from DANDI web application, e.g.:</p> <p><code>dandi download https://dandiarchive.org/dandiset/000023/0.210914.1900/files?location=sub-541516760%2F</code></p>"},{"location":"12_download/#download-a-specific-file-from-a-dandiset","title":"Download a specific file from a Dandiset","text":"<p>You can download a specific file from a Dandiset when the link for the specific file can be found on the DANDI web  application, e.g.:</p> <p><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/0.210914.1900/assets/1a93dc97-327d-4f9c-992d-c2149e7810ae/download/</code></p> <p>Hint: <code>dandi download</code> supports a number of resource identifiers to point to a Dandiset, folder, or file.  Providing  an incorrect URL (e.g. <code>dandi download wrongurl</code>) will provide a list of supported identifiers.</p>"},{"location":"135_validation/","title":"Validation Levels for NWB Files","text":"<p>To be accepted by DANDI, NWB files must conform to criteria that are enforced via three levels of validation:</p>"},{"location":"135_validation/#nwb-file-validation","title":"NWB File Validation","text":"<p>PyNWB validation is used to validate the NWB files,  ensuring that they meet the specifications of core NWB and of any NWB extensions that were used. Generally  speaking, all files produced by PyNWB and MatNWB should pass validation, however there are occasional bugs. More  often, NWB files that fail to meet these criteria have been created outside PyNWB and MatNWB.</p>"},{"location":"135_validation/#critical-nwb-checks","title":"Critical NWB Checks","text":"<p>The NWB Inspector scans NWB files using heuristics to find mistakes  or areas for improvements in NWB files. There are three levels of importance for checks: CRITICAL, BEST PRACTICE  VIOLATIONS, and BEST PRACTICE SUGGESTIONS. CRITICAL warnings indicate some internal inconsistency in the data of the  NWB files. The NWB Inspector will print out all warnings, but only CRITICAL warnings will prevent a file from being  uploaded to DANDI. Errors in NWB Inspector will be block upload as well, but reflect a problem with the NWB  Inspector software as opposed to the NWB file. </p>"},{"location":"135_validation/#missing-dandi-metadata","title":"Missing DANDI Metadata","text":"<p>DANDI has requirements for metadata beyond what is strictly required for NWB validation. The following metadata must  be present in the NWB file for a successful upload to DANDI: - You must define a <code>Subject</code> object. - The <code>Subject</code> object must have a <code>subject_id</code> attribute. - The <code>Subject</code> object must have a <code>species</code> attribute. This can either be the Latin binomial, e.g. \"Mus musculus\", or    an NCBI taxonomic identifier. - The <code>Subject</code> object must have a <code>sex</code> attribute. It must be \"M\", \"F\", \"O\" (other), or \"U\" (unknown). - The <code>Subject</code> object must have either <code>date_of_birth</code> or <code>age</code> attribute. It must be in ISO 8601 format, e.g. \"P70D\"    for 70 days, or, if it is a range, must be \"[lower]/[upper]\", e.g. \"P10W/P12W\", which means \"between 10 and 12 weeks\"</p> <p>These requirements are specified in the  DANDI configuration file of NWB Inspector.</p> <p>Passing all of these levels of validation can sometimes be tricky. If you have any questions, please ask them via the  DANDI Help Desk and we would be happy to assist you.</p>"},{"location":"13_upload/","title":"Creating Dandisets and Uploading Data","text":"<p>To create a new Dandiset and upload your data, you need to have a DANDI account.</p>"},{"location":"13_upload/#create-a-dandiset-and-add-data","title":"Create a Dandiset and Add Data","text":"<p>You can create a new Dandiset at https://dandiarchive.org. This Dandiset can be fully  public or embargoed  according to NIH policy. When you create a Dandiset, a permanent ID is automatically assigned to it. To prevent the production server from being inundated with test Dandisets, we encourage developers to develop  against the development server (https://gui-staging.dandiarchive.org/). Note  that the development server should not be used to stage your data. All data are uploaded as draft and can be adjusted before publishing on the production server. The development server is primarily used by users learning to use DANDI or by developers.</p> <p>The below instructions will alert you to where the commands for interacting with these  two different servers differ slightly. </p>"},{"location":"13_upload/#setup","title":"Setup","text":"<ol> <li>To create a new Dandiset and upload your data, you need to have a DANDI account. See the Create a DANDI Account page.</li> <li>Log in to DANDI and copy your API key. Click on your user initials in the     top-right corner after logging in. Production (dandiarchive.org) and staging (gui-staging.dandiarchive.org) servers        have different API keys and different logins.</li> <li> <p>Locally:</p> <ol> <li>Create a Python environment. This is not required, but strongly recommended; e.g. miniconda,      virtualenv.</li> <li> <p>Install the DANDI CLI into your Python environment:</p> <pre><code>pip install -U dandi\n</code></pre> </li> <li> <p>Store your API key somewhere that the CLI can find it; see \"Storing       Access Credentials\" below.</p> </li> </ol> </li> </ol>"},{"location":"13_upload/#data-uploadmanagement-workflow","title":"Data upload/management workflow","text":"<ol> <li>Register a Dandiset to generate an identifier. You will be asked to enter     basic metadata: a name (title) and description (abstract) for your dataset.     Click <code>NEW DANDISET</code> in the Web application (top right corner) after logging in.      After you provide a name and description, the dataset identifier will be created;      we will call this <code>&lt;dataset_id&gt;</code>.</li> <li> <p>NWB format:</p> <ol> <li>Convert your data to NWB 2.1+ in a local folder. Let's call this <code>&lt;source_folder&gt;</code>. We suggest beginning the conversion process using only a small amount of data so that common issues may be spotted earlier in the process. This step can be complex depending on your data. NeuroConv automates conversion to NWB from a variety of popular formats. nwb-overview.readthedocs.io points to more tools helpful for working with NWB files, and BIDS converters if you are preparing a BIDS dataset containing NWB files. Feel free to reach out to us for help.</li> <li> <p>Check your files for NWB Best Practices by installing the NWBInspector (<code>pip install -U nwbinspector</code>) and running</p> <pre><code>    nwbinspector &lt;source_folder&gt; --config dandi\n</code></pre> </li> <li> <p>Thoroughly read the NWBInspector report and try to address as many issues as possible. DANDI will prevent validation and upload of any issues labeled as level 'CRITICAL' or above when using the <code>--config dandi</code> option. See     \"Validation Levels for NWB Files\" for more information about validation criteria for     uploading NWB     files and which are deemed critical. We recommend regularly running the inspector early in the process to generate the best NWB files possible.  Note that some autodetected violations, such as <code>check_data_orientation</code>, may be safely ignored in the event     that the data is confirmed to be in the correct form; this can be done using either the <code>--ignore &lt;name_of_check_to_suppress&gt;</code> flag or a config file. See the NWBInspector CLI documentation for more details and other options, or type <code>nwbinspector --help</code>. If the report is too large to efficiently navigate in your console, you can save a report using</p> <pre><code>nwbinspector &lt;source_folder&gt; --config dandi --report-file-path &lt;report_location&gt;.txt\n</code></pre> </li> <li> <p>Once your files are confirmed to adhere to the Best Practices, perform an official validation of the NWB files by running: <code>dandi validate --ignore DANDI.NO_DANDISET_FOUND &lt;source_folder&gt;</code>.     If you are having trouble with validation, make sure the conversions were run with the most recent version of <code>dandi</code>, <code>PyNWB</code> and <code>MatNWB</code>.</p> </li> <li>Now, prepare and fully validate again within the dandiset folder used for upload:<pre><code>dandi download https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft\ncd &lt;dataset_id&gt;\ndandi organize &lt;source_folder&gt; -f dry\ndandi organize &lt;source_folder&gt;\ndandi validate .\ndandi upload\n</code></pre> </li> </ol> <p>Note that the <code>organize</code> steps should not be used if you are preparing a BIDS dataset with the NWB files. Uploading to the development server is controlled via <code>-i</code> option, e.g. <code>dandi upload -i dandi-staging</code>. Note that validation is also done during <code>upload</code>, but ensuring compliance using <code>validate</code> prior upload helps avoid interruptions of the lengthier upload process due to validation failures. 6. Add metadata by visiting your Dandiset landing page:    <code>https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft</code> and clicking on the <code>METADATA</code> link.</p> </li> </ol> <p>If you have an issue using the Python CLI, see the Dandi Debugging section.</p>"},{"location":"13_upload/#storing-access-credentials","title":"Storing Access Credentials","text":"<p>There are two options for storing your DANDI access credentials.</p> <ol> <li> <p><code>DANDI_API_KEY</code> Environment Variable</p> <ul> <li> <p>By default, the DANDI CLI looks for an API key in the <code>DANDI_API_KEY</code>   environment variable.  To set this on Linux or macOS, run:</p> <pre><code>export DANDI_API_KEY=personal-key-value\n</code></pre> </li> <li> <p>Note that there are no spaces around the \"=\".</p> </li> </ul> </li> <li> <p><code>keyring</code> Library</p> <ul> <li> <p>If the <code>DANDI_API_KEY</code> environment variable is not set, the CLI will look up the API     key using the keyring library, which     supports numerous backends, including the system keyring, an encrypted keyfile,     and a plaintext (unencrypted) keyfile.</p> </li> <li> <p>Specifying the <code>keyring</code> backend</p> <ul> <li>You can set the backend the <code>keyring</code> library uses either by setting the   <code>PYTHON_KEYRING_BACKEND</code> environment variable or by filling in the <code>keyring</code>   library's configuration file.</li> <li>IDs for the available backends can be listed by running <code>keyring --list</code>.</li> <li>If no backend is specified in this way, the library will use the available   backend with the highest priority.</li> <li>If the DANDI CLI encounters an error while attempting to fetch the API key   from the default keyring backend, it will fall back to using an encrypted   keyfile (the <code>keyrings.alt.file.EncryptedKeyring</code> backend).  If the keyfile   does not already exist, the CLI will ask you for confirmation; if you answer   \"yes,\" the <code>keyring</code> configuration file (if it does not already exist; see   above) will be configured to use <code>EncryptedKeyring</code> as the default backend.   If you answer \"no,\" the CLI will exit with an error, and you must store the   API key somewhere accessible to the CLI on your own.</li> </ul> </li> <li> <p>Storing the API key with <code>keyring</code></p> <ol> <li> <p>You can store your API key where the <code>keyring</code> library can find it by using   the <code>keyring</code> program: Run <code>keyring set dandi-api-dandi key</code> and enter the   API key when asked for the password for <code>key</code> in <code>dandi-api-dandi</code>.</p> </li> <li> <p>If the API key isn't stored in either the <code>DANDI_API_KEY</code> environment variable   or in the keyring, the CLI will prompt you to enter the API key, and then it   will store it in the keyring.  This may cause you to be prompted further; you   may be asked to enter a password to encrypt/decrypt the keyring, or you may be   asked by your operating system to confirm whether to give the DANDI CLI access to the   keyring.</p> </li> </ol> </li> </ul> </li> </ol>"},{"location":"14_publish/","title":"Publishing Dandisets","text":"<p>Once you create a Dandiset, DANDI will automatically create a <code>draft</code> version of the Dandiset that can be changed as many times as needed by editing the  metadata or uploading new files.</p> <p>When the draft version is ready, you can publish your Dandiset. This results in an immutable snapshot of your Dandiset with its own unique version number that others can cite. If you need to change the data or metadata, you can do so by continuing to modify the draft version and publishing a new version when you are ready.</p> <p>Follow these steps to publish your Dandiset:</p> <ol> <li> <p>Edit the Dandiset metadata, aiming to fix all Dandiset metadata validation    errors, and include any other useful information. For example, you may want    to edit the following fields:</p> <ul> <li>People and funding contributors</li> <li>Protocol information</li> <li>Keywords</li> <li>Related resources such as publications and code repositories</li> </ul> </li> <li> <p>Fix all asset metadata errors by modifying the asset files to eliminate    the errors and re-uploading them.</p> </li> <li> <p>When all the Dandiset metadata and asset metadata errors are fixed, and the Dandiset is made public if it was initially embargoed, the    <code>Publish</code> button (on the right panel of the Dandiset landing page) will    be enabled and turn green. Click the button to publish your Dandiset.</p> </li> <li> <p>In the lower right section of the Dandiset landing page, you should see    the new, published version of your Dandiset listed. Click on that link    to view this version.</p> </li> </ol> <p>NOTE: Dandisets with Zarr assets currently cannot be published. We are  actively working on enabling this feature.</p>"},{"location":"15_debugging/","title":"Debugging","text":"<p>If something goes wrong while using the Python CLI client, the first place to check for more information so that you can file a quality bug report is the logs.  Every command records a copy of its logs in a logfile, the location of which is reported to the user when the command finishes running.  The location of the logs varies by platform, e.g.:</p> <ul> <li>Linux: <code>~/.cache/dandi-cli/log</code> or <code>$XDG_CACHE_HOME/dandi-cli/log</code></li> <li>macOS: <code>~/Library/Logs/dandi-cli</code></li> </ul> <p>Logs are named with a combination of the time at which the <code>dandi</code> command started running and the process ID of the command.</p> <p>Recent versions of the client include all possible debugging information in the logs, but if you're using an older version, only log messages that were printed to the user when the command ran are recorded.  As a result, in order to get complete debugging information, you may have to rerun the problematic command, this time increasing the logging level by passing <code>-l DEBUG</code> or <code>--log-level DEBUG</code> on the command line.  Note that this option goes between the main <code>dandi</code> command and the name of the subcommand:</p> <pre><code># Right:\ndandi -l DEBUG upload\n\n# Wrong:\ndandi upload -l DEBUG\n</code></pre> <p>In addition, many commands can be put into a developer-specific mode for showing raw progress information instead of fancy progress bars.  For the <code>delete</code>, <code>organize</code>, <code>upload</code>, and <code>validate</code> commands, this can be done by setting the <code>DANDI_DEVEL</code> environment variable and passing <code>--devel-debug</code> to the command:</p> <pre><code>DANDI_DEVEL=1 dandi upload --devel-debug\n</code></pre> <p>For the <code>download</code> command, the equivalent is the <code>-f debug</code>/<code>--format debug</code> option:</p> <pre><code>dandi download -f debug\n</code></pre> <p>More advanced users who are familiar with the Python debugger can instruct the client to automatically open the debugger if any errors occur by supplying the <code>--pdb</code> option to the command.  Like the <code>-l</code>/<code>--log-level</code> option, the <code>--pdb</code> option must be placed between <code>dandi</code> and the name of the subcommand.</p>"},{"location":"16_account/","title":"Create a DANDI Account","text":"<p>A DANDI account is only required for specific features. Without a DANDI account, you can search, view, and download files. With a DANDI account, you can additionally create Dandisets and access the DANDI Hub to analyze existing data.</p>"},{"location":"16_account/#instructions","title":"Instructions","text":"<ol> <li>To create a DANDI account, first create a GitHub account if you don't have one.</li> <li>Using your GitHub account, register for a DANDI account by selecting the <code>LOG IN WITH GITHUB</code> button on the DANDI homepage.</li> <li>You will receive an email acknowledging that your request for an account will be reviewed within 24 hours.</li> <li>note: Requests from new GitHub accounts and from emails that are not an <code>.edu</code> domain might take longer to review and are more likely to be rejected, especially if there are no plans described to upload data to the archive.</li> <li>If your request for an account is approved, you will be able to log in to DANDI using GitHub by clicking the <code>LOG IN WITH GITHUB</code> button.</li> </ol>"},{"location":"20_project_structure/","title":"Project Structure","text":"<p>The DANDI project can be represented schematically:</p> <p></p> <p>The Client side contains the DANDI Python CLI and DANDI Web application.</p> <p>The Server side contains a RESTful API and DANDI JupyterHub.</p> <p>The Dandiset is a file organization to store data together with metadata.</p> <p>The DANDI project is organized around several GitHub repositories:</p> Repository Description DANDI archive Contains the code for deploying the client-side Web application frontend based on the Vue.js framework as well as a Django-based backend to run the DANDI REST API. DANDI JupyterHub Contains the code for deploying a JupyterHub instance to support interaction with the DANDI archive. DANDI Python client Contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. handbook Provides the contents of this website. helpdesk Contains our community help platform where you can submit issues. schema Provides the details and some supporting code for the DANDI metadata schema. schema Python library Provides a Python library for updating the schema and for creating and validating DANDI objects. website Provides an overview of the DANDI project and the team members and collaborators."},{"location":"30_data_standards/","title":"Data Standards","text":"<p>DANDI requires uploaded data to adhere to community data standards.  These standards help data curators package all the necessary metadata and provide a uniform structure so that data can be more easily understood and reused by future users.  DANDI also leverages these standards to provide features like data validation and automatic metadata extraction and search. DANDI currently supports two data standards: </p> <ul> <li>For cellular neurophysiology, such as electrophysiology and optical physiology, use Neurodata Without Borders (NWB)</li> <li>For neuroimaging data, such as MRI, use Brain Imaging Data Structure (BIDS)</li> </ul> <p>For microscopy data from immunostaining, we are using the BIDS extension for microscopy.</p> <p>To share data on DANDI, you will first need to convert your data to an appropriate standard. If you would like help determining which standard is most appropriate for your data, do not hesitate to reach out using the dandi helpdesk and we would be happy to assist.</p>"},{"location":"30_data_standards/#neurodata-without-borders-nwb","title":"Neurodata Without Borders (NWB)","text":"<p>NWB is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data. The NWB team supports APIs in Python (PyNWB) and MATLAB (MatNWB), with tutorials for writing data broken down by experiment type. See the NWB Tutorials page for more details. Also see the NWB Conversion Tools user guide for converting data for automated conversions from several popular proprietary data formats.  The best way to get help from the NWB community is through the NWB user Slack channel.</p>"},{"location":"30_data_standards/#brain-imaging-data-format-bids","title":"Brain Imaging Data Format (BIDS)","text":"<p>BIDS is a way to organize and describe neuroimaging and behavioral data.  See the Getting Started page for instructions for how to convert your neuroimaging data to BIDS.</p> <p>For microscopy and associated MR data, use the BIDS extension for microscopy.</p>"},{"location":"35_data_licenses/","title":"Data Licenses","text":"<p>To create a Dandiset, you must select a license under which to share the data. Because the DANDI Archive provides a platform for open data sharing, the licenses come from Creative Commons, an international nonprofit organization dedicated to establishing, growing, and maintaining a shared commons in the spirit of open source.</p> <p>These licenses enable the dataset's copyright holder to grant permissions to others to share and use the data for a wide range of purposes. The licenses available to users of the Archive are as follows:</p> <ul> <li> <p>Attribution (CC-BY-4.0). This license grants permission to share the data to others and to adapt the data by remixing, transforming, and building upon it, so long as appropriate credit is given to the copyright holder and any changes to the original are clearly indicated. The dataset may be used for any purpose (even commercial ones). Note that this license retains the original copyright while granting permissive access to the data to all others.</p> </li> <li> <p>Public domain dedication (CC0-1.0). This license dedicates the dataset to the public domain, relinquishing copyright and therefore allowing anyone to use the dataset for any purpose without restriction.</p> </li> </ul> <p>You can learn more about the theory of how the Creative Commons licenses operate at their website. If you have any questions or concerns, send a message to help@dandiarchive.org.</p>"},{"location":"40_development/","title":"Developer Notes","text":"<p>This page contains important information for anyone starting development work on the DANDI project.</p>"},{"location":"40_development/#overview","title":"Overview","text":"<p>The DANDI archive dev environment comprises three major pieces of software: <code>dandi-archive</code>, <code>dandi-cli</code>, and <code>dandi-schema</code>.</p>"},{"location":"40_development/#dandi-archive","title":"<code>dandi-archive</code>","text":"<p><code>dandi-archive</code> is the web frontend application; it connects to <code>dandi-api</code> and provides a user interface to all the DANDI functionality.  <code>dandi-archive</code> is a standard web application built with <code>yarn</code>. See the <code>dandi-archive</code> README for instructions on how to build it locally.</p> <p>The Django application makes use of several services to provide essential function for the DANDI REST API, including Postgres (to hold administrative data about the web application itself), Celery (to run asynchronous compute tasks as needed to implement API semantics), and RabbitMQ (to act as a message broker between Celery and the rest of the application).</p> <p>The easiest way to run the API along with its services is through a Docker Compose setup, as detailed in the Develop with Docker quickstart.</p>"},{"location":"40_development/#dandi-cli","title":"<code>dandi-cli</code>","text":"<p><code>dandi-cli</code> is a Python command line tool used to manage downloading and uploading of data with the archive. You may need to use this tool when developing new features for the frontend and backend, but there are other methods of faking data in the system to work with as well. You can install <code>dandi-cli</code> with a command like <code>pip install dandi</code> (then invoke <code>dandi</code> on the command line to run the tool), or build it locally following the instructions in the <code>dandi-cli</code> README.</p>"},{"location":"40_development/#dandi-schema","title":"<code>dandi-schema</code>","text":"<p><code>dandi-schema</code> is a Python library for  creating, maintaining, and validating the DANDI metadata models for dandisets  and assets. You may need to make use of this tool when improving models, or  migrating metadata. You can install <code>dandi-schema</code> with a command like  <code>pip install dandi-schema</code>. When releases are published through dandi-schema,  corresponding json-schemas are generated in the release folder of the dandi schema repo. See the <code>dandi-schema</code> README for instructions on  viewing the schemas.</p>"},{"location":"40_development/#technologies-used","title":"Technologies Used","text":"<p>This section details some foundational technologies used in <code>dandi-archive</code>. Some basic understanding of these technologies is the bare minimum requirement for contributing meaningfully, but keep in mind that the DANDI team can help you get spun up as well.</p> <p>JavaScript/TypeScript. The DANDI archive code is a standard JavaScript web application, but we try to implement new functionality using TypeScript.</p> <p>Vue/VueX. The application's components are written in Vue, and global application state is managed through VueX.</p> <p>Vuetify. The components make heavy use of the Vuetify component library.</p> <p>Python3. The backend code is written in Python 3.</p> <p>Django/drf/drf-yasg. The API infrastructure is implemented through a Django application. This means that application resources must be mapped to Django models, while Django views mediate API responses. The REST endpoints are implemented via Django Rest Framework (DRF), while DRF-YASG is used to generate Swagger documentation.</p> <p>For general help with <code>dandi-archive</code>, contact @waxlamp.</p>"},{"location":"40_development/#deployment","title":"Deployment","text":"<p>The DANDI project uses automated services to continuously deploy both the <code>dandi-api</code> backend and the <code>dandi-archive</code> frontend.</p> <p>Heroku manages backend deployment automatically from the <code>master</code> branch of the <code>dandi-api</code> repository. For this reason it is important that pull requests pass all CI tests before they are merged. Heroku configuration is in turn managed by Terraform code stored in the <code>dandi-infrastructure</code> repository. If you need access to the Heroku DANDI organization, talk to @satra.</p> <p>Netlify manages the frontend deployment process. Similarly to <code>dandi-api</code>, these deployments are based on the <code>master</code> branch of <code>dandi-archive</code>. The <code>netlify.toml</code> file controls Netlify settings. The @dandibot GitHub account is the \"owner\" of the Netlify account used for this purpose; in order to get access to that account, speak to @satra.</p>"},{"location":"40_development/#monitoring","title":"Monitoring","text":""},{"location":"40_development/#services-status","title":"Service(s) status","text":"<p>The DANDI project uses upptime to monitor the status of DANDI provided and third-party services. The configuration is available in .upptimerc.yml of the https://github.com/dandi/upptime repository, which is automatically updated by the upptime project pipelines. Upptime automatically opens new issues if any service becomes unresponsive, and closes issues whenever service comes back online. https://www.dandiarchive.org/upptime/ is the public dashboard for the status of DANDI services.</p>"},{"location":"40_development/#logging","title":"Logging","text":""},{"location":"40_development/#sentry","title":"Sentry","text":"<p>Sentry is used for error tracking main deployment. To access Sentry, login to https://dandiarchive.sentry.io .</p>"},{"location":"40_development/#heroku-papertrail","title":"Heroku &amp; Papertrail","text":"<p>The <code>dandi-api</code> and <code>dandi-api-staging</code> apps have the Papertrail add-on configured to capture logs. To access Papertrail, log in to the Heroku dashboard, proceed to the corresponding app and click on the \"Papertrail\" add-on.</p> <p>A cronjob on the <code>drogon</code> server backs up Papertrail logs as .csv files hourly at <code>/mnt/backup/dandi/papertrail-logs/{app}</code>. Moreover, <code>heroku logs</code> processes per app dump logs to <code>/mnt/backup/dandi/heroku-logs/{app}</code> directory.</p>"},{"location":"40_development/#continuous-integration-ci-jobs","title":"Continuous Integration (CI) Jobs","text":"<p>The DANDI project uses GitHub Actions for continuous integration. Logs for many of the repositories are archived on <code>drogon</code> server at <code>/mnt/backup/dandi/tinuous-logs/</code>.</p>"},{"location":"40_development/#code-hosting","title":"Code Hosting","text":"<p>All code repositories are hosted on GitHub. The easiest way to contribute is to gain push access to the repositories by talking to @waxlamp; this way, you can create pull requests based on branches within the origin repositories, which in turn allows for Netlify deploy previews and Heroku staging previews to be built.</p> <p>However, this is not strictly required. You can contribute using the standard fork-and-pull-request model, but under this workflow we will lose the benefit of those previews.</p>"},{"location":"40_development/#email-lists","title":"Email Lists","text":"<p>The project's email domain name services are managed via Terraform as AWS Route 53 entries. This allows the API server to send emails to users, etc. It also means we need a way to forward incoming emails to the proper mailing list--this is accomplished with a service called ImprovMX.</p> <p>The email addresses info@dandiarchive.org and help@dandiarchive.org are advertised to users as general email addresses to use to ask for information or help; both of them are forwarded to dandi@mit.edu, a mailing list containing the leaders and developers of the project. The forwarding is done by the ImprovMX service, and more such email addresses can be created as needed within that service.</p> <p>If you need the credentials for logging into ImprovMX, speak to Roni Choudhury (roni.choudhury@kitware.com).</p>"},{"location":"40_development/#miscellaneous-tips-and-information","title":"Miscellaneous Tips and Information","text":""},{"location":"40_development/#use-email-address-to-log-into-dev-django-admin-panel","title":"Use email address to log into dev Django admin panel","text":"<p>Once <code>dandi-api</code> is up and running, you can access the Django admin panel at http://localhost:8000/admin. The login page asks for a \"username\" but really it is expecting the email address associated with the username.</p> <p>One easy trick here is to supply the username again as the email address when you are setting up the superuser during initial setup.</p>"},{"location":"40_development/#refresh-github-login-to-log-into-prod-django-admin-panel","title":"Refresh GitHub login to log into prod Django admin panel","text":"<p>To log into the production Django admin panel, you must simply be logged into the DANDI Archive production instance using an admin account.</p> <p>However, at times the Django admin panel login seems to expire while the login to DANDI Archive proper is still live. In this case, simply log out of DANDI, log back in, and then go to the Django admin panel URL (e.g. https://api.dandiarchive.org/admin) and you should be logged back in there.</p>"},{"location":"40_development/#why-do-incoming-emails-to-dandiarchiveorg-look-crazy","title":"Why do incoming emails to dandiarchive.org look crazy?","text":"<p>When a user emails help@dandiarchive.org or info@dandiarchive.org, those messages are forwarded to dandi@mit.edu (see above) so that the dev team sees them. However, these emails arrive with a long, spammy-looking From address with a Heroku DNS domain; this seems to be an artifact of how mit.edu processes emails, and does not occur in general (e.g. messages sent from the API server to users).</p>"},{"location":"50_hub/","title":"Using the DANDI Hub","text":"<p>DANDI Hub is a JupyterHub instance in the cloud to interact with the data stored in DANDI, and is free to use for exploratory analysis of data on DANDI. For instructions on how to navigate JupyterHub see this YouTube tutorial. Note that DANDI Hub is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"50_hub/#registration","title":"Registration","text":"<p>To use the DANDI Hub, you must first register for an account using the DANDI website. See the Create a DANDI Account page.</p>"},{"location":"50_hub/#choosing-a-server-option","title":"Choosing a server option","text":"<p>When you start up the DANDI Hub, you will be asked to select across a number of server options. For basic exploration, Tiny or Base would most likely be appropriate. The DANDI Hub also currently offers Medium and Large options, which have more available memory and compute power. The \"T4 GPU inference\" server comes with an associated T4 GPU, and is intended to be used for applications that require GPU for inference. We request that users of this server be considerate of their usage of the DANDI Hub as a free community resource. Training large deep neural networks is not appropriate. A \"Base (MATLAB)\" server is also available, which provides a MATLAB cloud installation but you would be required to provide your own license.</p>"},{"location":"50_hub/#custom-server-image","title":"Custom server image","text":"<p>If you need additional software installed in the image, you can add a server image that will be made available for all users in the <code>Server Options</code> menu.  Add a server image by updating the <code>profileList</code> in the JupyterHub config file and submitting a pull request to the dandi-hub repository.  Once the pull request is merged, the DANDI team will redeploy JupyterHub and the image will be available.</p>"},{"location":"50_hub/#example-notebooks","title":"Example notebooks","text":"<p>The best way to share analyses on DANDI data is through the DANDI example notebooks. These notebooks are maintained in the dandi/example-notebooks repository which provides more information about their organization. Dandiset contributors are encouraged to use these notebooks to demonstrate how to read, analyze, and visualize the data, and how to produce figures from associated scientific publications.</p> <p>Notebooks can be added and updated through a pull request to the dandi/example-notebooks repository. Once the pull request is merged, your contributed notebook will be available to all DANDI Hub users.</p>"},{"location":"about/policies/","title":"General Policies v1.1.0","text":""},{"location":"about/policies/#content","title":"Content","text":"<ul> <li>Scope: Neurophysiology research. Raw and derived experimental data. Content   must not violate privacy or copyright, or breach confidentiality or non-disclosure   agreements for data collected from human subjects.</li> <li>Status of research data: Empirical (not simulated) data and associated metadata from any stage of the   research study's life cycle is accepted.  Simulated data is handled on a case-by-case basis, contact the DANDI team </li> <li>Eligible users: Anyone working with the data in the scope of the archive may register as a user of DANDI. All users are   allowed to deposit content for which they possess the appropriate rights   and which falls within the scope of the archive.</li> <li>Ownership: By uploading content, no change of ownership is implied and no   property rights are transferred to the DANDI team. All uploaded content remains   the property of the parties prior to submission and must be accompanied by a license allowing   DANDI project data access, archival, and re-distribution (see License below).</li> <li>Data file formats: DANDI only accepts data using standardized formats such   as Neurodata Without Borders, Brain Imaging Data Structure,   Neuroimaging Data Model, and other BRAIN Initiative   standards. We are working with the community to improve these standards and to   make DANDI archive FAIR.</li> <li>Data quality: All data are provided \u201cas-is\u201d, and the user shall hold   DANDI and data providers supplying data to the DANDI Archive free and harmless in   connection with the use of such data.</li> <li>Metadata types and sources: All metadata is stored internally in JSON format   according to a defined JSON schema. Metadata records violating the schema are not allowed.</li> <li>Language: Textual items must be in English. Latin names could be used in exceptional cases where appropriate.</li> <li>Licenses: Users must specify a license for each dataset chosen from the list of the DANDI archive approved licenses. Users allow for the DANDI archive to extract metadata records and make them available under permissive CC0 license.</li> </ul>"},{"location":"about/policies/#access-and-reuse","title":"Access and Reuse","text":"<ul> <li>Access to data objects: Files deposited to the archive are accessible to the public    openly or accessible to collaborators for embargoed datasets. Access to metadata and data    files is provided over standard protocols such as HTTPS.</li> <li>Use and reuse of data objects: Use and reuse is subject to the terms of the license   under which the data objects were deposited.</li> <li> <p>Metadata access and reuse: Metadata records, provided by the users or extracted from the assets, are licensed under CC0. All metadata is made publicly available and can be harvested.</p> </li> <li> <p>Embargo status: Users may deposit content under an embargo status and   provide an anticipated end date for the embargo. The repository will restrict   access to the data until the end of the embargo period, at which time the   content will automatically become publicly available. The end of the embargo   period is the earliest of the date provided by submitter, the first publication   using the data, or the end of funding support for the collection and/or dissemination   of the dataset.</p> </li> <li>Restricted access: Depositors of embargoed datasets have the ability to   share access with other collaborators. These files will not be made publicly   available till the end of the embargo period.</li> </ul>"},{"location":"about/policies/#removal","title":"Removal","text":"<ul> <li> <p>Revocation: Content not considered to fall under the scope of the repository   can be removed and associated DOIs issued by DANDI revoked. Inform the DANDI team   promptly, ideally no later than 24 hours from upload, about any suspected policy   violation. Alternatively, content found to already have an external DOI will   have the DANDI DOI invalidated and the record updated to indicate the original   external DOI. User access may be revoked on violation of Terms of Use.</p> </li> <li> <p>Withdrawal: If the uploaded research object must later be withdrawn, the   reason for the withdrawal will be indicated on a tombstone page, which will   henceforth be served in its place. Withdrawal is considered an exceptional   action, which normally should be requested and fully justified by the original   uploader. In any other circumstance reasonable attempts will be made to contact   the original uploader to obtain consent. The DOI and the URL of the original   object are retained.</p> </li> <li> <p>User data on Dandihub: At present, user data on Dandihub is being removed   periodically and Dandihub storage space should not be considered persistent.</p> </li> </ul>"},{"location":"about/policies/#longevity","title":"Longevity","text":"<ul> <li>Versions: Datasets are versioned when published. Prior to publishing the   state of a dataset may continue to evolve and the data or metadata are neither   versioned, nor guaranteed to persist. Derivatives of data files may be generated, but original content is   never modified.</li> <li>Replicas: All data files are stored on an AWS public bucket, with replicas   housed at Dartmouth College.  Data files are kept in multiple replicas at the   moment, but this may change over time, and no recovery mechanisms for unversioned data   should be assumed to be in place.</li> <li>Retention period: Versioned items will be retained for the lifetime of the repository.   This is currently the lifetime of the NIH award, which currently expires in   April 2029.</li> <li>Functional preservation: DANDI makes no promises of usability and   understandability of deposited objects.</li> <li>File preservation: Data files and metadata are backed up nightly and   replicated into multiple copies in different storage services.</li> <li>Fixity and authenticity: All data files are stored along with multiple   checksums of the file content. Files are regularly checked against their   checksums to assure that file content remains constant.</li> <li>Succession plans: In case of a repository shutdown, our best efforts will   be made to integrate all content into suitable alternative institutional and/or   other repositories overlapping in the scope of the DANDI archive.</li> </ul> <p>This policy document is derived from the Zenodo General Policies v1.0.</p>"},{"location":"about/terms/","title":"Terms of Use v1.0.1","text":"<p>The DANDI data archive (\"DANDI\") is offered by the DANDI project as part of its mission to make available the results of its work.</p> <p>Use of DANDI, both the uploading and downloading of data, denotes agreement with the following terms:</p> <ol> <li> <p>DANDI is an open dissemination research data repository for the preservation    and making available of research, educational and informational content. Access    to DANDI\u2019s content is open to all.</p> </li> <li> <p>Content may be uploaded free of charge by the US BRAIN Initiative and other    projects required to submit data to a public archive and those without ready    access to an organized data center.</p> </li> <li> <p>The uploader is exclusively responsible for the content that they upload to    DANDI and shall indemnify and hold the DANDI team free and harmless in    connection with their use of the service. The uploader shall ensure that their    content is suitable for open dissemination, and that it complies with these    terms and applicable laws, including, but not limited to, privacy, data    protection and intellectual property rights [1]. In addition, where data that    was originally sensitive personal data is being uploaded for open dissemination    through DANDI, the uploader shall ensure that such data is either anonymized    to an appropriate degree or fully consent cleared [2].</p> </li> <li> <p>Access to DANDI, and all content, is provided on an \"as-is\" basis. Users of    content (\"Users\") shall respect applicable license conditions. Download and    use of content from DANDI does not transfer any intellectual property rights    in the content to the User.</p> </li> <li> <p>Users are exclusively responsible for their use of content, and shall indemnify    and hold the DANDI team free and harmless in connection with their download    and/or use. Hosting and making content available through DANDI does not    represent any approval or endorsement of such content by the DANDI team.</p> </li> <li> <p>The DANDI team reserves the right, without notice, at its sole discretion and    without liability, (i) to alter, delete or block access to content that it    deems to be inappropriate or insufficiently protected, and (ii) to restrict    or remove User access where it considers that use of DANDI interferes with    its operations or violates these Terms of Use or applicable laws.</p> </li> <li> <p>Unless specified otherwise, DANDI metadata may be freely reused under the    CC0 waiver.</p> </li> <li> <p>These Terms of Use are subject to change by the DANDI team at any time and    without notice, other than through posting the updated Terms of Use on the    DANDI website.</p> </li> <li> <p>Uploaders considering DANDI for the storage of unanonymized or encrypted/unencrypted   sensitive personal data are advised to use bespoke platforms rather than open   dissemination services like DANDI for sharing their data.</p> </li> </ol> <p>[1] [2] See further the user pages regarding uploading for information on anonymization of datasets that contain sensitive personal information.</p> <p>If you have any questions or comments with respect to DANDI, or if you are unsure whether your intended use is in line with these Terms of Use, or if you seek permission for a use that does not fall within these Terms of Use, please contact us.</p> <p>This Terms of Service document is derived from the Zenodo terms of service v1.2.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/","title":"DANDI User Guide, Part I","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\nfrom dateutil.tz import tzlocal\nimport uuid\n\nimport numpy as np\n\nfrom pynwb.file import NWBFile, Subject\nfrom pynwb.ecephys import ElectricalSeries\nfrom pynwb import NWBHDF5IO\n\ndef create_nwbfile(subject_id):\n    nwbfile = NWBFile(\n        session_description='my first synthetic recording',\n        identifier=str(uuid.uuid4()),\n        session_start_time=datetime.now(tzlocal()),\n        experimenter='Dr. Bilbo Baggins',\n        lab='Bag End Laboratory',\n        institution='University of Middle Earth at the Shire',\n        experiment_description='I went on an adventure with thirteen dwarves to reclaim vast treasures.',\n        session_id='001',\n    )\n\n    nwbfile.subject = Subject(\n        subject_id=subject_id,\n        species='Mus musculus',\n        age='P90D',\n        sex='F',\n    )\n\n    device = nwbfile.create_device(name='trodes_rig123')\n\n    electrode_group = nwbfile.create_electrode_group(\n        name='tetrode1',\n        description=\"an example tetrode\",\n        location=\"hippocampus\",\n        device=device,\n    )\n\n    for _ in range(4):\n        nwbfile.add_electrode(\n            x=1.0, y=2.0, z=3.0,\n            imp=np.nan,\n            location='CA1',\n            filtering='none',\n            group=electrode_group,\n        )\n\n    electrode_table_region = nwbfile.create_electrode_table_region([0, 2], 'the first and third electrodes')\n\n\n    rate = 10.0\n    data_len = 1000\n    ephys_data = np.random.rand(data_len * 2).reshape((data_len, 2))\n    ephys_timestamps = np.arange(data_len) / rate\n\n    ephys_ts = ElectricalSeries(\n        name='test_ephys_data',\n        data=ephys_data,\n        electrodes=electrode_table_region,\n        timestamps=ephys_timestamps,\n        description=\"Random numbers generated with numpy.random.rand\"\n    )\n    nwbfile.add_acquisition(ephys_ts)\n    \n    return nwbfile\n</pre> from datetime import datetime from dateutil.tz import tzlocal import uuid  import numpy as np  from pynwb.file import NWBFile, Subject from pynwb.ecephys import ElectricalSeries from pynwb import NWBHDF5IO  def create_nwbfile(subject_id):     nwbfile = NWBFile(         session_description='my first synthetic recording',         identifier=str(uuid.uuid4()),         session_start_time=datetime.now(tzlocal()),         experimenter='Dr. Bilbo Baggins',         lab='Bag End Laboratory',         institution='University of Middle Earth at the Shire',         experiment_description='I went on an adventure with thirteen dwarves to reclaim vast treasures.',         session_id='001',     )      nwbfile.subject = Subject(         subject_id=subject_id,         species='Mus musculus',         age='P90D',         sex='F',     )      device = nwbfile.create_device(name='trodes_rig123')      electrode_group = nwbfile.create_electrode_group(         name='tetrode1',         description=\"an example tetrode\",         location=\"hippocampus\",         device=device,     )      for _ in range(4):         nwbfile.add_electrode(             x=1.0, y=2.0, z=3.0,             imp=np.nan,             location='CA1',             filtering='none',             group=electrode_group,         )      electrode_table_region = nwbfile.create_electrode_table_region([0, 2], 'the first and third electrodes')       rate = 10.0     data_len = 1000     ephys_data = np.random.rand(data_len * 2).reshape((data_len, 2))     ephys_timestamps = np.arange(data_len) / rate      ephys_ts = ElectricalSeries(         name='test_ephys_data',         data=ephys_data,         electrodes=electrode_table_region,         timestamps=ephys_timestamps,         description=\"Random numbers generated with numpy.random.rand\"     )     nwbfile.add_acquisition(ephys_ts)          return nwbfile       In\u00a0[\u00a0]: Copied! <pre>!pwd\n</pre> !pwd In\u00a0[\u00a0]: Copied! <pre>from os import mkdir\n\nmkdir('../data')\n\nnwbfile = create_nwbfile(subject_id='001')\n\nwith NWBHDF5IO('../data/ecephys_example.nwb', 'w') as io:\n    io.write(nwbfile)\n</pre> from os import mkdir  mkdir('../data')  nwbfile = create_nwbfile(subject_id='001')  with NWBHDF5IO('../data/ecephys_example.nwb', 'w') as io:     io.write(nwbfile) In\u00a0[\u00a0]: Copied! <pre>nwbfile = create_nwbfile(subject_id='002')\n\nwith NWBHDF5IO('../data/ecephys_example2.nwb', 'w') as io:\n    io.write(nwbfile)\n</pre> nwbfile = create_nwbfile(subject_id='002')  with NWBHDF5IO('../data/ecephys_example2.nwb', 'w') as io:     io.write(nwbfile) <p>Then go back to terminal and run</p> <pre>dandi organize ../data\ndandi upload -i dandi-staging\n</pre> In\u00a0[\u00a0]: Copied! <pre>from nwbwidgets import nwb2widget\n\nnwb2widget(nwbfile)\n</pre> from nwbwidgets import nwb2widget  nwb2widget(nwbfile) <p>It can also be used to explore any file shared on DANDI. You can use the DANDI API to access the s3 path of any file and stream it directly into NWB Widgets.</p> In\u00a0[\u00a0]: Copied! <pre># calcium imaging, Giocomo Lab (30 GB)\n#dandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"\n\n# neuropixel, Giocomo Lab (46 GB)\n#dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"\n\n# neuropixel, Allen Intitute\n#dandiset_id, filepath = \"000022\", \"sub-744912845/sub-744912845_ses-766640955.nwb\"\n\n# ecephys, Buzsaki Lab (15.2 GB)\n#dandiset_id, filepath = \"000003\", \"sub-YutaMouse41/sub-YutaMouse41_ses-YutaMouse41-150831_behavior+ecephys.nwb\"\n</pre> # calcium imaging, Giocomo Lab (30 GB) #dandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"  # neuropixel, Giocomo Lab (46 GB) #dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"  # neuropixel, Allen Intitute #dandiset_id, filepath = \"000022\", \"sub-744912845/sub-744912845_ses-766640955.nwb\"  # ecephys, Buzsaki Lab (15.2 GB) #dandiset_id, filepath = \"000003\", \"sub-YutaMouse41/sub-YutaMouse41_ses-YutaMouse41-150831_behavior+ecephys.nwb\" In\u00a0[\u00a0]: Copied! <pre>from dandi.dandiapi import DandiAPIClient\nfrom pynwb import NWBHDF5IO\nfrom nwbwidgets import nwb2widget\n\nwith DandiAPIClient() as client:\n    asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\n    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n\nio = NWBHDF5IO(s3_url, mode='r', load_namespaces=True, driver='ros3')\nnwb = io.read()\nnwb2widget(nwb)\n</pre> from dandi.dandiapi import DandiAPIClient from pynwb import NWBHDF5IO from nwbwidgets import nwb2widget  with DandiAPIClient() as client:     asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)     s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)  io = NWBHDF5IO(s3_url, mode='r', load_namespaces=True, driver='ros3') nwb = io.read() nwb2widget(nwb)"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#part-i","title":"Part I\u00b6","text":""},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#converting-data-to-nwb","title":"Converting data to NWB\u00b6","text":"<p>This is part 3 of the DANDI User Training on Nov 1, 2021.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#neurodata-without-borders-nwb","title":"Neurodata Without Borders (NWB)\u00b6","text":"<p>DANDI has chosen NWB as our supported format for exchanging neurophysiology data. NWB is a BRAIN Initiative-backed data standard designed to package all of the data and metadata associated with neurophysiology experiments into a single file that enables sharing and re-analysis of the data. This includes extracellular and intracellular electrophysiology, optical physiology, and behavior. NWB defines a data organization schema that ensure the crucial metadata is packaged in a standardized way. This includes not just the neurophysiology recordings, but also metadata about the subjects, equipment, task structure, etc.</p> <p></p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#creating-an-nwb-file","title":"Creating an NWB file\u00b6","text":"<p>Converting data to NWB is a big topic that merits its own workshop! Here, we will just give a surface introduction by quickly walk through a script for creating an NWB file. The following creates an example extracellular electrophysiology file with an electrodes table and data from voltage traces. Note also the subject-specific information, which is required by DANDI, and is read and used throughout the data publication process.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#nwb-tutorials","title":"NWB tutorials\u00b6","text":"PyNWB MatNWB Reading NWB files Jupyter notebook 15 min videoMATLAB Live Script Writing extracellular electrophysiology 23 min videoJupyter notebook 46 min videoWritten tutorial Writing intracellular electrophysiology Jupyter notebook Written tutorial Writing optical physiology 31 min videoJupyter notebook 39 min videoWritten tutorial Advanced write 26 min video 16 min videoWritten tutorial <p>If you think NWB might not meet your needs, please contact us on our help desk!</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#uploading-to-dandi","title":"Uploading to DANDI\u00b6","text":"<p>When you register a dandiset, it creates a permanent ID. For instructional purposes, we will be using a staging version of DANDI, so that we do not create real IDs for pretend datasets.</p> <p>We are going to use a staging version of DANDI.</p> <ol> <li><p>Go to DANDI staging (https://gui-staging.dandiarchive.org/) and use your GitHub account to log in.</p> </li> <li><p>Click on your initials in the upper right and copy your API key.</p> </li> </ol> <p></p> <ol> <li>Now create a new dataset by clicking the NEW DATASET button</li> </ol> <p></p> <ol> <li>Create a new Launcher</li> </ol> <p></p> <p>and launch a Terminal window.</p> <p></p> <ol> <li>Assign your DANDI API key as an environmental variable:</li> </ol> <pre>export DANDI_API_KEY=your-key-here\n</pre> <p>should look like</p> <pre>export DANDI_API_KEY=d71c0db17827aac067896f612f48af667890000\n</pre> <p>Confirm that this worked with the following line, which should print your key.</p> <pre>echo $DANDI_API_KEY\n</pre> <ol> <li>Copy the URL from the dataset you have created (something like <code>https://gui-staging.dandiarchive.org/#/dandiset/100507</code>)</li> </ol> <p>and in the Terminal window, run</p> <pre>dandi download \"https://gui-staging.dandiarchive.org/#/dandiset/100507\"  # &lt;-- your dandiset ID here\ncd 100507 # &lt;-- your dandiset ID here\ndandi organize ../data -f dry\ndandi organize ../data\ndandi upload -i dandi-staging  # (on the non-staging server, this line would simply be: dandi upload)\n</pre> <ol> <li>Now refresh the landing page of your dandiset and you should see that it is no longer empty.</li> </ol> <p></p> <p>You can explore all the NWB files within by clicking Files</p> <p></p> <ol> <li>Adding more data to a dandiset</li> </ol>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#visualization-nwb-files","title":"Visualization NWB files\u00b6","text":"<p>NWB Widgets is a library of interactive data visualizations that works automatically with any NWB file. This can be very useful for visually confirming any conversion.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/","title":"DANDI User Guide, Part II","text":"In\u00a0[1]: Copied! <pre>!dandi --help\n</pre> !dandi --help <pre>Usage: dandi [OPTIONS] COMMAND [ARGS]...\n\n  A client to support interactions with DANDI archive\n  (http://dandiarchive.org).\n\n  To see help for a specific command, run\n\n      dandi COMMAND --help\n\n  e.g. dandi upload --help\n\nOptions:\n  --version\n  -l, --log-level [DEBUG|INFO|WARNING|ERROR|CRITICAL]\n                                  Log level (case insensitive).  May be\n                                  specified as an integer.  [default: INFO]\n  --pdb                           Fall into pdb if errors out\n  --help                          Show this message and exit.\n\nCommands:\n  delete            Delete dandisets and assets from the server.\n  digest            Calculate file digests\n  download          Download a file or entire folder from DANDI\n  instances         List known Dandi Archive instances that the CLI can...\n  ls                List .nwb files and dandisets metadata.\n  organize          (Re)organize files according to the metadata.\n  shell-completion  Emit shell script for enabling command completion.\n  upload            Upload dandiset (files) to DANDI archive.\n  validate          Validate files for NWB (and DANDI) compliance.\n</pre> <p>which provides you with the overall syntax for using the <code>dandi</code> CLI, and lists commands and common options which could be specified right after <code>dandi</code> and before any particular <code>COMMAND</code>.</p> <p>More information on a particular command could be obtained by adding <code>--help</code> after the <code>COMMAND</code>, e.g.:</p> In\u00a0[2]: Copied! <pre>!dandi ls --help\n</pre> !dandi ls --help <pre>Usage: dandi ls [OPTIONS] [PATHS]...\n\n  List .nwb files and dandisets metadata.\n\n  Patterns for known setups:\n   - DANDI:&lt;dandiset id&gt;\n   - https://dandiarchive.org/...\n   - https://identifiers.org/DANDI:&lt;dandiset id&gt;\n   - https://&lt;server&gt;[/api]/[#/]dandiset/&lt;dandiset id&gt;[/&lt;version&gt;][/files[?location=&lt;path&gt;]]\n   - https://*dandiarchive-org.netflify.app/...\n   - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;[/versions[/&lt;version&gt;]]\n   - https://&lt;server&gt;[/api]/assets/&lt;asset id&gt;[/download]\n   - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/&lt;asset id&gt;[/download]\n   - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/?path=&lt;path&gt;\n   - dandi://&lt;instance name&gt;/&lt;dandiset id&gt;[@&lt;version&gt;][/&lt;path&gt;]\n   - https://&lt;server&gt;/...\n\nOptions:\n  -F, --fields TEXT               Comma-separated list of fields to display.\n                                  An empty value to trigger a list of\n                                  available fields to be printed out\n  -f, --format [auto|pyout|json|json_pp|json_lines|yaml]\n                                  Choose the format/frontend for output. If\n                                  'auto', 'pyout' will be used in case of\n                                  multiple files, and 'yaml' for a single\n                                  file.\n  -r, --recursive                 Recurse into content of\n                                  dandisets/directories. Only .nwb files will\n                                  be considered.\n  -J, --jobs INTEGER              Number of parallel download jobs.  [default:\n                                  6]\n  --metadata [api|all|assets]\n  --schema VERSION                Convert metadata to new schema version\n  --help                          Show this message and exit.\n</pre> <p>which as you can see has a number of options which could become handy.</p> <p>Let's try the <code>dandi ls</code> command right away on the dandiset you created in the previous section.</p> <ol> <li>In the Terminal (recommendation - make it wide) run</li> </ol> <pre>dandi ls -r data/100507  # &lt;-- path to your dandiset here\n</pre> <p>which should present you with a tabular view of metadata for dandiset and the asset(s) you have in it.</p> <ol> <li><p>Such view could be quite \"busy\". To provide a more useful/targeted listing of data at hand, use the <code>-F</code> option to only view a subset of fields, e.g. add <code>-F age,session_id</code>.</p> </li> <li><p>Try <code>-f</code> to change the format of the output (e.g., from tabular to YAML).</p> </li> </ol> <p>Note: if you point <code>ls</code> to a single file, by default it would produce YAML output.</p> <ol> <li>As the <code>dandi ls --help</code> output suggested, it can also operate on remote dandisets available from DANDI archive. When pointed to a remote URL though, it outputs information about the assets as known to the archive, and places metadata into a <code>metadata</code> key.  In the Terminal, try</li> </ol> <pre>dandi ls -r -f yaml --metadata all DANDI:000037\n</pre> <p>this will list the top-level metadata of the dandiset, as well as metadata for each individual asset.</p> <p>Note: As you can see from the above invocation, <code>ls</code> (as well, as <code>download</code>) supports URLs which you can simply copy/paste from the browser while navigating https://gui.dandiarchive.org .</p> <ol> <li>You could also point to a specific folder, e.g. while navigating it in the web UI:</li> </ol> <p></p> <p>Copy/paste the URL you see in the browser address bar to your <code>dandi ls</code> invocation in the Terminal.</p> <ol> <li>You can also download individual files if you copy the URL from the browser's context menu with the download icon:</li> </ol> <p></p> <p>and paste it into <code>dandi ls</code> invocation.</p> <p>NOTE: When embedding URLs into your analysis scripts, we strongly recommend using persistent URLs, such as URLs to the API server, <code>DANDI:&lt;id&gt;</code> identifiers, or <code>dandi://&lt;instance name&gt;/&lt;dandiset id&gt;[@&lt;version&gt;][/&lt;path&gt;]</code>. When citing dandisets in publications it is best to use DOIs.</p> <p>To try this out, run</p> <pre>dandi ls --metadata all https://api.dandiarchive.org/api/assets/834a2598-927c-4d56-91c6-92eeb9ef005c/download/\n</pre> <p>you can also try out some other files (AKA assets) in the archive.</p> In\u00a0[3]: Copied! <pre>!dandi download --help\n</pre> !dandi download --help <pre>Usage: dandi download [OPTIONS] [URL]...\n\n  Download a file or entire folder from DANDI\n\nOptions:\n  -o, --output-dir DIRECTORY      Directory where to download to (directory\n                                  must exist). Files will be downloaded with\n                                  paths relative to that directory.\n  -e, --existing [error|skip|overwrite|overwrite-different|refresh]\n                                  What to do if a file found existing locally.\n                                  'refresh': verify that according to the size\n                                  and mtime, it is the same file, if not -\n                                  download and overwrite.  [default: error]\n  -f, --format [pyout|debug]      Choose the format/frontend for output. TODO:\n                                  support all of the ls\n  -J, --jobs INTEGER              Number of parallel download jobs.  [default:\n                                  6]\n  --download [assets,dandiset.yaml,all]\n                                  Comma-separated list of elements to download\n                                  [default: all]\n  --sync                          Delete local assets that do not exist on the\n                                  server\n  -i, --dandi-instance [dandi|dandi-api-local-docker-tests|dandi-devel|dandi-staging]\n                                  DANDI instance to use\n  --help                          Show this message and exit.\n</pre> <p>Please review the options. And although it does not say it yet, the <code>download</code> command supports all those URL patterns which you saw listed by <code>dandi ls</code>. <code>dandi download</code> will provide you with them if you enter some unrecognized URL, e.g.:</p> In\u00a0[4]: Copied! <pre>!dandi download from-the-ether\n</pre> !dandi download from-the-ether <pre>2021-11-01 15:06:46,026 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20211101150645Z-554.log\nError: We do not know how to map URL from-the-ether to our servers.\nPatterns for known setups:\n - DANDI:&lt;dandiset id&gt;\n - https://dandiarchive.org/...\n - https://identifiers.org/DANDI:&lt;dandiset id&gt;\n - https://&lt;server&gt;[/api]/[#/]dandiset/&lt;dandiset id&gt;[/&lt;version&gt;][/files[?location=&lt;path&gt;]]\n - https://*dandiarchive-org.netflify.app/...\n - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;[/versions[/&lt;version&gt;]]\n - https://&lt;server&gt;[/api]/assets/&lt;asset id&gt;[/download]\n - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/&lt;asset id&gt;[/download]\n - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/?path=&lt;path&gt;\n - dandi://&lt;instance name&gt;/&lt;dandiset id&gt;[@&lt;version&gt;][/&lt;path&gt;]\n - https://&lt;server&gt;/...\n</pre> <p>so you can download an entire dandiset, with all the assets if you provide a resource identifier (e.g., <code>DANDI:000027</code>) or a URL to that dandiset as you copy paste it from the browser (e.g. https://gui.dandiarchive.org/#/dandiset/000027/).</p> <p>If a URL points to the staging DANDI archive, <code>dandi download</code> will interact with that server.</p> <p>Keeping in mind the options and URL patterns listed above, and try the following out in a Terminal:</p> <ol> <li>Download the entire 000027 dandiset (from the main archive).</li> </ol> <p>Note: if a dandiset was already published, and the version is not contained in the URL, <code>download</code> will download the most recent release, and not the \"draft\" version</p> <ol> <li><p>Download a <code>draft</code> version of 000027 into some other folder.</p> </li> <li><p>(\"optional\"- bonus point) What is different between draft and published (0.210831.2033) version of the dandiset?</p> <p>Hint: <code>diff -Naur folder1/ folder2/</code> could be used in the Terminal to find an answer.</p> </li> <li><p>Download the <code>sub-anm369962/</code> folder from <code>000006</code> dandiset.</p> </li> </ol> In\u00a0[5]: Copied! <pre>!dandi validate --help\n</pre> !dandi validate --help <pre>Usage: dandi validate [OPTIONS] [PATHS]...\n\n  Validate files for NWB (and DANDI) compliance.\n\n  Exits with non-0 exit code if any file is not compliant.\n\nOptions:\n  --help  Show this message and exit.\n</pre> In\u00a0[7]: Copied! <pre>!dandi validate ../data/ecephys_example.nwb  # &lt;-- put path to the .nwb files you want to validate here\n</pre> !dandi validate ../data/ecephys_example.nwb  # &lt;-- put path to the .nwb files you want to validate here <pre>2021-11-01 15:08:01,558 [    INFO] Note: detected 72 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n2021-11-01 15:08:01,559 [    INFO] Note: NumExpr detected 72 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n2021-11-01 15:08:01,559 [    INFO] NumExpr defaulting to 8 threads.\n2021-11-01 15:08:03,211 [    INFO] ../data/ecephys_example.nwb: ok\nSummary: No validation errors among 1 file(s)\n2021-11-01 15:08:03,211 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20211101150800Z-724.log\n</pre> In\u00a0[8]: Copied! <pre># enter your Python code here\n</pre> # enter your Python code here <ol> <li>Following the original example and/or documentation, get an object representing the dandiset of interest to you (e.g. <code>000006</code>) and download one of the assets using Python code:</li> </ol> In\u00a0[9]: Copied! <pre># enter your Python code here\n</pre> # enter your Python code here <p>All components (such as the https://gui.dandiarchive.org web interfiace) and tools (such as the <code>dandi</code> CLI and Python library) interact with the archive via the DANDI API (Application Programming Interface), which is provided by the <code>https://api.dandiarchive.org</code> server. This API server is the \"Heart\" of the archive, which manages all dandisets and assets in the archive and orchestrates access and deposition of data to AWS S3 bucket.</p> <p>All code of the DANDI API server is available openly from https://github.com/dandi/dandi-api/ . A number of convenient interfaces are available to help you learn about what features it provides, and how to interact with the server.</p> <p>Both production and staging servers have a Swagger interface, which you can reach by going to https://api.dandiarchive.org/swagger/ for production (and https://api-staging.dandiarchive.org/swagger/ for staging, which is where your test datasets are):</p> <p></p> <p>Read-only interaction with the archive (such as listing dandisets, their assets, etc.) does not even require authentication, and in the following brief exercises we will interact with the API server directly in the Terminal, but such interactions could be coded virtually in any programming language.</p> <p>Note: For the production server we also have https://dandi.readme.io , which provides even better UI and code snippets in wide range of languages.</p> <ol> <li>List dandisets known to the archive</li> </ol> <ul> <li>Find \"GET <code>/dandisets/</code>\" end point in swagger interface</li> <li>Click on that row to expand it down and reveal options for that call</li> <li>Click on \"Try it out\" button, possibly scroll down after the options, until you see \"Execute\" button</li> <li>Click on \"Execute\"</li> </ul> <p>After a short while you will see the response in its entirety below, alongside a curl invocation which you can copy and paste into your Terminal to execute, and to obtain a similar result.</p> <ol> <li>List assets \"under a path\"</li> </ol> <ul> <li>Choose a dandiset ID (<code>versions__dandiset__pk</code> within Swagger interface for <code>/assets</code> end points), e.g. <code>000006</code> you would like to list assets for</li> <li>Find end point for function <code>dandisets_versions_assets_list</code></li> <li>If you do not enter any value any value for the <code>path</code> option -- all assets of the dandiset will be listed. If you enter some path -- all assets under that path (including immediately in that directory or any subdirectory below) will be listed.</li> <li>For dandiset version (<code>versions__version</code> within Swagger interface) you can enter specific existing version of the dandiset (if was published) or \"draft\" version</li> <li>Click on \"Execute\"</li> </ul> <p>Although these exercises are simplistic, and typically you would not interact with the archive through the API (but rather use the <code>dandi</code> CLI or Python library), we hope understanding that all operations could also be programmed in any language of your choice can encourage you to interface with the DANDI archive across all platforms, software or web applications you work on.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#part-ii-interacting-with-the-archive-in-different-ways","title":"Part II, Interacting with the archive \"in different ways\"\u00b6","text":"<p>This is part 4 of the DANDI User Training on Nov 1, 2021.</p> <p>In the previous parts you have already briefly interacted with the archive via the Web UI, the <code>dandi</code> command line interface (CLI), and have even used the <code>dandi</code> Python library to access sample files directly from the Python interpreter.</p> <p>Note: Both the CLI and the Python library are provided by the dandi package on PyPI (and Conda), with the underlying codebase being available on GitHub as https://github.com/dandi/dandi-cli/.</p> <p>In this part we will explore more of the <code>dandi</code> CLI/Python library functionality, and introduce you to the DANDI acrhive API server, which allows you to interact with the archive directly from the CLI or programming language of your choice (e.g., MATLAB).</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-command-line-interface","title":"dandi Command Line interface\u00b6","text":"<p>The DANDI Handbook provides a brief introduction to various functions of the <code>dandi</code> CLI, which we will practice using in this section.</p> <p>As with any sane command line tool, <code>dandi</code> provides brief documentation on its usage if you add <code>--help</code> to its invocation:</p> <p>Note: in the code cell below you see commands prefixed with <code>!</code>. This annotation instructs the Jupyter notebook to run that command in bash. You could achieve the same effect by running that command in the Terminal, without typing leading <code>!</code>.  In the exercises below please run these commands in the Terminal.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-download-download-dandisetsfoldersfiles-from-the-archive","title":"dandi download - Download dandisets/folders/files from the archive\u00b6","text":"<p><code>dandi download</code> is probably the most frequently used command by a \"data consumer\" user interested primarily in data download.</p> <p>Let's first see which options it provides:</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-validate-validate-nwb-files-before-upload","title":"dandi validate - Validate NWB files before upload\u00b6","text":"<p>is a very useful command for any \"data producer\". As we have mentioned in the previous portion of the User Guide, all data uploaded to the DANDI archive must follow established standards such as NWB and BIDS. To ensure standard compliance <code>dandi upload</code> validates all files intended for upload and skips (by default) any file which fails validation.</p> <p><code>dandi validate</code> should be used before the upload of data to ensure that <code>.nwb</code> files do not have any internal NWB inconsistencies, and contain metadata required by DANDI archive. At the moment, the interface of the <code>validate</code> command is very trivial:</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-python-library","title":"dandi Python library\u00b6","text":"<p>The <code>dandi</code> command line interface we have practiced with above is a part of the <code>dandi</code> Python package, which also provides Python interfaces to interact with any instance of the DANDI archive (hint: the <code>dandi instances</code> command will list known instances of the archive).</p> <p>In the previous section you already used the library in the following Python code snippet:</p> <pre>from dandi.dandiapi import DandiAPIClient\n\nwith DandiAPIClient() as client:\n    asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\n    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n</pre> <p>https://dandi.readthedocs.io provides documentation not only on CLI, but also on Python interfaces.</p> <p>dandi.dandiapi module used in the snippet above provides high level interfaces which you can use in your scripts or applications.</p> <ol> <li>There is an example at the start of the the dandi.dandiapi documentation. Copy/paste and edit, or enter it in the cell below, and modify it to just list all dandisets in the main instance of the archive:</li> </ol> <p>Hint: You can \"run\" the code by pressing \"Shift-Enter\" or clicking on \"Run\" button in the menu.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-api-server","title":"DANDI API Server\u00b6","text":""},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#datalad-dandisets","title":"DataLad dandisets\u00b6","text":"<p>If you like <code>git</code> and/or are interested in using a data management system to take control over your code, data, and computing environment, DataLad provides a solution. A recent DataLad paper in JOSS provides a concise introduction and overview of the features, the DataLad Handbook is a trove of knowledge about DataLad, and the DataLad YouTube channel has many informational videos and tutorials.</p> <p>In a nutshell: All dandisets in the main DANDI archive (not staging) are also made available as DataLad datasets from GitHub: https://github.com/dandisets .</p> <p>Note: they are updated regularly but not in real time, so it takes up to a day for the most recent changes to be propagated.</p> <p>One of the very convenient features of DataLad is the ability to provide a \"view\" of the entire \"tree\" of files in the dandiset (locally, and without downloading all of its content at once). All files under DataLad control are unambiguously version controlled (it is all git and git-annex underneath). But even beyond the reliability advantages of version control, DataLad makes it easy to <code>get</code>, or later <code>drop</code>, any needed content without figuring out where any particular file needs to be downloaded from.</p> <p>For a brief example, let's quickly \"install\" the 000026 dandiset, the size of which is over 11TB if downloaded in full, and <code>get</code> and <code>drop</code> some files.</p> <ol> <li>In the Terminal run</li> </ol> <pre>datalad install https://github.com/dandisets/000026\ncd 000026\n</pre> <ol> <li><p>Use regular <code>cd</code> and <code>ls</code> commands to navigate through the dataset.</p> </li> <li><p>Use <code>datalad get PATHs</code> to get content for file(s) or folder(s) of interest.</p> </li> <li><p>Use <code>datalad drop PATHs</code> to drop the content and reclaim local disk space.</p> </li> </ol> <p>A complete example on a tiny test dandiset.  You are welcome to try on other dandisets:</p> <pre>datalad install https://github.com/dandisets/000027\ncd 000027\ndatalad get sub-RAT123/\n# now you have access to the files under that directory - can use `dandi ls` etc\n# and after you are done working with that data, you are ready to drop the content\ndatalad drop sub-RAT123/\n</pre>"},{"location":"tutorials/NWBWidget-demo/","title":"NWBWidget demo","text":"In\u00a0[\u00a0]: Copied! <pre>import pynwb\nfrom pynwb import NWBHDF5IO\nfrom nwbwidgets import nwb2widget\nimport requests\n\ndef _search_assets(url, filepath):\n    response = requests.request(\"GET\", url, headers={\"Accept\": \"application/json\"}).json() \n    \n    for asset in response[\"results\"]:\n        if filepath == asset[\"path\"]:\n            return asset[\"asset_id\"]\n    \n    if response.get(\"next\", None):\n        return _search_assets(response[\"next\"], filepath)\n    \n    raise ValueError(f'path {filepath} not found in dandiset {dandiset_id}.')\n\n\ndef get_asset_id(dandiset_id, filepath):\n    url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/\"\n    return _search_assets(url, filepath)\n\ndef get_s3_url(dandiset_id, filepath):\n    \"\"\"Get the s3 location for any NWB file on DANDI\"\"\"\n\n    asset_id = get_asset_id(dandiset_id, filepath)\n    url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\"\n    \n    s3_url = requests.request(url=url, method='head').url\n    if '?' in s3_url:\n        return s3_url[:s3_url.index('?')]\n    return s3_url\n</pre> import pynwb from pynwb import NWBHDF5IO from nwbwidgets import nwb2widget import requests  def _search_assets(url, filepath):     response = requests.request(\"GET\", url, headers={\"Accept\": \"application/json\"}).json()           for asset in response[\"results\"]:         if filepath == asset[\"path\"]:             return asset[\"asset_id\"]          if response.get(\"next\", None):         return _search_assets(response[\"next\"], filepath)          raise ValueError(f'path {filepath} not found in dandiset {dandiset_id}.')   def get_asset_id(dandiset_id, filepath):     url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/\"     return _search_assets(url, filepath)  def get_s3_url(dandiset_id, filepath):     \"\"\"Get the s3 location for any NWB file on DANDI\"\"\"      asset_id = get_asset_id(dandiset_id, filepath)     url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\"          s3_url = requests.request(url=url, method='head').url     if '?' in s3_url:         return s3_url[:s3_url.index('?')]     return s3_url In\u00a0[\u00a0]: Copied! <pre># calcium imaging, Giocomo Lab (30 GB)\ndandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"\n\n# neuropixel, Giocomo Lab (46 GB)\n#dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"\n\ns3_path = get_s3_url(dandiset_id, filepath)\n</pre> # calcium imaging, Giocomo Lab (30 GB) dandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"  # neuropixel, Giocomo Lab (46 GB) #dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"  s3_path = get_s3_url(dandiset_id, filepath) <p>Note that these NWB files are quite large. In fact, we have chosen NWB files that contain raw data to demonstrate how data streaming can efficiently deal with these large files. Streaming works efficiently with NWB Widgets so that only the data necessary to create each view is read from DANDI. As a result, data transfer is minimized and data can be explored efficiently.</p> In\u00a0[\u00a0]: Copied! <pre># use the \"Read Only S3\" (ros3) driver to stream data directly from DANDI (or any other S3 location)\nio = NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3')\n\nnwb = io.read()\nnwb2widget(nwb)\n</pre> # use the \"Read Only S3\" (ros3) driver to stream data directly from DANDI (or any other S3 location) io = NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3')  nwb = io.read() nwb2widget(nwb) In\u00a0[\u00a0]: Copied! <pre># icephys, optogenetics, and behavior, Svoboda Lab (632 MB)\ndandiset_id, filepath = \"000005\", \"sub-anm324650/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\"\nasset_id = get_asset_id(dandiset_id, filepath)\nprint(f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\")\n</pre> # icephys, optogenetics, and behavior, Svoboda Lab (632 MB) dandiset_id, filepath = \"000005\", \"sub-anm324650/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\" asset_id = get_asset_id(dandiset_id, filepath) print(f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\") <p>click the link that is output above ^^</p> In\u00a0[\u00a0]: Copied! <pre>import os.path as op\nio = NWBHDF5IO(op.expanduser(\"~/Downloads/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\"), mode='r', load_namespaces=True)\n\nnwb = io.read()\nnwb2widget(nwb)\n</pre> import os.path as op io = NWBHDF5IO(op.expanduser(\"~/Downloads/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\"), mode='r', load_namespaces=True)  nwb = io.read() nwb2widget(nwb) <p>When running with local data, NWBWidgets still only reads the data that is necessary from disk.</p>"},{"location":"tutorials/NWBWidget-demo/#nwb-widgets","title":"NWB Widgets\u00b6","text":"<p>NWB Widgets is a python package for automatic, interactive, performant exploration of data in NWB files. This notebook demonstrates how to use NWB Widgets to explore data, and how to stream data from the DANDI archive directly into NWB Widgets.</p> <p>NWB Widgets uses the metadata of the NWB file to understand the contents and infer what visualizations make sense for this data. The code is exactly the same for visualizing each different NWB filea. We demonstrate this here with one calcium imaging NWB file and one with Neuropixel extracellular electrophysiology.</p> <p>While this notebook can be run on a properly configured environment anywhere, it will be particularly easy to use and performant using the \"NWBstream\" environment deployed for free on DANDI Hub.</p>"},{"location":"tutorials/NWBWidget-demo/#running-nwbwidgets-locally","title":"Running NWBWidgets locally\u00b6","text":"<p>You can also download NWB files and run NWB Widgets locally by passing the local path (and omitting the ros3 driver flag). Here we will demonstrate this with a smaller intracellular electrophysiology dataset.</p>"},{"location":"tutorials/analysis-demo/","title":"Streaming and interacting with NWB data","text":"<p>First, let's import a few classes. If you are not running this notebook on DANDI Hub, you will need to install these packages using <code>pip</code> or your favorite Python package manager. For example:</p> <pre><code>pip install dandi pynwb fsspec requests aiohttp matplotlib pynapple seaborn\n</code></pre> In\u00a0[1]: Copied! <pre>from dandi.dandiapi import DandiAPIClient\nimport fsspec\nfrom fsspec.implementations.cached import CachingFileSystem\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pynwb\n</pre> from dandi.dandiapi import DandiAPIClient import fsspec from fsspec.implementations.cached import CachingFileSystem import h5py import matplotlib.pyplot as plt import numpy as np import pynwb <p>The data used in this tutorial were used in this publication: Sargolini, et al. \"Conjunctive representation of position, direction, and velocity in entorhinal cortex.\" Science 312.5774 (2006): 758-762. The data can be found on the DANDI Archive in Dandiset 000582.</p> In\u00a0[2]: Copied! <pre>dandiset_id = \"000582\"\nfilepath = \"sub-10073/sub-10073_ses-17010302_behavior+ecephys.nwb\"\nwith DandiAPIClient() as client:\n    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)\n    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n</pre> dandiset_id = \"000582\" filepath = \"sub-10073/sub-10073_ses-17010302_behavior+ecephys.nwb\" with DandiAPIClient() as client:     asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)     s3_url = asset.get_content_url(follow_redirects=1, strip_query=True) In\u00a0[3]: Copied! <pre>s3_url\n</pre> s3_url Out[3]: <pre>'https://dandiarchive.s3.amazonaws.com/blobs/26a/22c/26a22c31-09bc-43a4-9187-edc7394ed12c'</pre> <p>There are multiple methods to stream NWB files. We currently recommend fsspec, but additional options are described in the pynwb streaming tutorial.</p> In\u00a0[4]: Copied! <pre># create a virtual http filesystem with local caching\nfs = CachingFileSystem(\n    fs=fsspec.filesystem(\"http\"),\n    cache_storage=\"nwb-cache\",  # local folder for the cache\n)\n</pre> # create a virtual http filesystem with local caching fs = CachingFileSystem(     fs=fsspec.filesystem(\"http\"),     cache_storage=\"nwb-cache\",  # local folder for the cache ) In\u00a0[5]: Copied! <pre># open the file using the S3 URL\nf = fs.open(s3_url, \"rb\")\nfile = h5py.File(f)\nio = pynwb.NWBHDF5IO(file=file)\nnwbfile = io.read()\n</pre> # open the file using the S3 URL f = fs.open(s3_url, \"rb\") file = h5py.File(f) io = pynwb.NWBHDF5IO(file=file) nwbfile = io.read() <p>You can print the <code>NWBFile</code> object in a Jupyter notebook to get a simplified, interactive representation of the contents of the NWB file.</p> In\u00a0[6]: Copied! <pre>nwbfile\n</pre> nwbfile Out[6]: root (NWBFile)session_description: This session includes spike and position times for recorded cells from a Long Evans rat that was running in a 1 x 1 meter enclosure. The cells were recorded in the dorsocaudal 25% portion of the medial entorhinal cortex (MEC).Position is given for two LEDs to enable calculation of head direction.identifier: 294b7de1-a624-44d8-b1a1-28028dd2cf0csession_start_time1900-01-01 00:00:00+01:00timestamps_reference_time1900-01-01 00:00:00+01:00file_create_date02023-09-16 15:50:09.775622+02:00experimenter('Sargolini, Francesca',)related_publications('https://doi.org/10.1126/science.1125572',)acquisitionElectricalSeriesstarting_time: 0.0rate: 4800.0resolution: -1.0comments: no commentsdescription: The EEG signals from one electrode amplified 8000-10000 times, lowpass-filtered at 500 Hz (single pole), and stored at 4800 Hz (16 bits/sample).conversion: 1e-06offset: 0.0unit: voltsdatastarting_time_unit: secondselectrodesdescription: all electrodestabledescription: metadata about extracellular electrodestable location group group_name id 0 MEC ElectrodeGroup pynwb.ecephys.ElectrodeGroup at 0x5893589184\\nFields:\\n  description: The name of the ElectrodeGroup this electrode is a part of.\\n  device: EEG pynwb.device.Device at 0x5895127888\\nFields:\\n  description: The device used to record EEG signals.\\n\\n  location: MEC\\n ElectrodeGroup keywordsprocessingbehaviordescription: Processed behavioral data.data_interfacesPositionspatial_seriesSpatialSeriesLED1resolution: -1.0comments: no commentsdescription: Position (x, y) for the first tracking LED.conversion: 0.01offset: 0.0unit: metersdatatimestampstimestamps_unit: secondsinterval: 1reference_frame: (0,0) is not known.ecephysdescription: Processed electrical series data.data_interfacesLFPelectrical_seriesElectricalSeriesLFPstarting_time: 0.0rate: 250.0resolution: -1.0comments: no commentsdescription: The EEG signals from one electrode stored at 250 Hz.conversion: 1e-06offset: 0.0unit: voltsdatastarting_time_unit: secondselectrodesdescription: all electrodestabledescription: metadata about extracellular electrodestable location group group_name id 0 MEC ElectrodeGroup pynwb.ecephys.ElectrodeGroup at 0x5893589184\\nFields:\\n  description: The name of the ElectrodeGroup this electrode is a part of.\\n  device: EEG pynwb.device.Device at 0x5895127888\\nFields:\\n  description: The device used to record EEG signals.\\n\\n  location: MEC\\n ElectrodeGroup epoch_tagsset()electrodesdescription: metadata about extracellular electrodestable location group group_name id 0 MEC ElectrodeGroup pynwb.ecephys.ElectrodeGroup at 0x5893589184\\nFields:\\n  description: The name of the ElectrodeGroup this electrode is a part of.\\n  device: EEG pynwb.device.Device at 0x5895127888\\nFields:\\n  description: The device used to record EEG signals.\\n\\n  location: MEC\\n ElectrodeGroup electrode_groupsElectrodeGroupdescription: The name of the ElectrodeGroup this electrode is a part of.location: MECdevicedescription: The device used to record EEG signals.devicesEEGdescription: The device used to record EEG signals.subjectage: P3M/P5Mage__reference: birthdescription: A Long Evans rat.sex: Mspecies: Rattus norvegicussubject_id: 10073weight: 0.35/0.45unitsdescription: Autogenerated by NWBFilewaveform_unit: voltstable unit_name spike_times histology hemisphere depth id 0 t1c1 [0.7903958333333333, 0.794, 0.8111666666666667, 0.8313541666666666, 0.9217708333333333, 1.0205208333333333, 1.3573020833333334, 1.6583229166666666, 1.6768645833333333, 2.7457708333333333, 4.008697916666667, 4.01678125, 4.402270833333334, 4.522583333333333, 4.527708333333333, 5.598760416666667, 5.61415625, 5.617927083333333, 5.68934375, 5.701510416666666, 5.714885416666666, 5.71740625, 5.723197916666667, 5.806802083333333, 5.8149375, 5.8207708333333334, 6.1641875, 6.201979166666667, 6.2260625, 6.2363125, 6.3546875, 6.363916666666666, 6.480145833333333, 6.48803125, 6.899927083333333, 6.992489583333334, 6.995885416666667, 7.033135416666667, 7.098052083333333, 7.10146875, 7.105677083333333, 7.245541666666667, 11.069739583333334, 11.499979166666666, 11.5111875, 11.522375, 11.6165, 11.702270833333333, 11.714625, 11.7245, 11.920979166666667, 11.983385416666666, 11.98634375, 11.995645833333333, 12.08903125, 12.105385416666667, 12.148697916666666, 12.157114583333334, 12.165864583333333, 13.123729166666667, 13.206645833333333, 13.224875, 13.232479166666666, 13.362927083333334, 13.40890625, 13.522697916666667, 13.530177083333333, 13.54115625, 13.623927083333333, 13.685302083333333, 13.738260416666666, 13.757302083333334, 13.812510416666667, 13.855427083333334, 13.971072916666667, 13.98659375, 13.992822916666666, 14.082979166666666, 14.162229166666666, 14.165541666666666, 14.394229166666667, 14.397375, 14.488708333333333, 14.492708333333333, 14.500604166666667, 14.547833333333333, 14.563979166666666, 14.5875, 14.590729166666666, 14.59375, 14.601708333333333, 14.608729166666667, 14.620083333333334, 14.647083333333333, 14.699260416666666, 14.70190625, 14.741635416666666, 14.801010416666667, 14.80403125, 14.88940625, ...] MEC LII 0.0024 1 t2c1 [1.0451354166666667, 1.7003854166666668, 2.3154375, 11.046822916666667, 14.239729166666667, 14.822927083333333, 14.837010416666667, 19.281322916666667, 19.585395833333333, 19.603958333333335, 19.719083333333334, 19.722625, 19.819458333333333, 19.822979166666666, 19.8256875, 19.829520833333333, 28.67203125, 29.932614583333333, 30.8508125, 30.951416666666667, 31.050583333333332, 31.106822916666665, 31.410760416666665, 36.44560416666667, 46.90858333333333, 47.34158333333333, 47.397375, 47.4190625, 47.47598958333333, 48.156, 59.66520833333333, 59.66970833333333, 59.78719791666666, 60.00934375, 60.12197916666667, 60.12890625, 60.458333333333336, 60.8745625, 61.55807291666667, 62.76680208333333, 62.94321875, 63.27045833333333, 63.4875, 64.47296875, 67.56085416666667, 67.57622916666666, 67.72179166666666, 67.72525, 67.80897916666666, 67.814625, 67.8276875, 67.91458333333334, 67.94144791666666, 67.95988541666667, 68.02155208333333, 68.13059375, 68.20140625, 68.33521875, 68.40710416666667, 68.57409375, 68.58878125, 68.681375, 68.70475, 68.75991666666667, 68.832625, 68.9400625, 68.94379166666667, 71.02867708333333, 71.25821875, 71.41845833333333, 72.28534375, 78.7965, 79.7308125, 79.76083333333334, 79.84297916666667, 79.91891666666666, 79.97095833333333, 80.08979166666667, 83.240875, 88.275625, 91.66017708333334, 91.67665625, 92.069125, 92.10191666666667, 92.56378125, 92.56932291666666, 92.57228125, 92.58971875, 92.59544791666667, 92.67121875, 92.67442708333333, 92.81330208333334, 92.84265625, 93.16694791666667, 93.28475, 93.47997916666667, 93.49264583333333, 93.51422916666667, 93.51847916666667, 93.52220833333334, ...] MEC LII 0.0024 2 t2c3 [0.18273958333333334, 0.5340729166666667, 0.5707291666666666, 0.7023333333333334, 0.7242604166666666, 0.7670520833333333, 0.88084375, 1.06084375, 1.1815, 1.2828333333333333, 1.2922916666666666, 1.4162291666666667, 1.5238333333333334, 1.5430416666666666, 1.5909479166666667, 1.6242291666666666, 1.6451041666666666, 1.7132604166666667, 3.0415729166666665, 5.295635416666666, 5.516947916666667, 6.2452604166666665, 6.336885416666667, 6.6401666666666666, 7.0116875, 7.089395833333334, 7.20128125, 8.1189375, 8.1546875, 8.251333333333333, 8.605614583333333, 10.502302083333333, 10.848864583333333, 10.931302083333334, 11.852802083333334, 11.86840625, 11.877864583333333, 11.963645833333333, 13.12328125, 13.53028125, 13.688145833333333, 13.751333333333333, 14.159802083333334, 16.810760416666668, 16.91994791666667, 17.323583333333332, 17.454822916666668, 19.2395, 19.306625, 19.73028125, 21.200885416666665, 21.439739583333335, 22.102395833333333, 22.926447916666667, 23.722489583333335, 23.85609375, 25.524104166666667, 25.860020833333333, 26.32103125, 26.6210625, 26.66503125, 26.753708333333332, 26.846958333333333, 26.8845, 26.959229166666667, 27.08540625, 27.1913125, 27.271979166666668, 27.31240625, 27.537260416666665, 27.791447916666666, 27.805697916666666, 27.817010416666665, 27.90071875, 27.917572916666668, 28.02446875, 28.13378125, 28.15521875, 28.32228125, 28.363958333333333, 28.38533333333333, 28.608604166666666, 28.6311875, 29.11065625, 30.226875, 33.932625, 34.056666666666665, 34.590354166666664, 36.890760416666666, 38.448458333333335, 38.4585, 38.78067708333333, 39.00878125, 39.022072916666666, 39.23472916666667, 39.24640625, 39.25477083333333, 39.60910416666667, 40.26836458333333, 40.29759375, ...] MEC LII 0.0024 3 t3c1 [1.0358229166666666, 1.04803125, 1.6964270833333333, 1.7780416666666667, 1.7842083333333334, 1.8619166666666667, 1.8659375, 1.879125, 1.8885416666666666, 1.9763333333333333, 2.0486666666666666, 2.1341875, 2.1698541666666666, 2.2325416666666666, 2.245875, 2.251375, 2.2590625, 2.2775833333333333, 2.282395833333333, 2.2996041666666667, 2.3334375, 2.3404375, 2.3472708333333334, 2.3584375, 2.380541666666667, 2.3935104166666665, 2.40184375, 2.4201979166666665, 2.44096875, 2.45184375, 2.4599270833333335, 2.4681770833333334, 2.4746770833333334, 2.4855729166666665, 2.5209479166666666, 2.55503125, 2.5716979166666665, 2.5834895833333333, 2.591552083333333, 2.6127604166666667, 2.6709895833333333, 2.88928125, 3.134916666666667, 3.2006875, 3.3154375, 3.4335625, 4.24340625, 4.2707395833333335, 4.34684375, 14.862104166666667, 15.2978125, 15.543479166666666, 16.012020833333334, 16.19559375, 16.322864583333335, 16.855166666666666, 16.8641875, 19.730333333333334, 19.756604166666666, 19.76535416666667, 19.8773125, 19.9308125, 20.047833333333333, 20.227822916666668, 20.318989583333334, 20.336260416666665, 20.47409375, 20.584364583333333, 20.78321875, 20.79134375, 20.810072916666666, 20.824833333333334, 20.8931875, 20.906208333333332, 20.918354166666667, 20.9286875, 21.020104166666666, 21.109541666666665, 21.1209375, 21.129208333333334, 21.139520833333332, 28.955604166666667, 28.969427083333333, 29.12302083333333, 29.227, 29.93634375, 30.745760416666666, 30.862416666666668, 33.691541666666666, 33.70033333333333, 33.70483333333333, 45.592125, 48.75952083333333, 52.425875, 52.44083333333333, 53.031697916666666, 53.03701041666667, 57.72391666666667, 57.736291666666666, 57.74427083333333, ...] MEC LII 0.0024 <p>... and 4 more rows.</p>experiment_description: The sample includes conjunctive cells and head direction cells from layers III and V of medial entorhinal cortex and have been published in Sargolini et al. (Science, 2006).session_id: 17010302lab: Moserinstitution: Centre for the Biology of Memory, Norwegian University of Science and Technology <p>Access <code>nwbfile.subject</code> to get information about the subject used in this experiment, including their age, sex, species, and ID. Age uses the ISO 8601 standard for time durations - <code>P3M</code> corresponds to 3 months old, and <code>P3M/P5M</code> means the subject was between 3-5 months old.</p> In\u00a0[7]: Copied! <pre>nwbfile.subject\n</pre> nwbfile.subject Out[7]: subject (Subject)age: P3M/P5Mage__reference: birthdescription: A Long Evans rat.sex: Mspecies: Rattus norvegicussubject_id: 10073weight: 0.35/0.45 <p>Now let's access the position of the animal, which is stored in a <code>SpatialSeries</code> object at this particular path within the NWB file.</p> In\u00a0[8]: Copied! <pre>position = nwbfile.processing[\"behavior\"][\"Position\"][\"SpatialSeriesLED1\"]\nposition\n</pre> position = nwbfile.processing[\"behavior\"][\"Position\"][\"SpatialSeriesLED1\"] position Out[8]: SpatialSeriesLED1 (SpatialSeries)resolution: -1.0comments: no commentsdescription: Position (x, y) for the first tracking LED.conversion: 0.01offset: 0.0unit: metersdatatimestampstimestamps_unit: secondsinterval: 1reference_frame: (0,0) is not known. In\u00a0[9]: Copied! <pre>position.data\n</pre> position.data Out[9]: <pre>&lt;HDF5 dataset \"data\": shape (30000, 2), type \"&lt;f8\"&gt;</pre> In\u00a0[10]: Copied! <pre>pos_data = position.data[:]\npos_data\n</pre> pos_data = position.data[:] pos_data Out[10]: <pre>array([[-0.4427946 , 17.10393108],\n       [-0.4427946 , 17.10393108],\n       [ 1.26778258, 16.85956291],\n       ...,\n       [ 2.00088708,  6.84046804],\n       [ 2.00088708,  6.84046804],\n       [ 0.77904624,  4.64115453]])</pre> In\u00a0[11]: Copied! <pre>x = position.data[:,0]\nx\n</pre> x = position.data[:,0] x Out[11]: <pre>array([-0.4427946 , -0.4427946 ,  1.26778258, ...,  2.00088708,\n        2.00088708,  0.77904624])</pre> In\u00a0[12]: Copied! <pre>ts = position.timestamps[:]\nts\n</pre> ts = position.timestamps[:] ts Out[12]: <pre>array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 5.9994e+02, 5.9996e+02,\n       5.9998e+02])</pre> <p>Now let's use those values to plot the X position of the subject over time. All times in NWB are stored in seconds relative to the session start time.</p> In\u00a0[13]: Copied! <pre>plt.plot(ts, x)\nplt.xlabel(\"Time (seconds)\")\nplt.ylabel(\"X coordinate of the subject\");\n</pre> plt.plot(ts, x) plt.xlabel(\"Time (seconds)\") plt.ylabel(\"X coordinate of the subject\"); <p>We can also slice in the time dimension. Let's plot the (x, y) position of the subject for the first 100 time points.</p> In\u00a0[14]: Copied! <pre>time_indices = slice(0, 100)\nfig, ax = plt.subplots()\nax.scatter(position.data[time_indices,0], position.data[time_indices,1], c=position.timestamps[time_indices])\nax.plot(position.data[time_indices,0], position.data[time_indices,1], color='k', zorder=0)\nax.set(xlabel=\"X coordinate\", ylabel=\"Y coordinate\")\n</pre> time_indices = slice(0, 100) fig, ax = plt.subplots() ax.scatter(position.data[time_indices,0], position.data[time_indices,1], c=position.timestamps[time_indices]) ax.plot(position.data[time_indices,0], position.data[time_indices,1], color='k', zorder=0) ax.set(xlabel=\"X coordinate\", ylabel=\"Y coordinate\") Out[14]: <pre>[Text(0.5, 0, 'X coordinate'), Text(0, 0.5, 'Y coordinate')]</pre> In\u00a0[15]: Copied! <pre>nwbfile.units\n</pre> nwbfile.units Out[15]: units (Units)description: Autogenerated by NWBFilewaveform_unit: voltstable unit_name spike_times histology hemisphere depth id 0 t1c1 [0.7903958333333333, 0.794, 0.8111666666666667, 0.8313541666666666, 0.9217708333333333, 1.0205208333333333, 1.3573020833333334, 1.6583229166666666, 1.6768645833333333, 2.7457708333333333, 4.008697916666667, 4.01678125, 4.402270833333334, 4.522583333333333, 4.527708333333333, 5.598760416666667, 5.61415625, 5.617927083333333, 5.68934375, 5.701510416666666, 5.714885416666666, 5.71740625, 5.723197916666667, 5.806802083333333, 5.8149375, 5.8207708333333334, 6.1641875, 6.201979166666667, 6.2260625, 6.2363125, 6.3546875, 6.363916666666666, 6.480145833333333, 6.48803125, 6.899927083333333, 6.992489583333334, 6.995885416666667, 7.033135416666667, 7.098052083333333, 7.10146875, 7.105677083333333, 7.245541666666667, 11.069739583333334, 11.499979166666666, 11.5111875, 11.522375, 11.6165, 11.702270833333333, 11.714625, 11.7245, 11.920979166666667, 11.983385416666666, 11.98634375, 11.995645833333333, 12.08903125, 12.105385416666667, 12.148697916666666, 12.157114583333334, 12.165864583333333, 13.123729166666667, 13.206645833333333, 13.224875, 13.232479166666666, 13.362927083333334, 13.40890625, 13.522697916666667, 13.530177083333333, 13.54115625, 13.623927083333333, 13.685302083333333, 13.738260416666666, 13.757302083333334, 13.812510416666667, 13.855427083333334, 13.971072916666667, 13.98659375, 13.992822916666666, 14.082979166666666, 14.162229166666666, 14.165541666666666, 14.394229166666667, 14.397375, 14.488708333333333, 14.492708333333333, 14.500604166666667, 14.547833333333333, 14.563979166666666, 14.5875, 14.590729166666666, 14.59375, 14.601708333333333, 14.608729166666667, 14.620083333333334, 14.647083333333333, 14.699260416666666, 14.70190625, 14.741635416666666, 14.801010416666667, 14.80403125, 14.88940625, ...] MEC LII 0.0024 1 t2c1 [1.0451354166666667, 1.7003854166666668, 2.3154375, 11.046822916666667, 14.239729166666667, 14.822927083333333, 14.837010416666667, 19.281322916666667, 19.585395833333333, 19.603958333333335, 19.719083333333334, 19.722625, 19.819458333333333, 19.822979166666666, 19.8256875, 19.829520833333333, 28.67203125, 29.932614583333333, 30.8508125, 30.951416666666667, 31.050583333333332, 31.106822916666665, 31.410760416666665, 36.44560416666667, 46.90858333333333, 47.34158333333333, 47.397375, 47.4190625, 47.47598958333333, 48.156, 59.66520833333333, 59.66970833333333, 59.78719791666666, 60.00934375, 60.12197916666667, 60.12890625, 60.458333333333336, 60.8745625, 61.55807291666667, 62.76680208333333, 62.94321875, 63.27045833333333, 63.4875, 64.47296875, 67.56085416666667, 67.57622916666666, 67.72179166666666, 67.72525, 67.80897916666666, 67.814625, 67.8276875, 67.91458333333334, 67.94144791666666, 67.95988541666667, 68.02155208333333, 68.13059375, 68.20140625, 68.33521875, 68.40710416666667, 68.57409375, 68.58878125, 68.681375, 68.70475, 68.75991666666667, 68.832625, 68.9400625, 68.94379166666667, 71.02867708333333, 71.25821875, 71.41845833333333, 72.28534375, 78.7965, 79.7308125, 79.76083333333334, 79.84297916666667, 79.91891666666666, 79.97095833333333, 80.08979166666667, 83.240875, 88.275625, 91.66017708333334, 91.67665625, 92.069125, 92.10191666666667, 92.56378125, 92.56932291666666, 92.57228125, 92.58971875, 92.59544791666667, 92.67121875, 92.67442708333333, 92.81330208333334, 92.84265625, 93.16694791666667, 93.28475, 93.47997916666667, 93.49264583333333, 93.51422916666667, 93.51847916666667, 93.52220833333334, ...] MEC LII 0.0024 2 t2c3 [0.18273958333333334, 0.5340729166666667, 0.5707291666666666, 0.7023333333333334, 0.7242604166666666, 0.7670520833333333, 0.88084375, 1.06084375, 1.1815, 1.2828333333333333, 1.2922916666666666, 1.4162291666666667, 1.5238333333333334, 1.5430416666666666, 1.5909479166666667, 1.6242291666666666, 1.6451041666666666, 1.7132604166666667, 3.0415729166666665, 5.295635416666666, 5.516947916666667, 6.2452604166666665, 6.336885416666667, 6.6401666666666666, 7.0116875, 7.089395833333334, 7.20128125, 8.1189375, 8.1546875, 8.251333333333333, 8.605614583333333, 10.502302083333333, 10.848864583333333, 10.931302083333334, 11.852802083333334, 11.86840625, 11.877864583333333, 11.963645833333333, 13.12328125, 13.53028125, 13.688145833333333, 13.751333333333333, 14.159802083333334, 16.810760416666668, 16.91994791666667, 17.323583333333332, 17.454822916666668, 19.2395, 19.306625, 19.73028125, 21.200885416666665, 21.439739583333335, 22.102395833333333, 22.926447916666667, 23.722489583333335, 23.85609375, 25.524104166666667, 25.860020833333333, 26.32103125, 26.6210625, 26.66503125, 26.753708333333332, 26.846958333333333, 26.8845, 26.959229166666667, 27.08540625, 27.1913125, 27.271979166666668, 27.31240625, 27.537260416666665, 27.791447916666666, 27.805697916666666, 27.817010416666665, 27.90071875, 27.917572916666668, 28.02446875, 28.13378125, 28.15521875, 28.32228125, 28.363958333333333, 28.38533333333333, 28.608604166666666, 28.6311875, 29.11065625, 30.226875, 33.932625, 34.056666666666665, 34.590354166666664, 36.890760416666666, 38.448458333333335, 38.4585, 38.78067708333333, 39.00878125, 39.022072916666666, 39.23472916666667, 39.24640625, 39.25477083333333, 39.60910416666667, 40.26836458333333, 40.29759375, ...] MEC LII 0.0024 3 t3c1 [1.0358229166666666, 1.04803125, 1.6964270833333333, 1.7780416666666667, 1.7842083333333334, 1.8619166666666667, 1.8659375, 1.879125, 1.8885416666666666, 1.9763333333333333, 2.0486666666666666, 2.1341875, 2.1698541666666666, 2.2325416666666666, 2.245875, 2.251375, 2.2590625, 2.2775833333333333, 2.282395833333333, 2.2996041666666667, 2.3334375, 2.3404375, 2.3472708333333334, 2.3584375, 2.380541666666667, 2.3935104166666665, 2.40184375, 2.4201979166666665, 2.44096875, 2.45184375, 2.4599270833333335, 2.4681770833333334, 2.4746770833333334, 2.4855729166666665, 2.5209479166666666, 2.55503125, 2.5716979166666665, 2.5834895833333333, 2.591552083333333, 2.6127604166666667, 2.6709895833333333, 2.88928125, 3.134916666666667, 3.2006875, 3.3154375, 3.4335625, 4.24340625, 4.2707395833333335, 4.34684375, 14.862104166666667, 15.2978125, 15.543479166666666, 16.012020833333334, 16.19559375, 16.322864583333335, 16.855166666666666, 16.8641875, 19.730333333333334, 19.756604166666666, 19.76535416666667, 19.8773125, 19.9308125, 20.047833333333333, 20.227822916666668, 20.318989583333334, 20.336260416666665, 20.47409375, 20.584364583333333, 20.78321875, 20.79134375, 20.810072916666666, 20.824833333333334, 20.8931875, 20.906208333333332, 20.918354166666667, 20.9286875, 21.020104166666666, 21.109541666666665, 21.1209375, 21.129208333333334, 21.139520833333332, 28.955604166666667, 28.969427083333333, 29.12302083333333, 29.227, 29.93634375, 30.745760416666666, 30.862416666666668, 33.691541666666666, 33.70033333333333, 33.70483333333333, 45.592125, 48.75952083333333, 52.425875, 52.44083333333333, 53.031697916666666, 53.03701041666667, 57.72391666666667, 57.736291666666666, 57.74427083333333, ...] MEC LII 0.0024 <p>... and 4 more rows.</p> <p>We can view the single unit data as a pandas <code>DataFrame</code>.</p> In\u00a0[16]: Copied! <pre>units_df = nwbfile.units.to_dataframe()\nunits_df\n</pre> units_df = nwbfile.units.to_dataframe() units_df Out[16]: unit_name spike_times histology hemisphere depth id 0 t1c1 [0.7903958333333333, 0.794, 0.8111666666666667... MEC LII 0.0024 1 t2c1 [1.0451354166666667, 1.7003854166666668, 2.315... MEC LII 0.0024 2 t2c3 [0.18273958333333334, 0.5340729166666667, 0.57... MEC LII 0.0024 3 t3c1 [1.0358229166666666, 1.04803125, 1.69642708333... MEC LII 0.0024 4 t3c2 [2.43025, 2.4398333333333335, 3.17965625, 3.39... MEC LII 0.0024 5 t3c3 [2.1157708333333334, 2.425427083333333, 3.3630... MEC LII 0.0024 6 t3c4 [0.07945833333333334, 2.244947916666667, 3.173... MEC LII 0.0024 7 t4c1 [2.4301666666666666, 2.439770833333333, 3.1795... MEC LII 0.0024 <p>To access the spike times of the first single unit, index this pandas dataframe with the column name, \u201cspike_times\u201d, and the row index, 0. All times in NWB are stored in seconds relative to the session start time.</p> In\u00a0[17]: Copied! <pre>units_df[\"spike_times\"][0]\n</pre> units_df[\"spike_times\"][0] Out[17]: <pre>array([  0.79039583,   0.794     ,   0.81116667, ..., 595.28703125,\n       595.53125   , 599.68578125])</pre> <p>We can use these spike times to generate raster plots of single unit activity over time.</p> In\u00a0[18]: Copied! <pre>fig, ax = plt.subplots()\nfor i, st in enumerate(units_df['spike_times']):\n    ax.plot(st, np.ones_like(st) + i, '|', markersize=20)\n    ax.set(xlabel='Time (s)', ylabel='Unit number')\n</pre> fig, ax = plt.subplots() for i, st in enumerate(units_df['spike_times']):     ax.plot(st, np.ones_like(st) + i, '|', markersize=20)     ax.set(xlabel='Time (s)', ylabel='Unit number') <p>We can also inspect the columns of the <code>Units</code> table for useful metadata such as descriptions of the values of each column.</p> In\u00a0[19]: Copied! <pre>nwbfile.units[\"depth\"].description\n</pre> nwbfile.units[\"depth\"].description Out[19]: <pre>'Indicates the depth of the inserted electrodes in meters.'</pre> <p>Pynapple is a light-weight python library for neurophysiological data analysis that accepts NWB files as input.</p> <p>Let's import pynapple and seaborn, set some seaborn plotting parameters, and load the streamed NWB file into Pynapple by creating a <code>nap.NWBFile</code> object using the <code>pynwb.NWBFile</code> object that we created earlier from <code>io.read()</code>.</p> In\u00a0[20]: Copied! <pre>import pynapple as nap\nimport seaborn as sns\n\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\n\nnwb = nap.NWBFile(nwbfile)\n\nnwb\n</pre> import pynapple as nap import seaborn as sns  custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False} sns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)  nwb = nap.NWBFile(nwbfile)  nwb Out[20]: <pre>17010302\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\n\u2502 Keys                \u2502 Type     \u2502\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\u2502 units               \u2502 TsGroup  \u2502\n\u2502 ElectricalSeriesLFP \u2502 Tsd      \u2502\n\u2502 SpatialSeriesLED1   \u2502 TsdFrame \u2502\n\u2502 ElectricalSeries    \u2502 Tsd      \u2502\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519</pre> <p>We can load the sorted units as a pynapple <code>TsGroup</code> for inspection.</p> In\u00a0[21]: Copied! <pre>units = nwb[\"units\"]\nunits\n</pre> units = nwb[\"units\"] units Out[21]: <pre>  Index    rate  unit_name    histology    hemisphere      depth\n-------  ------  -----------  -----------  ------------  -------\n      0    2.93  t1c1         MEC LII                     0.0024\n      1    1.5   t2c1         MEC LII                     0.0024\n      2    2.58  t2c3         MEC LII                     0.0024\n      3    1.13  t3c1         MEC LII                     0.0024\n      4    1.29  t3c2         MEC LII                     0.0024\n      5    1.36  t3c3         MEC LII                     0.0024\n      6    2.89  t3c4         MEC LII                     0.0024\n      7    1.47  t4c1         MEC LII                     0.0024</pre> <p>We can also load the position data, which is a pynapple <code>TsdFrame</code>.</p> In\u00a0[22]: Copied! <pre>position = nwb[\"SpatialSeriesLED1\"]\nposition\n</pre> position = nwb[\"SpatialSeriesLED1\"] position Out[22]: <pre>Time (s)            x         y\n----------  ---------  --------\n0.0         -0.442795  17.1039\n0.02        -0.442795  17.1039\n0.04         1.26778   16.8596\n0.06         1.26778   16.8596\n0.08        -0.198426  17.1039\n...\n599.9        2.24526    4.88552\n599.92       2.24526    4.88552\n599.94       2.00089    6.84047\n599.96       2.00089    6.84047\n599.98       0.779046   4.64115\ndtype: float64, shape: (30000, 2)</pre> <p>Next, let's compute the 2d tuning curves and plot them.</p> In\u00a0[23]: Copied! <pre>tc, binsxy = nap.compute_2d_tuning_curves(units, position, 20)\n\nextent = (\n    np.min(position[\"x\"]),\n    np.max(position[\"x\"]),\n    np.min(position[\"y\"]),\n    np.max(position[\"y\"]),\n)\n\nplt.figure(figsize=(15, 7))\nfor i in tc.keys():\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(tc[i], origin=\"lower\", extent=extent, aspect=\"auto\")\n    plt.title(\"Unit {}\".format(i))\nplt.tight_layout()\nplt.show()\n</pre> tc, binsxy = nap.compute_2d_tuning_curves(units, position, 20)  extent = (     np.min(position[\"x\"]),     np.max(position[\"x\"]),     np.min(position[\"y\"]),     np.max(position[\"y\"]), )  plt.figure(figsize=(15, 7)) for i in tc.keys():     plt.subplot(2, 4, i + 1)     plt.imshow(tc[i], origin=\"lower\", extent=extent, aspect=\"auto\")     plt.title(\"Unit {}\".format(i)) plt.tight_layout() plt.show() <pre>/Users/smprince/anaconda3/envs/test2/lib/python3.12/site-packages/pynapple/process/tuning_curves.py:223: RuntimeWarning: invalid value encountered in divide\n  count = count / occupancy\n</pre> <p>Finally, let's plot the spikes of unit 1, which has a nice grid. Let's use the function <code>value_from</code> to assign to each spike the closest position in time.</p> In\u00a0[24]: Copied! <pre>plt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.imshow(tc[1], origin=\"lower\", extent=extent, aspect=\"auto\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.subplot(122)\nplt.plot(position[\"y\"], position[\"x\"], color=\"grey\")\nspk_pos = units[1].value_from(position)\nplt.plot(spk_pos[\"y\"], spk_pos[\"x\"], \"o\", color=\"red\", markersize=5, alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(15, 6)) plt.subplot(121) plt.imshow(tc[1], origin=\"lower\", extent=extent, aspect=\"auto\") plt.xlabel(\"x\") plt.ylabel(\"y\")  plt.subplot(122) plt.plot(position[\"y\"], position[\"x\"], color=\"grey\") spk_pos = units[1].value_from(position) plt.plot(spk_pos[\"y\"], spk_pos[\"x\"], \"o\", color=\"red\", markersize=5, alpha=0.5) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.tight_layout() plt.show() <p>It is good practice to close any open file and IO objects when you are done working with them so that they can be modified by other processes.</p> In\u00a0[25]: Copied! <pre>io.close()\nfile.close()\nf.close()\n</pre> io.close() file.close() f.close() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/analysis-demo/#streaming-and-interacting-with-nwb-data-from-dandi","title":"Streaming and interacting with NWB data from DANDI\u00b6","text":""},{"location":"tutorials/analysis-demo/#lazy-loading-of-datasets","title":"Lazy loading of datasets\u00b6","text":"<p>Data arrays are read passively from the NWB file. Accessing the <code>data</code> attribute of the <code>SpatialSeries</code> object does not read the data values, but presents an <code>h5py.Dataset</code> object that can be indexed to read data. You can use the <code>[:]</code> operator to read the entire data array into memory.</p>"},{"location":"tutorials/analysis-demo/#slicing-datasets","title":"Slicing datasets\u00b6","text":"<p>Especially with very large datasets, it is often preferable to read only a portion of the data. To do this, index or slice into the <code>data</code> attribute just like if you were indexing or slicing a numpy array.</p> <p>Let's get the X coordinates of the subject for all timestamps and get the timestamps.</p>"},{"location":"tutorials/analysis-demo/#access-single-unit-data","title":"Access single unit data\u00b6","text":"<p>Data and metadata about sorted single units are stored in a <code>Units</code> object. It stores metadata about each single unit in a tabular form, where each row represents a unit with spike times and additional metadata.</p>"},{"location":"tutorials/analysis-demo/#using-pynapple-for-data-analysis","title":"Using Pynapple for data analysis\u00b6","text":""}]}