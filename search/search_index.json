{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the DANDI Archive Documentation","text":"<p>The Web interface to the DANDI archive is located at https://dandiarchive.org. This documentation explains how to interact with the archive.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<p>If you want to know more about the DANDI project, its goals, and the problems it tries to solve, check out the Introduction.</p> <p>To start using the archive, head over to Using DANDI in the User Guide section.</p> <p>If are a developer and want to know how the project is organized, check out the Project Structure page in the Developer Guide section.</p>"},{"location":"#where-to-get-help","title":"Where to Get Help","text":"<p>You can communicate with the DANDI team in a variety of ways, depending on your needs:</p> <ul> <li>You can ask questions, report bugs, or  request features at our helpdesk.</li> <li>For interacting with the global neuroscience community, post on https://neurostars.org and use the tag dandi.</li> <li>You can use the DANDI Slack workspace, which we will invite you to after approving your registration on    DANDI using GitHub (this registration is required to upload data or to use the DANDI    JupyterHub). See here for details on how to    register.</li> <li>Email us: info@dandiarchive.org</li> </ul>"},{"location":"#contributing-and-feedback","title":"Contributing and Feedback","text":"<p>We are looking for people to give us feedback on this documentation. If anything is unclear, open an issue on our repository. You can also get in touch on  our Slack channel, which is available to those who have registered an account on the archive.</p> <p>If you want to get started right away and contribute directly to this documentation, see the About This Documentation section.</p>"},{"location":"#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"01_introduction/","title":"Introduction","text":""},{"location":"01_introduction/#what-is-dandi","title":"What is DANDI?","text":"<p>DANDI is:</p> <ul> <li>An open data archive to submit neurophysiology data for electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments.</li> <li>A persistent, versioned, and growing collection of standardized datasets.</li> <li>A place to house data to collaborate across research sites.</li> <li>Supported by the BRAIN Initiative and the AWS Public dataset programs.</li> </ul> <p>DANDI provides significant benefits:</p> <ul> <li>A FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized neurophysiology and associated data.</li> <li>Rich metadata to support search across data.</li> <li>Consistent and transparent data standards to simplify data reuse and software development. We use the Neurodata Without Borders,  Brain Imaging Data Structure, Neuroimaging Data Model (NIDM), and other BRAIN Initiative standards to organize and search the data. See Data Standards for more information.</li> <li>The data can be accessed programmatically allowing for software to work directly with data in the cloud.</li> <li>The infrastructure is built on a software stack of open source products, thus enriching the ecosystem.</li> </ul>"},{"location":"01_introduction/#properties-of-dandi","title":"Properties of DANDI","text":"<p>Data identifiers: The archive provides persistent identifiers for versioned datasets and assets, thus improving reproducibility of neurophysiology research.</p> <p>Data storage: Cloud-based platform on AWS. Data are available from a public S3 bucket. Data from embargoed datasets are available from a private bucket to owners only.</p> <p>Type of data The archive accepts cellular neurophysiology data including electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments and other associated data (e.g. participant information, MRI or other modalities).</p> <p>Accepted Standards and Data File Formats: NWB (HDF5), BIDS (NIfTI, JSON, PNG, TIF, OME.TIF, OME.BTF, OME.ZARR)  (see Data Standards for more details)</p>"},{"location":"01_introduction/#neurophysiology-informatics-challenges-and-dandi-solutions","title":"Neurophysiology Informatics Challenges and DANDI Solutions","text":"Challenges Solutions Most raw data stays in laboratories. DANDI provides a public archive for dissemination of raw and derived data. Non-standardized datasets lead to significant resource needs to understand and adapt code to these datasets. DANDI standardizes all data using NWB and BIDS standards. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. The DANDI ecosystem provides tools for converting data from different instruments into NWB and BIDS. There are many domain general places to house data (e.g. Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant scientific metadata. DANDI is focused on neurophysiology data and related metadata. Datasets are growing larger, requiring compute services to be closer to data. DANDI provides Dandihub, a JupyterHub instance close to the data. Neurotechnology is evolving and requires changes to metadata and data storage. DANDI works with community members to improve data standards and formats. Consolidating and creating robust algorithms (e.g. spike sorting) requires varied data sources. DANDI provides access to many different datasets."},{"location":"100_about_this_doc/","title":"About This Documentation","text":"<p>This documentation is a work in progress and we welcome all input: if something is missing or unclear, let us know by opening an issue on our helpdesk.</p>"},{"location":"100_about_this_doc/#serving-the-docs-locally","title":"Serving the Docs Locally","text":"<p>This project uses the MkDocs tool with the Material theme and extra plugins to generate the website.</p> <p>To test locally, you will need to install the Python dependencies. To do that, type the following commands:</p> <pre><code>git clone https://github.com/dandi/handbook.git\ncd handbook\npip install -r requirements.txt\n</code></pre> <p>If you are working on your fork, simply replace <code>https://github.com/dandi/handbook.git</code> with <code>git clone git@github.com/&lt;username&gt;/handbook.git</code> , where <code>&lt;username&gt;</code> is your GitHub username.</p> <p>Once done, you need to run MkDocs. Simply type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Finally, open <code>http://127.0.0.1:8000/</code> in your browser, and you should see the default home page of the documentation being displayed.</p>"},{"location":"10_using_dandi/","title":"Using DANDI","text":"<p>DANDI allows you to work with stored neurophysiology data in multiple ways. You can search, view, and download files, all without registering for a DANDI account. As a registered user, you can also create these collections of data  along with metadata and publish them to the DANDI platform. </p>"},{"location":"10_using_dandi/#dandisets","title":"Dandisets","text":"<p>DANDI stores cellular neurophysiology data in Dandisets.</p> <p>A Dandiset is a collection of assets (files and their metadata) and metadata about the collection.</p> <ul> <li>A Dandiset is organized in a structured manner to help users and software tools interact with it.</li> <li>Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g. https://identifiers.org/DANDI:000004). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset.</li> </ul>"},{"location":"10_using_dandi/#quick-start","title":"Quick Start","text":"<p>If you are new to DANDI, all you need is an Internet connection to use the DANDI Web application to view  and download files from a  public  Dandiset.  Registration is not required.</p> <p>To view a specific public Dandiset and download one of its files:</p> <ol> <li> <p>At the top of the DANDI Web application, click PUBLIC DANDISETS to see all Dandisets currently available in the     archive. You can sort them by name, identifier, or date of modification.</p> </li> <li> <p>Search for a specific Dandiset by contributor name, modality, or species.</p> </li> <li> <p>Click a Dandiset to open its landing page and view important information such as contact information,     description, license, access information and keywords, and simple statistics.</p> </li> <li> <p>From the right side of the Dandiset landing page, click FILES to see a list of all folders and files for that     Dandiset. Click the download icon  to download a     specific file.  Note: To download an entire Dandiset, you will need to follow the instructions in the     Download section to install and use the DANDI Python client tool.</p> </li> </ol>"},{"location":"10_using_dandi/#next-steps","title":"Next steps","text":"<p>Although anyone on the Internet  can view and download public Dandisets, registered users can also create Dandisets, upload data, and publish  the Dandiset to generate a DOI for it. See the sections that follow for more detailed information about the DANDI project, as well as instructions on how  to work with public Dandisets or to create and publish you own as a registered user. </p>"},{"location":"10_using_dandi/#dandiset-actions","title":"Dandiset Actions","text":"<p>The DANDI project contains the DANDI Web application, the DANDI Python client tool, and the DANDI JupyterHub  instance. These tools can be used to perform actions on Dandisets. </p> <p></p> <p>You can learn more about the Dandiset actions in separate sections:</p> <ul> <li>View</li> <li>Download</li> <li>Upload</li> <li>Publish</li> </ul>"},{"location":"10_using_dandi/#tools-to-interact-with-dandi","title":"Tools to interact with DANDI","text":""},{"location":"10_using_dandi/#dandi-web-application","title":"DANDI Web application","text":"<p>The DANDI Web application allows you to:</p> <ul> <li>Search across all public Dandisets</li> <li>Download data from public Dandisets</li> <li>Create a new Dandiset and provide metadata</li> <li>Publish your Dandiset</li> </ul>"},{"location":"10_using_dandi/#dandi-python-client","title":"DANDI Python client","text":"<p>The DANDI Python client allows you to:</p> <ul> <li>Download Dandisets and individual subject folders or files</li> <li>Organize your data locally before upload</li> <li>Upload Dandisets</li> </ul> <p>Before you can use the DANDI Python client, you have to install the package with <code>pip install dandi</code> in a Python 3.8+ environment.</p> <p>You should check the Dandi Debugging section in case of any problems.</p>"},{"location":"10_using_dandi/#dandihub-analysis-platform","title":"Dandihub analysis platform","text":"<p>Dandihub provides a JupyterHub instance in the cloud to interact with the data stored in DANDI.</p> <p>To use the hub, you will need to register for an account using the DANDI Web application.  Note that <code>Dandihub</code> is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"10_using_dandi/#technical-limitations","title":"Technical limitations","text":"<ul> <li>File name/path: There is a limit of 512 characters for the full path length within a dandiset.</li> <li>Volume and size: There is a limit of 5TB per file. We currently   accept any size of standardized datasets, as long as you can upload them over   an HTTPS connection. However, we ask you contact us if you plan to upload more than 10TB of data.</li> </ul>"},{"location":"10_using_dandi/#citing-dandi","title":"Citing DANDI","text":"<p>You can add the following statement to the methods section of your manuscript.</p> <p>Data and associated metadata were uploaded to the DANDI archive [RRID:SCR_017571] using    the Python command line tool (https://doi.org/10.5281/zenodo.3692138). The data were first    converted into the NWB format (https://doi.org/10.1101/2021.03.13.435173) and  organized    into a BIDS-like (https://doi.org/10.1038/sdata.2016.44) structure.</p> <p>You can refer to DANDI using any of the following options:</p> <ul> <li> <p>Using an RRID RRID:SCR_017571. </p> </li> <li> <p>Using the DANDI CLI reference: https://doi.org/10.5281/zenodo.3692138</p> </li> </ul>"},{"location":"11_view/","title":"Viewing Dandisets","text":""},{"location":"11_view/#browse-dandisets","title":"Browse Dandisets","text":"<p>When you go to the DANDI Web application, you can click on <code>PUBLIC DANDISET</code> to access all Dandisets currently available  in the archive, and you can sort them by name, identifier, size, or date of modification.</p> <p></p>"},{"location":"11_view/#search-dandisets","title":"Search Dandisets","text":"<p>In addition, you can search across the Dandisets for any text part of the Dandiset metadata record.  The text may be about contributor names, modalities, or species.  For example,  <code>\"house mouse\"</code> will return a subset of all Dandisets, while <code>\"mouse house\"</code> will likely not return any. When unquoted each word is used as an <code>OR</code>.</p> <p></p> <p>When you click on one of the Dandisets, you can see that the searching phrase can appear in the description, keywords, or in the assets summary.</p> <p></p>"},{"location":"11_view/#dandisets-metadata","title":"Dandisets Metadata","text":"<p>The landing page of each Dandiset contains important information including  metadata provided by the owners such as contact information, description, license, access information and keywords,   simple statistics for a Dandiset such as size of the Dandiset and number of files, or  a summary of the Dandiset including information about species, techniques, and standards.</p> <p></p> <p>If you scroll down, you will also find: - Assets Summary - Funding Information - Related Resources</p> <p>While most of the metadata is summarized on the landing page, some additional information can be  found by clicking <code>Metadata</code> on the right-side panel. For Dandiset owners, this button also allows  adding relevant metadata to populate the landing page.</p> <p></p>"},{"location":"11_view/#file-view","title":"File View","text":"<p>The right side panel allows you also to access a file browser to navigate the list of folders and files in a Dandiset.</p> <p></p> <p>Any file in the Dandiset has a download icon You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. In addition, there is an info icon that leads to full asset metadata. Some files also have a link to external  services that can open the file. Note: that these services often have size limits and hence are activated only for appropriately sized files.</p>"},{"location":"11_view/#my-dandisets","title":"My Dandisets","text":"<p>If you log in as a registered user, you will also see <code>My Dandisets</code> tab:</p> <p></p> <p>By clicking the tab, you can access all the Dandisets you own. For these Dandisets, you can edit and update  metadata through the Dandiset actions section, and add or remove other owners or data.</p>"},{"location":"12_download/","title":"Downloading Data and Dandisets","text":"<p>You can download the content of a Dandiset using the DANDI Web application (such a specific file) or entire Dandisets using the DANDI Python CLI.</p>"},{"location":"12_download/#using-the-dandi-web-application","title":"Using the DANDI Web Application","text":"<p>Once you have the Dandiset you are interested in (see more in the Dandiset View section), you can download the content of the Dandiset. On the landing page of each Dandiset, you can find <code>Download</code> button on the right-hand panel. After clicking the button, you will see the specific command you can use with DANDI Python CLI (as well as the information on how to download the CLI).</p> <p></p>"},{"location":"12_download/#download-specific-files","title":"Download specific files","text":"<p>The right-side panel of the Dandiset landing page allows you also to access the list of folders and files.</p> <p></p> <p>Each file in the Dandiset has a download icon next to it, clicking the icon will start the download process.</p>"},{"location":"12_download/#using-the-python-cli-client","title":"Using the Python CLI Client","text":"<p>The DANDI Python client gives you more options, such as downloading entire Dandisets.</p> <p>Before You Begin: You need to have Python 3.8+ and install the DANDI Python Client using <code>pip install dandi</code>. If you have an issue using the Python CLI, see the Dandi Debugging section.</p>"},{"location":"12_download/#download-a-dandiset","title":"Download a Dandiset","text":"<p>To download an entire Dandiset, you can use the same command as suggested by DANDI web application, e.g.:</p> <p><code>dandi download DANDI:000023</code></p>"},{"location":"12_download/#download-data-for-a-specific-subject-from-a-dandiset","title":"Download data for a specific subject from a Dandiset","text":"<p>You can download data for specific subjects. Names of the subjects can be found on DANDI web application or by running a command with the DANDI CLI: <code>dandi ls -r DANDI:000023</code>. Once you have the subject ID, you can download the data, e.g.:</p> <p><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/_draft_/assets/?path=sub-811677083</code></p> <p>You should replace <code>_draft_</code> with a specific version you are interested in (e.g. <code>0.210914.1900</code> in the case of this Dandiset).</p> <p>You can also use the link from DANDI web application, e.g.:</p> <p><code>dandi download https://dandiarchive.org/dandiset/000023/0.210914.1900/files?location=sub-541516760%2F</code></p>"},{"location":"12_download/#download-a-specific-file-from-a-dandiset","title":"Download a specific file from a Dandiset","text":"<p>You can download a specific file from a Dandiset when the link for the specific file can be found on the DANDI web application, e.g.:</p> <p><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/0.210914.1900/assets/1a93dc97-327d-4f9c-992d-c2149e7810ae/download/</code></p> <p>Hint: <code>dandi download</code> supports a number of resource identifiers to point to a Dandiset, folder, or file.  Providing an incorrect URL (e.g. <code>dandi download wrongurl</code>) will provide a list of supported identifiers.</p>"},{"location":"12_download/#using-datalad","title":"Using DataLad","text":"<p>All dandisets are regularly mirrored to DataLad datasets which are made available at the GitHub organization https://github.com/dandisets. Where present, individual Zarr files are included as subdatasets (git submodules) hosted in the GitHub organization https://github.com/dandizarrs/.</p> <p>The Git revision histories of each dataset reflect the Dandiset's draft state as of each execution of the mirroring job. Published Dandiset versions are tagged with Git tags.</p> <p>With DataLad, you can: - clone an entire dataset, - use a specific version of it, - explore history of modifications, - download content of files of interest, - locally discard the content of no-longer-needed files, - use the dataset in a reproducible manner, - include it as a subdataset in your own DataLad dataset, - use https://github.com/datalad/datalad-fuse/ to FUSE-mount individual locally-cloned dandisets so that their files' contents are transparently streamed to your DANDI/DataLad-unaware tools, - etc.</p> <p>Learn more about DataLad from its handbook at https://handbook.datalad.org/.</p> <p>Developers' note: DataLad datasets are created using the dandi/backups2datalad tool which is also available for use by the community to similarly maintain mirrors of independent DANDI deployments as DataLad datasets.</p>"},{"location":"12_download/#using-webdav","title":"Using WebDAV","text":"<p>DANDI provides a WebDAV service at https://webdav.dandiarchive.org/ for accessing the data in the DANDI archive. You can use any WebDAV client or even a web browser to access the data - any dandiset, any version, any file or collection of files. You can use any web download tool to download the data from the DANDI archive, e.g.</p> <pre><code>wget -r -np -nH --cut-dirs=3 https://webdav.dandiarchive.org/dandisets/000027/releases/0.210831.2033/\n</code></pre> <p>for a download of a specific release <code>0.210831.2033</code> of the <code>000027</code> dandiset.</p> <p>Note: The WebDAV service does not directly serve any file contents; it instead relies on redirects to AWS S3 storage where the contents are stored. You might need to configure your WebDAV client to follow redirects; e.g., for the davfs2 WebDAV client, set <code>follow_redirect</code> to <code>1</code> in <code>/etc/davfs2/davfs2.conf</code>.</p> <p>Developers' note: The WebDAV service's code is available at https://github.com/dandi/dandidav/ and can also be used for independent DANDI deployments.</p>"},{"location":"135_validation/","title":"Validation Levels for NWB Files","text":"<p>To be accepted by DANDI, NWB files must conform to criteria that are enforced via three levels of validation:</p>"},{"location":"135_validation/#nwb-file-validation","title":"NWB File Validation","text":"<p>PyNWB validation is used to validate the NWB files,  ensuring that they meet the specifications of core NWB and of any NWB extensions that were used. Generally  speaking, all files produced by PyNWB and MatNWB should pass validation, however there are occasional bugs. More  often, NWB files that fail to meet these criteria have been created outside PyNWB and MatNWB.</p>"},{"location":"135_validation/#critical-nwb-checks","title":"Critical NWB Checks","text":"<p>The NWB Inspector scans NWB files using heuristics to find mistakes  or areas for improvements in NWB files. There are three levels of importance for checks: CRITICAL, BEST PRACTICE  VIOLATIONS, and BEST PRACTICE SUGGESTIONS. CRITICAL warnings indicate some internal inconsistency in the data of the  NWB files. The NWB Inspector will print out all warnings, but only CRITICAL warnings will prevent a file from being  uploaded to DANDI. Errors in NWB Inspector will be block upload as well, but reflect a problem with the NWB  Inspector software as opposed to the NWB file. </p>"},{"location":"135_validation/#missing-dandi-metadata","title":"Missing DANDI Metadata","text":"<p>DANDI has requirements for metadata beyond what is strictly required for NWB validation. The following metadata must  be present in the NWB file for a successful upload to DANDI: - You must define a <code>Subject</code> object. - The <code>Subject</code> object must have a <code>subject_id</code> attribute. - The <code>Subject</code> object must have a <code>species</code> attribute. This can either be the Latin binomial, e.g. \"Mus musculus\", or    an NCBI taxonomic identifier. - The <code>Subject</code> object must have a <code>sex</code> attribute. It must be \"M\", \"F\", \"O\" (other), or \"U\" (unknown). - The <code>Subject</code> object must have either <code>date_of_birth</code> or <code>age</code> attribute. It must be in ISO 8601 format, e.g. \"P70D\"    for 70 days, or, if it is a range, must be \"[lower]/[upper]\", e.g. \"P10W/P12W\", which means \"between 10 and 12 weeks\"</p> <p>These requirements are specified in the  DANDI configuration file of NWB Inspector.</p> <p>Passing all of these levels of validation can sometimes be tricky. If you have any questions, please ask them via the  DANDI Help Desk and we would be happy to assist you.</p>"},{"location":"13_upload/","title":"Creating Dandisets and Uploading Data","text":"<p>This page provides instructions for creating a new Dandiset and uploading data to DANDI.</p>"},{"location":"13_upload/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Convert data to NWB. You should start by converting your data to NWB format (2.1+). We suggest beginning the conversion process using only a small amount of data so that common issues may be spotted earlier in the process.   This step can be complex depending on your data. Consider using the following tools:</p> <ol> <li>NWB Graphical User Interface for Data Entry (GUIDE) is a cross-platform desktop application for converting data from common proprietary formats to NWB and uploading it to DANDI.</li> <li>NeuroConv is a Python library that automates  conversion to NWB from a variety of popular formats. See the Conversion Gallery for example conversion scripts.</li> <li>PyNWB and MatNWB are APIs in Python and MATLAB that allow full flexibility in reading and writing data. (PyNWB tutorials, MatNWB tutorials)</li> <li>NWB Overview Docs points to more tools helpful for working with NWB files.</li> </ol> <p>Feel free to reach out to us for help.</p> </li> <li> <p>Choose a server.</p> <ul> <li>Production server: https://dandiarchive.org. This is the main server for DANDI and should be used for sharing neuroscience data.   When you create a Dandiset, a permanent ID is automatically assigned to it.   This Dandiset can be fully public or embargoed according to NIH policy.   All data are uploaded as draft and can be adjusted before publishing on the production server.</li> <li>Development server: https://gui-staging.dandiarchive.org. This server is for testing and learning how to use DANDI.   It is not recommended for sharing data, but is recommended for testing the DANDI CLI and GUI or as a testing platform for developers.   Note that the development server should not be used to stage your data.</li> </ul> <p>The below instructions will alert you to where the commands for interacting with these two different servers differ slightly. </p> </li> <li> <p>Register for DANDI and copy the API key. To create a new Dandiset and upload your data, you need to have a DANDI account.</p> <ul> <li>If you do not already have an account, see Create a DANDI Account page for instructions. </li> <li>Once you are logged in, copy your API key.  Click on your user initials in the top-right corner after logging in.  Production (https://dandiarchive.org) and staging (https://gui-staging.dandiarchive.org) servers have different API keys and different logins.</li> <li>Store your API key somewhere that the CLI can find it; see \"Storing Access Credentials\" below.</li> </ul> </li> </ol>"},{"location":"13_upload/#data-uploadmanagement-workflow","title":"Data upload/management workflow","text":"<p>The NWB GUIDE provides a graphical interface for inspecting and validating NWB files, as well as for uploading data to DANDI. See the NWB GUIDE Dataset Publication Tutorial for more information.</p> <p>The below instructions show how to do the same thing programmatically using the command line interface (CLI). The CLI approach may be more suitable for users who are comfortable with the command line or who need to automate the process, or for advanced use-cases.</p> <ol> <li>Create a new Dandiset. <ul> <li>Click <code>NEW DANDISET</code> in the Web application (top right corner) after logging in.</li> <li>You will be asked to enter basic metadata: a name (title) and description (abstract) for your dataset. </li> <li>After you provide a name and description, the dataset identifier will be created; we will call this <code>&lt;dataset_id&gt;</code>.</li> </ul> </li> <li> <p>Check your files for NWB Best Practices.    Run NWB Inspector programmatically. Install the Python library (<code>pip install -U nwbinspector</code>) and run:</p> <pre><code>nwbinspector &lt;source_folder&gt; --config dandi\n</code></pre> <p>If the report is too large to efficiently navigate in your console, you can save a report using</p> <pre><code>nwbinspector &lt;source_folder&gt; --config dandi --report-file-path &lt;report_location&gt;.txt\n</code></pre> <p>For more details and other options, run:</p> <pre><code>nwbinspector --help\n</code></pre> <p>Thoroughly read the NWBInspector report and try to address as many issues as possible.  DANDI will prevent validation and upload of any issues labeled as level 'CRITICAL' or above when using the <code>--config dandi</code> option.  See \"Validation Levels for NWB Files\" for more information about validation criteria for   uploading NWB files and which are deemed critical. We recommend regularly running the inspector early in the process to generate the best NWB files possible. Note that some auto-detected violations, such as <code>check_data_orientation</code>, may be safely ignored in the event   that the data is confirmed to be in the correct form. See the NWBInspector CLI documentation for more information.</p> </li> <li> <p>Install the DANDI Client.</p> <pre><code>pip install -U dandi\n</code></pre> </li> <li> <p>Validate NWB files. Perform a validation of the NWB files by running:</p> <pre><code>dandi validate --ignore DANDI.NO_DANDISET_FOUND &lt;source_folder&gt;\n</code></pre> <p>If you are having trouble with validation, make sure the conversions were run with the most recent version of <code>dandi</code>, <code>PyNWB</code> and <code>MatNWB</code>.</p> </li> <li> <p>Upload the data to DANDI. This can either be done through the NWB GUIDE, or programmatically:</p> <pre><code>dandi download https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft\ncd &lt;dataset_id&gt;\ndandi organize &lt;source_folder&gt; -f dry\ndandi organize &lt;source_folder&gt;\ndandi validate .\ndandi upload\n</code></pre> <p>Note that the <code>organize</code> steps should not be used if you are preparing a BIDS dataset with the NWB files.  Uploading to the development server is controlled via <code>-i</code> option, e.g. <code>dandi upload -i dandi-staging</code>.  Note that validation is also done during <code>upload</code>, but ensuring compliance using <code>validate</code> prior to upload helps avoid interruptions of the lengthier upload process due to validation failures.  If you have an issue using the DANDI Client, see the DANDI Debugging section.</p> </li> <li> <p>Add metadata to the Dandiset. Visit your Dandiset landing page:    <code>https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft</code> and click on the <code>METADATA</code> link.</p> </li> </ol>"},{"location":"13_upload/#storing-access-credentials","title":"Storing Access Credentials","text":"<p>There are two options for storing your DANDI access credentials.</p> <ol> <li> <p><code>DANDI_API_KEY</code> Environment Variable</p> <ul> <li> <p>By default, the DANDI CLI looks for an API key in the <code>DANDI_API_KEY</code>   environment variable.  To set this on Linux or macOS, run:</p> <pre><code>export DANDI_API_KEY=personal-key-value\n</code></pre> </li> <li> <p>Note that there are no spaces around the \"=\".</p> </li> </ul> </li> <li> <p><code>keyring</code> Library</p> <ul> <li> <p>If the <code>DANDI_API_KEY</code> environment variable is not set, the CLI will look up the API     key using the keyring library, which     supports numerous backends, including the system keyring, an encrypted keyfile,     and a plaintext (unencrypted) keyfile.</p> </li> <li> <p>Specifying the <code>keyring</code> backend</p> <ul> <li> <p>You can set the backend the <code>keyring</code> library uses either by setting   the <code>PYTHON_KEYRING_BACKEND</code> environment variable or by filling in   the <code>keyring</code> library's configuration file.</p> </li> <li> <p>IDs for the available backends can be listed by running <code>keyring   --list</code>.</p> </li> <li> <p>If no backend is specified in this way, the library will use the   available backend with the highest priority.</p> </li> <li> <p>If the DANDI CLI encounters an error while attempting to fetch the   API key from the default keyring backend, it will fall back to using   an encrypted keyfile (the <code>keyrings.alt.file.EncryptedKeyring</code>   backend).  If the keyfile does not already exist, the CLI will ask   you for confirmation; if you answer \"yes,\" the <code>keyring</code>   configuration file (if it does not already exist; see above) will be   configured to use <code>EncryptedKeyring</code> as the default backend.  If you   answer \"no,\" the CLI will exit with an error, and you must store the   API key somewhere accessible to the CLI on your own.</p> <ul> <li> <p>Unless a different location is set via the <code>keyring</code>   configuration file, the encrypted keyfile will be located at the   following path:</p> <ul> <li> <p>On Linux and macOS, if the <code>XDG_DATA_HOME</code> environment   variable is set to a nonempty string, the keyfile will be at   <code>$XDG_DATA_HOME/python_keyring/crypted_pass.cfg</code>; otherwise,   it will be at   <code>~/.local/share/python_keyring/crypted_pass.cfg</code>.</p> </li> <li> <p>On Windows, if the <code>LOCALAPPDATA</code> environment variable is   set, the keyfile will be at <code>%LOCALAPPDATA%\\Python   Keyring\\crypted_pass.cfg</code>; otherwise, if the <code>ProgramData</code>   environment variable is set, the keyfile will be at   <code>%ProgramData%\\Python Keyring\\crypted_pass.cfg</code>; otherwise,   it will be at <code>Python Keyring\\crypted_pass.cfg</code> within the   current directory.</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Storing the API key with <code>keyring</code></p> <ol> <li> <p>You can store your API key where the <code>keyring</code> library can find it by using   the <code>keyring</code> program: Run <code>keyring set dandi-api-dandi key</code> and enter the   API key when asked for the password for <code>key</code> in <code>dandi-api-dandi</code>.</p> </li> <li> <p>If the API key isn't stored in either the <code>DANDI_API_KEY</code> environment variable   or in the keyring, the CLI will prompt you to enter the API key, and then it   will store it in the keyring.  This may cause you to be prompted further; you   may be asked to enter a password to encrypt/decrypt the keyring, or you may be   asked by your operating system to confirm whether to give the DANDI CLI access to the   keyring.</p> </li> </ol> </li> </ul> </li> </ol>"},{"location":"14_publish/","title":"Publishing Dandisets","text":"<p>Once you create a Dandiset, DANDI will automatically create a <code>draft</code> version of the Dandiset that can be changed as many times as needed by editing the  metadata or uploading new files.</p> <p>When the draft version is ready, you can publish your Dandiset. This results in an immutable snapshot of your Dandiset with its own unique version number that others can cite. If you need to change the data or metadata, you can do so by continuing to modify the draft version and publishing a new version when you are ready.</p> <p>Follow these steps to publish your Dandiset:</p> <ol> <li> <p>Edit the Dandiset metadata, aiming to fix all Dandiset metadata validation    errors, and include any other useful information. For example, you may want    to edit the following fields:</p> <ul> <li>People and funding contributors</li> <li>Protocol information</li> <li>Keywords</li> <li>Related resources such as publications and code repositories</li> </ul> </li> <li> <p>Fix all asset metadata errors by modifying the asset files to eliminate    the errors and re-uploading them.</p> </li> <li> <p>When all the Dandiset metadata and asset metadata errors are fixed, and the Dandiset is made public if it was initially embargoed, the    <code>Publish</code> button (on the right panel of the Dandiset landing page) will    be enabled and turn green. Click the button to publish your Dandiset.</p> </li> <li> <p>In the lower right section of the Dandiset landing page, you should see    the new, published version of your Dandiset listed. Click on that link    to view this version.</p> </li> </ol> <p>NOTE: Dandisets with Zarr assets currently cannot be published. We are  actively working on enabling this feature.</p>"},{"location":"15_debugging/","title":"Debugging","text":"<p>If something goes wrong while using the Python CLI client, the first place to check for more information so that you can file a quality bug report is the logs.  Every command records a copy of its logs in a logfile, the location of which is reported to the user when the command finishes running.  The location of the logs varies by platform, e.g.:</p> <ul> <li>Linux: <code>~/.cache/dandi-cli/log</code> or <code>$XDG_CACHE_HOME/dandi-cli/log</code></li> <li>macOS: <code>~/Library/Logs/dandi-cli</code></li> </ul> <p>Logs are named with a combination of the time at which the <code>dandi</code> command started running and the process ID of the command.</p> <p>Recent versions of the client include all possible debugging information in the logs, but if you're using an older version, only log messages that were printed to the user when the command ran are recorded.  As a result, in order to get complete debugging information, you may have to rerun the problematic command, this time increasing the logging level by passing <code>-l DEBUG</code> or <code>--log-level DEBUG</code> on the command line.  Note that this option goes between the main <code>dandi</code> command and the name of the subcommand:</p> <pre><code># Right:\ndandi -l DEBUG upload\n\n# Wrong:\ndandi upload -l DEBUG\n</code></pre> <p>In addition, many commands can be put into a developer-specific mode for showing raw progress information instead of fancy progress bars.  For the <code>delete</code>, <code>organize</code>, <code>upload</code>, and <code>validate</code> commands, this can be done by setting the <code>DANDI_DEVEL</code> environment variable and passing <code>--devel-debug</code> to the command:</p> <pre><code>DANDI_DEVEL=1 dandi upload --devel-debug\n</code></pre> <p>For the <code>download</code> command, the equivalent is the <code>-f debug</code>/<code>--format debug</code> option:</p> <pre><code>dandi download -f debug\n</code></pre> <p>More advanced users who are familiar with the Python debugger can instruct the client to automatically open the debugger if any errors occur by supplying the <code>--pdb</code> option to the command.  Like the <code>-l</code>/<code>--log-level</code> option, the <code>--pdb</code> option must be placed between <code>dandi</code> and the name of the subcommand.</p>"},{"location":"16_account/","title":"Create a DANDI Account","text":"<p>A DANDI account enhances your capabilities within the DANDI Archive. Without an account, users can freely search, view, and download available datasets. With an account, users can create and edit Dandisets, and use the DANDI Hub to analyze data.</p> <p>DANDI provides two servers:</p> <ul> <li>Main server: https://dandiarchive.org/ - This is the primary platform for most users.</li> <li>Staging server: https://gui-staging.dandiarchive.org/ - Ideal for training and testing purposes.</li> </ul> <p>Accounts are independently managed on each server, allowing users to register on one or both, depending on their testing and deployment needs.</p> <p>DANDI is freely accessible to the neuroscience research community. Membership is usually granted automatically to GitHub accounts with a <code>.edu</code> or similar academic email. If your registration is denied:</p> <ul> <li>With an academic email not linked to your GitHub, please contact help@dandiarchive.org for assistance using this email address.</li> <li>Without an academic email, account approval is still possible under specific circumstances. Appeal decisions at help@dandiarchive.org.</li> </ul>"},{"location":"16_account/#how-to-register-for-a-dandi-account","title":"How to Register for a DANDI Account","text":"<ol> <li>Create a GitHub Account: If not already a GitHub user, sign up here.</li> <li>Register on DANDI: Navigate to the DANDI homepage and click the <code>LOG IN WITH GITHUB</code> button to register using your GitHub account.</li> <li>Confirmation of Review: Post-registration, you will receive an email confirming that your account is under review. Your request will be reviewed within 24 hours.</li> <li>Note: Reviews may extend beyond 24 hours for new GitHub accounts or non-.edu email addresses, particularly if the registration does not describe immediate plans to contribute data.</li> <li>Accessing DANDI: Upon approval, access DANDI by logging in through the <code>LOG IN WITH GITHUB</code> button.</li> </ol> <p>For support or further inquiries, reach out to help@dandiarchive.org.</p>"},{"location":"20_project_structure/","title":"Project Structure","text":"<p>The DANDI project can be represented schematically:</p> <p></p> <p>The Client side contains the DANDI Python CLI and DANDI Web application.</p> <p>The Server side contains a RESTful API and DANDI JupyterHub.</p> <p>The Dandiset is a file organization to store data together with metadata.</p> <p>The DANDI project is organized around several GitHub repositories:</p> Repository Description DANDI archive Contains the code for deploying the client-side Web application frontend based on the Vue.js framework as well as a Django-based backend to run the DANDI REST API. DANDI JupyterHub Contains the code for deploying a JupyterHub instance to support interaction with the DANDI archive. DANDI Python client Contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. handbook Provides the contents of this website. helpdesk Contains our community help platform where you can submit issues. schema Provides the details and some supporting code for the DANDI metadata schema. schema Python library Provides a Python library for updating the schema and for creating and validating DANDI objects. website Provides an overview of the DANDI project and the team members and collaborators."},{"location":"30_data_standards/","title":"Data Standards","text":"<p>DANDI requires uploaded data to adhere to community data standards.  These standards help data curators package all the necessary metadata and provide a uniform structure so that data can be more easily understood and reused by future users.  DANDI also leverages these standards to provide features like data validation and automatic metadata extraction and search. DANDI currently supports two data standards: </p> <ul> <li>For cellular neurophysiology, such as electrophysiology and optical physiology, use Neurodata Without Borders (NWB)</li> <li>For neuroimaging data, such as MRI, use Brain Imaging Data Structure (BIDS)</li> </ul> <p>For microscopy data from immunostaining, we are using the BIDS extension for microscopy.</p> <p>To share data on DANDI, you will first need to convert your data to an appropriate standard. If you would like help determining which standard is most appropriate for your data, do not hesitate to reach out using the dandi helpdesk and we would be happy to assist.</p>"},{"location":"30_data_standards/#neurodata-without-borders-nwb","title":"Neurodata Without Borders (NWB)","text":"<p>NWB is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data. The NWB team supports APIs in Python (PyNWB) and MATLAB (MatNWB), with tutorials for writing data broken down by experiment type. See the NWB Tutorials page for more details. Also see the NWB Conversion Tools user guide for converting data for automated conversions from several popular proprietary data formats.  The best way to get help from the NWB community is through the NWB user Slack channel.</p>"},{"location":"30_data_standards/#brain-imaging-data-format-bids","title":"Brain Imaging Data Format (BIDS)","text":"<p>BIDS is a way to organize and describe neuroimaging and behavioral data.  See the Getting Started page for instructions for how to convert your neuroimaging data to BIDS.</p> <p>For microscopy and associated MR data, use the BIDS extension for microscopy.</p>"},{"location":"35_data_licenses/","title":"Data Licenses","text":"<p>To create a Dandiset, you must select a license under which to share the data. Because the DANDI Archive provides a platform for open data sharing, the licenses come from Creative Commons, an international nonprofit organization dedicated to establishing, growing, and maintaining a shared commons in the spirit of open source.</p> <p>These licenses enable the dataset's copyright holder to grant permissions to others to share and use the data for a wide range of purposes. The licenses available to users of the Archive are as follows:</p> <ul> <li> <p>Attribution (CC-BY-4.0). This license grants permission to share the data to others and to adapt the data by remixing, transforming, and building upon it, so long as appropriate credit is given to the copyright holder and any changes to the original are clearly indicated. The dataset may be used for any purpose (even commercial ones). Note that this license retains the original copyright while granting permissive access to the data to all others.</p> </li> <li> <p>Public domain dedication (CC0-1.0). This license dedicates the dataset to the public domain, relinquishing copyright and therefore allowing anyone to use the dataset for any purpose without restriction.</p> </li> </ul> <p>You can learn more about the theory of how the Creative Commons licenses operate at their website. If you have any questions or concerns, send a message to help@dandiarchive.org.</p>"},{"location":"40_development/","title":"Developer Notes","text":"<p>This page contains important information for anyone starting development work on the DANDI project.</p>"},{"location":"40_development/#overview","title":"Overview","text":"<p>The DANDI archive dev environment comprises three major pieces of software: <code>dandi-archive</code>, <code>dandi-cli</code>, and <code>dandi-schema</code>.</p>"},{"location":"40_development/#dandi-archive","title":"<code>dandi-archive</code>","text":"<p><code>dandi-archive</code> is the web frontend application; it connects to <code>dandi-api</code> and provides a user interface to all the DANDI functionality.  <code>dandi-archive</code> is a standard web application built with <code>yarn</code>. See the <code>dandi-archive</code> README for instructions on how to build it locally.</p> <p>The Django application makes use of several services to provide essential function for the DANDI REST API, including Postgres (to hold administrative data about the web application itself), Celery (to run asynchronous compute tasks as needed to implement API semantics), and RabbitMQ (to act as a message broker between Celery and the rest of the application).</p> <p>The easiest way to run the API along with its services is through a Docker Compose setup, as detailed in the Develop with Docker quickstart.</p>"},{"location":"40_development/#dandi-cli","title":"<code>dandi-cli</code>","text":"<p><code>dandi-cli</code> is a Python command line tool used to manage downloading and uploading of data with the archive. You may need to use this tool when developing new features for the frontend and backend, but there are other methods of faking data in the system to work with as well. You can install <code>dandi-cli</code> with a command like <code>pip install dandi</code> (then invoke <code>dandi</code> on the command line to run the tool), or build it locally following the instructions in the <code>dandi-cli</code> README.</p>"},{"location":"40_development/#dandi-schema","title":"<code>dandi-schema</code>","text":"<p><code>dandi-schema</code> is a Python library for  creating, maintaining, and validating the DANDI metadata models for dandisets  and assets. You may need to make use of this tool when improving models, or  migrating metadata. You can install <code>dandi-schema</code> with a command like  <code>pip install dandi-schema</code>. When releases are published through dandi-schema,  corresponding json-schemas are generated in the release folder of the dandi schema repo. See the <code>dandi-schema</code> README for instructions on  viewing the schemas.</p>"},{"location":"40_development/#technologies-used","title":"Technologies Used","text":"<p>This section details some foundational technologies used in <code>dandi-archive</code>. Some basic understanding of these technologies is the bare minimum requirement for contributing meaningfully, but keep in mind that the DANDI team can help you get spun up as well.</p> <p>JavaScript/TypeScript. The DANDI archive code is a standard JavaScript web application, but we try to implement new functionality using TypeScript.</p> <p>Vue/VueX. The application's components are written in Vue, and global application state is managed through VueX.</p> <p>Vuetify. The components make heavy use of the Vuetify component library.</p> <p>Python3. The backend code is written in Python 3.</p> <p>Django/drf/drf-yasg. The API infrastructure is implemented through a Django application. This means that application resources must be mapped to Django models, while Django views mediate API responses. The REST endpoints are implemented via Django Rest Framework (DRF), while DRF-YASG is used to generate Swagger documentation.</p> <p>For general help with <code>dandi-archive</code>, contact @waxlamp.</p>"},{"location":"40_development/#deployment","title":"Deployment","text":"<p>The DANDI project uses automated services to continuously deploy both the <code>dandi-api</code> backend and the <code>dandi-archive</code> frontend.</p> <p>Heroku manages backend deployment automatically from the <code>master</code> branch of the <code>dandi-api</code> repository. For this reason it is important that pull requests pass all CI tests before they are merged. Heroku configuration is in turn managed by Terraform code stored in the <code>dandi-infrastructure</code> repository. If you need access to the Heroku DANDI organization, talk to @satra.</p> <p>Netlify manages the frontend deployment process. Similarly to <code>dandi-api</code>, these deployments are based on the <code>master</code> branch of <code>dandi-archive</code>. The <code>netlify.toml</code> file controls Netlify settings. The @dandibot GitHub account is the \"owner\" of the Netlify account used for this purpose; in order to get access to that account, speak to @satra.</p>"},{"location":"40_development/#monitoring","title":"Monitoring","text":""},{"location":"40_development/#services-status","title":"Service(s) status","text":"<p>The DANDI project uses upptime to monitor the status of DANDI provided and third-party services. The configuration is available in .upptimerc.yml of the https://github.com/dandi/upptime repository, which is automatically updated by the upptime project pipelines. Upptime automatically opens new issues if any service becomes unresponsive, and closes issues whenever service comes back online. https://www.dandiarchive.org/upptime/ is the public dashboard for the status of DANDI services.</p>"},{"location":"40_development/#logging","title":"Logging","text":""},{"location":"40_development/#sentry","title":"Sentry","text":"<p>Sentry is used for error tracking main deployment. To access Sentry, login to https://dandiarchive.sentry.io .</p>"},{"location":"40_development/#heroku-papertrail","title":"Heroku &amp; Papertrail","text":"<p>The <code>dandi-api</code> and <code>dandi-api-staging</code> apps have the Papertrail add-on configured to capture logs. To access Papertrail, log in to the Heroku dashboard, proceed to the corresponding app and click on the \"Papertrail\" add-on.</p> <p>A cronjob on the <code>drogon</code> server backs up Papertrail logs as .csv files hourly at <code>/mnt/backup/dandi/papertrail-logs/{app}</code>. Moreover, <code>heroku logs</code> processes per app dump logs to <code>/mnt/backup/dandi/heroku-logs/{app}</code> directory.</p>"},{"location":"40_development/#continuous-integration-ci-jobs","title":"Continuous Integration (CI) Jobs","text":"<p>The DANDI project uses GitHub Actions for continuous integration. Logs for many of the repositories are archived on <code>drogon</code> server at <code>/mnt/backup/dandi/tinuous-logs/</code>.</p>"},{"location":"40_development/#code-hosting","title":"Code Hosting","text":"<p>All code repositories are hosted on GitHub. The easiest way to contribute is to gain push access to the repositories by talking to @waxlamp; this way, you can create pull requests based on branches within the origin repositories, which in turn allows for Netlify deploy previews and Heroku staging previews to be built.</p> <p>However, this is not strictly required. You can contribute using the standard fork-and-pull-request model, but under this workflow we will lose the benefit of those previews.</p>"},{"location":"40_development/#email-lists","title":"Email Lists","text":"<p>The project's email domain name services are managed via Terraform as AWS Route 53 entries. This allows the API server to send emails to users, etc. It also means we need a way to forward incoming emails to the proper mailing list--this is accomplished with a service called ImprovMX.</p> <p>The email addresses info@dandiarchive.org and help@dandiarchive.org are advertised to users as general email addresses to use to ask for information or help; both of them are forwarded to dandi@mit.edu, a mailing list containing the leaders and developers of the project. The forwarding is done by the ImprovMX service, and more such email addresses can be created as needed within that service.</p> <p>If you need the credentials for logging into ImprovMX, speak to Roni Choudhury (roni.choudhury@kitware.com).</p>"},{"location":"40_development/#miscellaneous-tips-and-information","title":"Miscellaneous Tips and Information","text":""},{"location":"40_development/#use-email-address-to-log-into-dev-django-admin-panel","title":"Use email address to log into dev Django admin panel","text":"<p>Once <code>dandi-api</code> is up and running, you can access the Django admin panel at http://localhost:8000/admin. The login page asks for a \"username\" but really it is expecting the email address associated with the username.</p> <p>One easy trick here is to supply the username again as the email address when you are setting up the superuser during initial setup.</p>"},{"location":"40_development/#refresh-github-login-to-log-into-prod-django-admin-panel","title":"Refresh GitHub login to log into prod Django admin panel","text":"<p>To log into the production Django admin panel, you must simply be logged into the DANDI Archive production instance using an admin account.</p> <p>However, at times the Django admin panel login seems to expire while the login to DANDI Archive proper is still live. In this case, simply log out of DANDI, log back in, and then go to the Django admin panel URL (e.g. https://api.dandiarchive.org/admin) and you should be logged back in there.</p>"},{"location":"40_development/#why-do-incoming-emails-to-dandiarchiveorg-look-crazy","title":"Why do incoming emails to dandiarchive.org look crazy?","text":"<p>When a user emails help@dandiarchive.org or info@dandiarchive.org, those messages are forwarded to dandi@mit.edu (see above) so that the dev team sees them. However, these emails arrive with a long, spammy-looking From address with a Heroku DNS domain; this seems to be an artifact of how mit.edu processes emails, and does not occur in general (e.g. messages sent from the API server to users).</p>"},{"location":"50_hub/","title":"Using the DANDI Hub","text":"<p>DANDI Hub is a JupyterHub instance in the cloud to interact with the data stored in DANDI, and is free to use for exploratory analysis of data on DANDI. For instructions on how to navigate JupyterHub see this YouTube tutorial. Note that DANDI Hub is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"50_hub/#registration","title":"Registration","text":"<p>To use the DANDI Hub, you must first register for an account using the DANDI website. See the Create a DANDI Account page.</p>"},{"location":"50_hub/#choosing-a-server-option","title":"Choosing a server option","text":"<p>When you start up the DANDI Hub, you will be asked to select across a number of server options. For basic exploration, Tiny or Base would most likely be appropriate. The DANDI Hub also currently offers Medium and Large options, which have more available memory and compute power. The \"T4 GPU inference\" server comes with an associated T4 GPU, and is intended to be used for applications that require GPU for inference. We request that users of this server be considerate of their usage of the DANDI Hub as a free community resource. Training large deep neural networks is not appropriate. A \"Base (MATLAB)\" server is also available, which provides a MATLAB cloud installation but you would be required to provide your own license.</p>"},{"location":"50_hub/#custom-server-image","title":"Custom server image","text":"<p>If you need additional software installed in the image, you can add a server image that will be made available for all users in the <code>Server Options</code> menu.  Add a server image by updating the <code>profileList</code> in the JupyterHub config file and submitting a pull request to the dandi-hub repository.  Once the pull request is merged, the DANDI team will redeploy JupyterHub and the image will be available.</p>"},{"location":"50_hub/#example-notebooks","title":"Example notebooks","text":"<p>The best way to share analyses on DANDI data is through the DANDI example notebooks. These notebooks are maintained in the dandi/example-notebooks repository which provides more information about their organization. Dandiset contributors are encouraged to use these notebooks to demonstrate how to read, analyze, and visualize the data, and how to produce figures from associated scientific publications.</p> <p>Notebooks can be added and updated through a pull request to the dandi/example-notebooks repository. Once the pull request is merged, your contributed notebook will be available to all DANDI Hub users.</p>"},{"location":"about/policies/","title":"General Policies v1.1.0","text":""},{"location":"about/policies/#content","title":"Content","text":"<ul> <li>Scope: Neurophysiology research. Raw and derived experimental data. Content   must not violate privacy or copyright, or breach confidentiality or non-disclosure   agreements for data collected from human subjects.</li> <li>Status of research data: Empirical (not simulated) data and associated metadata from any stage of the   research study's life cycle is accepted.  Simulated data is handled on a case-by-case basis, contact the DANDI team </li> <li>Eligible users: Anyone working with the data in the scope of the archive may register as a user of DANDI. All users are   allowed to deposit content for which they possess the appropriate rights   and which falls within the scope of the archive.</li> <li>Ownership: By uploading content, no change of ownership is implied and no   property rights are transferred to the DANDI team. All uploaded content remains   the property of the parties prior to submission and must be accompanied by a license allowing   DANDI project data access, archival, and re-distribution (see License below).</li> <li>Data file formats: DANDI only accepts data using standardized formats such   as Neurodata Without Borders, Brain Imaging Data Structure,   Neuroimaging Data Model, and other BRAIN Initiative   standards. We are working with the community to improve these standards and to   make DANDI archive FAIR.</li> <li>Data quality: All data are provided \u201cas-is\u201d, and the user shall hold   DANDI and data providers supplying data to the DANDI Archive free and harmless in   connection with the use of such data.</li> <li>Metadata types and sources: All metadata is stored internally in JSON format   according to a defined JSON schema. Metadata records violating the schema are not allowed.</li> <li>Language: Textual items must be in English. Latin names could be used in exceptional cases where appropriate.</li> <li>Licenses: Users must specify a license for each dataset chosen from the list of the DANDI archive approved licenses. Users allow for the DANDI archive to extract metadata records and make them available under permissive CC0 license.</li> </ul>"},{"location":"about/policies/#access-and-reuse","title":"Access and Reuse","text":"<ul> <li>Access to data objects: Files deposited to the archive are accessible to the public    openly or accessible to collaborators for embargoed datasets. Access to metadata and data    files is provided over standard protocols such as HTTPS.</li> <li>Use and reuse of data objects: Use and reuse is subject to the terms of the license   under which the data objects were deposited.</li> <li> <p>Metadata access and reuse: Metadata records, provided by the users or extracted from the assets, are licensed under CC0. All metadata is made publicly available and can be harvested.</p> </li> <li> <p>Embargo status: Users may deposit content under an embargo status and   provide an anticipated end date for the embargo. The repository will restrict   access to the data until the end of the embargo period, at which time the   content will automatically become publicly available. The end of the embargo   period is the earliest of the date provided by submitter, the first publication   using the data, or the end of funding support for the collection and/or dissemination   of the dataset.</p> </li> <li>Restricted access: Depositors of embargoed datasets have the ability to   share access with other collaborators. These files will not be made publicly   available till the end of the embargo period.</li> </ul>"},{"location":"about/policies/#removal","title":"Removal","text":"<ul> <li> <p>Revocation: Content not considered to fall under the scope of the repository   can be removed and associated DOIs issued by DANDI revoked. Inform the DANDI team   promptly, ideally no later than 24 hours from upload, about any suspected policy   violation. Alternatively, content found to already have an external DOI will   have the DANDI DOI invalidated and the record updated to indicate the original   external DOI. User access may be revoked on violation of Terms of Use.</p> </li> <li> <p>Withdrawal: If the uploaded research object must later be withdrawn, the   reason for the withdrawal will be indicated on a tombstone page, which will   henceforth be served in its place. Withdrawal is considered an exceptional   action, which normally should be requested and fully justified by the original   uploader. In any other circumstance reasonable attempts will be made to contact   the original uploader to obtain consent. The DOI and the URL of the original   object are retained.</p> </li> <li> <p>User data on Dandihub: At present, user data on Dandihub is being removed   periodically and Dandihub storage space should not be considered persistent.</p> </li> </ul>"},{"location":"about/policies/#longevity","title":"Longevity","text":"<ul> <li>Versions: Datasets are versioned when published. Prior to publishing the   state of a dataset may continue to evolve and the data or metadata are neither   versioned, nor guaranteed to persist. Derivatives of data files may be generated, but original content is   never modified.</li> <li>Replicas: All data files are stored on an AWS public bucket, with replicas   housed at Dartmouth College.  Data files are kept in multiple replicas at the   moment, but this may change over time, and no recovery mechanisms for unversioned data   should be assumed to be in place.</li> <li>Retention period: Versioned items will be retained for the lifetime of the repository.   This is currently the lifetime of the NIH award, which currently expires in   April 2029.</li> <li>Functional preservation: DANDI makes no promises of usability and   understandability of deposited objects.</li> <li>File preservation: Data files and metadata are backed up nightly and   replicated into multiple copies in different storage services.</li> <li>Fixity and authenticity: All data files are stored along with multiple   checksums of the file content. Files are regularly checked against their   checksums to assure that file content remains constant.</li> <li>Succession plans: In case of a repository shutdown, our best efforts will   be made to integrate all content into suitable alternative institutional and/or   other repositories overlapping in the scope of the DANDI archive.</li> </ul> <p>This policy document is derived from the Zenodo General Policies v1.0.</p>"},{"location":"about/terms/","title":"Terms of Use v1.0.1","text":"<p>The DANDI data archive (\"DANDI\") is offered by the DANDI project as part of its mission to make available the results of its work.</p> <p>Use of DANDI, both the uploading and downloading of data, denotes agreement with the following terms:</p> <ol> <li> <p>DANDI is an open dissemination research data repository for the preservation    and making available of research, educational and informational content. Access    to DANDI\u2019s content is open to all.</p> </li> <li> <p>Content may be uploaded free of charge by the US BRAIN Initiative and other    projects required to submit data to a public archive and those without ready    access to an organized data center.</p> </li> <li> <p>The uploader is exclusively responsible for the content that they upload to    DANDI and shall indemnify and hold the DANDI team free and harmless in    connection with their use of the service. The uploader shall ensure that their    content is suitable for open dissemination, and that it complies with these    terms and applicable laws, including, but not limited to, privacy, data    protection and intellectual property rights [1]. In addition, where data that    was originally sensitive personal data is being uploaded for open dissemination    through DANDI, the uploader shall ensure that such data is either anonymized    to an appropriate degree or fully consent cleared [2].</p> </li> <li> <p>Access to DANDI, and all content, is provided on an \"as-is\" basis. Users of    content (\"Users\") shall respect applicable license conditions. Download and    use of content from DANDI does not transfer any intellectual property rights    in the content to the User.</p> </li> <li> <p>Users are exclusively responsible for their use of content, and shall indemnify    and hold the DANDI team free and harmless in connection with their download    and/or use. Hosting and making content available through DANDI does not    represent any approval or endorsement of such content by the DANDI team.</p> </li> <li> <p>The DANDI team reserves the right, without notice, at its sole discretion and    without liability, (i) to alter, delete or block access to content that it    deems to be inappropriate or insufficiently protected, and (ii) to restrict    or remove User access where it considers that use of DANDI interferes with    its operations or violates these Terms of Use or applicable laws.</p> </li> <li> <p>Unless specified otherwise, DANDI metadata may be freely reused under the    CC0 waiver.</p> </li> <li> <p>These Terms of Use are subject to change by the DANDI team at any time and    without notice, other than through posting the updated Terms of Use on the    DANDI website.</p> </li> <li> <p>Uploaders considering DANDI for the storage of unanonymized or encrypted/unencrypted   sensitive personal data are advised to use bespoke platforms rather than open   dissemination services like DANDI for sharing their data.</p> </li> </ol> <p>[1] [2] See further the user pages regarding uploading for information on anonymization of datasets that contain sensitive personal information.</p> <p>If you have any questions or comments with respect to DANDI, or if you are unsure whether your intended use is in line with these Terms of Use, or if you seek permission for a use that does not fall within these Terms of Use, please contact us.</p> <p>This Terms of Service document is derived from the Zenodo terms of service v1.2.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/","title":"DANDI User Guide, Part I","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\nfrom dateutil.tz import tzlocal\nimport uuid\n\nimport numpy as np\n\nfrom pynwb.file import NWBFile, Subject\nfrom pynwb.ecephys import ElectricalSeries\nfrom pynwb import NWBHDF5IO\n\ndef create_nwbfile(subject_id):\n    nwbfile = NWBFile(\n        session_description='my first synthetic recording',\n        identifier=str(uuid.uuid4()),\n        session_start_time=datetime.now(tzlocal()),\n        experimenter='Dr. Bilbo Baggins',\n        lab='Bag End Laboratory',\n        institution='University of Middle Earth at the Shire',\n        experiment_description='I went on an adventure with thirteen dwarves to reclaim vast treasures.',\n        session_id='001',\n    )\n\n    nwbfile.subject = Subject(\n        subject_id=subject_id,\n        species='Mus musculus',\n        age='P90D',\n        sex='F',\n    )\n\n    device = nwbfile.create_device(name='trodes_rig123')\n\n    electrode_group = nwbfile.create_electrode_group(\n        name='tetrode1',\n        description=\"an example tetrode\",\n        location=\"hippocampus\",\n        device=device,\n    )\n\n    for _ in range(4):\n        nwbfile.add_electrode(\n            x=1.0, y=2.0, z=3.0,\n            imp=np.nan,\n            location='CA1',\n            filtering='none',\n            group=electrode_group,\n        )\n\n    electrode_table_region = nwbfile.create_electrode_table_region([0, 2], 'the first and third electrodes')\n\n\n    rate = 10.0\n    data_len = 1000\n    ephys_data = np.random.rand(data_len * 2).reshape((data_len, 2))\n    ephys_timestamps = np.arange(data_len) / rate\n\n    ephys_ts = ElectricalSeries(\n        name='test_ephys_data',\n        data=ephys_data,\n        electrodes=electrode_table_region,\n        timestamps=ephys_timestamps,\n        description=\"Random numbers generated with numpy.random.rand\"\n    )\n    nwbfile.add_acquisition(ephys_ts)\n    \n    return nwbfile\n</pre> from datetime import datetime from dateutil.tz import tzlocal import uuid  import numpy as np  from pynwb.file import NWBFile, Subject from pynwb.ecephys import ElectricalSeries from pynwb import NWBHDF5IO  def create_nwbfile(subject_id):     nwbfile = NWBFile(         session_description='my first synthetic recording',         identifier=str(uuid.uuid4()),         session_start_time=datetime.now(tzlocal()),         experimenter='Dr. Bilbo Baggins',         lab='Bag End Laboratory',         institution='University of Middle Earth at the Shire',         experiment_description='I went on an adventure with thirteen dwarves to reclaim vast treasures.',         session_id='001',     )      nwbfile.subject = Subject(         subject_id=subject_id,         species='Mus musculus',         age='P90D',         sex='F',     )      device = nwbfile.create_device(name='trodes_rig123')      electrode_group = nwbfile.create_electrode_group(         name='tetrode1',         description=\"an example tetrode\",         location=\"hippocampus\",         device=device,     )      for _ in range(4):         nwbfile.add_electrode(             x=1.0, y=2.0, z=3.0,             imp=np.nan,             location='CA1',             filtering='none',             group=electrode_group,         )      electrode_table_region = nwbfile.create_electrode_table_region([0, 2], 'the first and third electrodes')       rate = 10.0     data_len = 1000     ephys_data = np.random.rand(data_len * 2).reshape((data_len, 2))     ephys_timestamps = np.arange(data_len) / rate      ephys_ts = ElectricalSeries(         name='test_ephys_data',         data=ephys_data,         electrodes=electrode_table_region,         timestamps=ephys_timestamps,         description=\"Random numbers generated with numpy.random.rand\"     )     nwbfile.add_acquisition(ephys_ts)          return nwbfile       In\u00a0[\u00a0]: Copied! <pre>!pwd\n</pre> !pwd In\u00a0[\u00a0]: Copied! <pre>from os import mkdir\n\nmkdir('../data')\n\nnwbfile = create_nwbfile(subject_id='001')\n\nwith NWBHDF5IO('../data/ecephys_example.nwb', 'w') as io:\n    io.write(nwbfile)\n</pre> from os import mkdir  mkdir('../data')  nwbfile = create_nwbfile(subject_id='001')  with NWBHDF5IO('../data/ecephys_example.nwb', 'w') as io:     io.write(nwbfile) In\u00a0[\u00a0]: Copied! <pre>nwbfile = create_nwbfile(subject_id='002')\n\nwith NWBHDF5IO('../data/ecephys_example2.nwb', 'w') as io:\n    io.write(nwbfile)\n</pre> nwbfile = create_nwbfile(subject_id='002')  with NWBHDF5IO('../data/ecephys_example2.nwb', 'w') as io:     io.write(nwbfile) <p>Then go back to terminal and run</p> <pre>dandi organize ../data\ndandi upload -i dandi-staging\n</pre> In\u00a0[\u00a0]: Copied! <pre>from nwbwidgets import nwb2widget\n\nnwb2widget(nwbfile)\n</pre> from nwbwidgets import nwb2widget  nwb2widget(nwbfile) <p>It can also be used to explore any file shared on DANDI. You can use the DANDI API to access the s3 path of any file and stream it directly into NWB Widgets.</p> In\u00a0[\u00a0]: Copied! <pre># calcium imaging, Giocomo Lab (30 GB)\n#dandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"\n\n# neuropixel, Giocomo Lab (46 GB)\n#dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"\n\n# neuropixel, Allen Intitute\n#dandiset_id, filepath = \"000022\", \"sub-744912845/sub-744912845_ses-766640955.nwb\"\n\n# ecephys, Buzsaki Lab (15.2 GB)\n#dandiset_id, filepath = \"000003\", \"sub-YutaMouse41/sub-YutaMouse41_ses-YutaMouse41-150831_behavior+ecephys.nwb\"\n</pre> # calcium imaging, Giocomo Lab (30 GB) #dandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"  # neuropixel, Giocomo Lab (46 GB) #dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"  # neuropixel, Allen Intitute #dandiset_id, filepath = \"000022\", \"sub-744912845/sub-744912845_ses-766640955.nwb\"  # ecephys, Buzsaki Lab (15.2 GB) #dandiset_id, filepath = \"000003\", \"sub-YutaMouse41/sub-YutaMouse41_ses-YutaMouse41-150831_behavior+ecephys.nwb\" In\u00a0[\u00a0]: Copied! <pre>from dandi.dandiapi import DandiAPIClient\nfrom pynwb import NWBHDF5IO\nfrom nwbwidgets import nwb2widget\n\nwith DandiAPIClient() as client:\n    asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\n    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n\nio = NWBHDF5IO(s3_url, mode='r', load_namespaces=True, driver='ros3')\nnwb = io.read()\nnwb2widget(nwb)\n</pre> from dandi.dandiapi import DandiAPIClient from pynwb import NWBHDF5IO from nwbwidgets import nwb2widget  with DandiAPIClient() as client:     asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)     s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)  io = NWBHDF5IO(s3_url, mode='r', load_namespaces=True, driver='ros3') nwb = io.read() nwb2widget(nwb)"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#part-i","title":"Part I\u00b6","text":""},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#converting-data-to-nwb","title":"Converting data to NWB\u00b6","text":"<p>This is part 3 of the DANDI User Training on Nov 1, 2021.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#neurodata-without-borders-nwb","title":"Neurodata Without Borders (NWB)\u00b6","text":"<p>DANDI has chosen NWB as our supported format for exchanging neurophysiology data. NWB is a BRAIN Initiative-backed data standard designed to package all of the data and metadata associated with neurophysiology experiments into a single file that enables sharing and re-analysis of the data. This includes extracellular and intracellular electrophysiology, optical physiology, and behavior. NWB defines a data organization schema that ensure the crucial metadata is packaged in a standardized way. This includes not just the neurophysiology recordings, but also metadata about the subjects, equipment, task structure, etc.</p> <p></p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#creating-an-nwb-file","title":"Creating an NWB file\u00b6","text":"<p>Converting data to NWB is a big topic that merits its own workshop! Here, we will just give a surface introduction by quickly walk through a script for creating an NWB file. The following creates an example extracellular electrophysiology file with an electrodes table and data from voltage traces. Note also the subject-specific information, which is required by DANDI, and is read and used throughout the data publication process.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#nwb-tutorials","title":"NWB tutorials\u00b6","text":"PyNWB MatNWB Reading NWB files Jupyter notebook 15 min videoMATLAB Live Script Writing extracellular electrophysiology 23 min videoJupyter notebook 46 min videoWritten tutorial Writing intracellular electrophysiology Jupyter notebook Written tutorial Writing optical physiology 31 min videoJupyter notebook 39 min videoWritten tutorial Advanced write 26 min video 16 min videoWritten tutorial <p>If you think NWB might not meet your needs, please contact us on our help desk!</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#uploading-to-dandi","title":"Uploading to DANDI\u00b6","text":"<p>When you register a dandiset, it creates a permanent ID. For instructional purposes, we will be using a staging version of DANDI, so that we do not create real IDs for pretend datasets.</p> <p>We are going to use a staging version of DANDI.</p> <ol> <li><p>Go to DANDI staging (https://gui-staging.dandiarchive.org/) and use your GitHub account to log in.</p> </li> <li><p>Click on your initials in the upper right and copy your API key.</p> </li> </ol> <p></p> <ol> <li>Now create a new dataset by clicking the NEW DATASET button</li> </ol> <p></p> <ol> <li>Create a new Launcher</li> </ol> <p></p> <p>and launch a Terminal window.</p> <p></p> <ol> <li>Assign your DANDI API key as an environmental variable:</li> </ol> <pre>export DANDI_API_KEY=your-key-here\n</pre> <p>should look like</p> <pre>export DANDI_API_KEY=d71c0db17827aac067896f612f48af667890000\n</pre> <p>Confirm that this worked with the following line, which should print your key.</p> <pre>echo $DANDI_API_KEY\n</pre> <ol> <li>Copy the URL from the dataset you have created (something like <code>https://gui-staging.dandiarchive.org/#/dandiset/100507</code>)</li> </ol> <p>and in the Terminal window, run</p> <pre>dandi download \"https://gui-staging.dandiarchive.org/#/dandiset/100507\"  # &lt;-- your dandiset ID here\ncd 100507 # &lt;-- your dandiset ID here\ndandi organize ../data -f dry\ndandi organize ../data\ndandi upload -i dandi-staging  # (on the non-staging server, this line would simply be: dandi upload)\n</pre> <ol> <li>Now refresh the landing page of your dandiset and you should see that it is no longer empty.</li> </ol> <p></p> <p>You can explore all the NWB files within by clicking Files</p> <p></p> <ol> <li>Adding more data to a dandiset</li> </ol>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20I/#visualization-nwb-files","title":"Visualization NWB files\u00b6","text":"<p>NWB Widgets is a library of interactive data visualizations that works automatically with any NWB file. This can be very useful for visually confirming any conversion.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/","title":"DANDI User Guide, Part II","text":"In\u00a0[1]: Copied! <pre>!dandi --help\n</pre> !dandi --help <pre>Usage: dandi [OPTIONS] COMMAND [ARGS]...\n\n  A client to support interactions with DANDI archive\n  (http://dandiarchive.org).\n\n  To see help for a specific command, run\n\n      dandi COMMAND --help\n\n  e.g. dandi upload --help\n\nOptions:\n  --version\n  -l, --log-level [DEBUG|INFO|WARNING|ERROR|CRITICAL]\n                                  Log level (case insensitive).  May be\n                                  specified as an integer.  [default: INFO]\n  --pdb                           Fall into pdb if errors out\n  --help                          Show this message and exit.\n\nCommands:\n  delete            Delete dandisets and assets from the server.\n  digest            Calculate file digests\n  download          Download a file or entire folder from DANDI\n  instances         List known Dandi Archive instances that the CLI can...\n  ls                List .nwb files and dandisets metadata.\n  organize          (Re)organize files according to the metadata.\n  shell-completion  Emit shell script for enabling command completion.\n  upload            Upload dandiset (files) to DANDI archive.\n  validate          Validate files for NWB (and DANDI) compliance.\n</pre> <p>which provides you with the overall syntax for using the <code>dandi</code> CLI, and lists commands and common options which could be specified right after <code>dandi</code> and before any particular <code>COMMAND</code>.</p> <p>More information on a particular command could be obtained by adding <code>--help</code> after the <code>COMMAND</code>, e.g.:</p> In\u00a0[2]: Copied! <pre>!dandi ls --help\n</pre> !dandi ls --help <pre>Usage: dandi ls [OPTIONS] [PATHS]...\n\n  List .nwb files and dandisets metadata.\n\n  Patterns for known setups:\n   - DANDI:&lt;dandiset id&gt;\n   - https://dandiarchive.org/...\n   - https://identifiers.org/DANDI:&lt;dandiset id&gt;\n   - https://&lt;server&gt;[/api]/[#/]dandiset/&lt;dandiset id&gt;[/&lt;version&gt;][/files[?location=&lt;path&gt;]]\n   - https://*dandiarchive-org.netflify.app/...\n   - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;[/versions[/&lt;version&gt;]]\n   - https://&lt;server&gt;[/api]/assets/&lt;asset id&gt;[/download]\n   - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/&lt;asset id&gt;[/download]\n   - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/?path=&lt;path&gt;\n   - dandi://&lt;instance name&gt;/&lt;dandiset id&gt;[@&lt;version&gt;][/&lt;path&gt;]\n   - https://&lt;server&gt;/...\n\nOptions:\n  -F, --fields TEXT               Comma-separated list of fields to display.\n                                  An empty value to trigger a list of\n                                  available fields to be printed out\n  -f, --format [auto|pyout|json|json_pp|json_lines|yaml]\n                                  Choose the format/frontend for output. If\n                                  'auto', 'pyout' will be used in case of\n                                  multiple files, and 'yaml' for a single\n                                  file.\n  -r, --recursive                 Recurse into content of\n                                  dandisets/directories. Only .nwb files will\n                                  be considered.\n  -J, --jobs INTEGER              Number of parallel download jobs.  [default:\n                                  6]\n  --metadata [api|all|assets]\n  --schema VERSION                Convert metadata to new schema version\n  --help                          Show this message and exit.\n</pre> <p>which as you can see has a number of options which could become handy.</p> <p>Let's try the <code>dandi ls</code> command right away on the dandiset you created in the previous section.</p> <ol> <li>In the Terminal (recommendation - make it wide) run</li> </ol> <pre>dandi ls -r data/100507  # &lt;-- path to your dandiset here\n</pre> <p>which should present you with a tabular view of metadata for dandiset and the asset(s) you have in it.</p> <ol> <li><p>Such view could be quite \"busy\". To provide a more useful/targeted listing of data at hand, use the <code>-F</code> option to only view a subset of fields, e.g. add <code>-F age,session_id</code>.</p> </li> <li><p>Try <code>-f</code> to change the format of the output (e.g., from tabular to YAML).</p> </li> </ol> <p>Note: if you point <code>ls</code> to a single file, by default it would produce YAML output.</p> <ol> <li>As the <code>dandi ls --help</code> output suggested, it can also operate on remote dandisets available from DANDI archive. When pointed to a remote URL though, it outputs information about the assets as known to the archive, and places metadata into a <code>metadata</code> key.  In the Terminal, try</li> </ol> <pre>dandi ls -r -f yaml --metadata all DANDI:000037\n</pre> <p>this will list the top-level metadata of the dandiset, as well as metadata for each individual asset.</p> <p>Note: As you can see from the above invocation, <code>ls</code> (as well, as <code>download</code>) supports URLs which you can simply copy/paste from the browser while navigating https://gui.dandiarchive.org .</p> <ol> <li>You could also point to a specific folder, e.g. while navigating it in the web UI:</li> </ol> <p></p> <p>Copy/paste the URL you see in the browser address bar to your <code>dandi ls</code> invocation in the Terminal.</p> <ol> <li>You can also download individual files if you copy the URL from the browser's context menu with the download icon:</li> </ol> <p></p> <p>and paste it into <code>dandi ls</code> invocation.</p> <p>NOTE: When embedding URLs into your analysis scripts, we strongly recommend using persistent URLs, such as URLs to the API server, <code>DANDI:&lt;id&gt;</code> identifiers, or <code>dandi://&lt;instance name&gt;/&lt;dandiset id&gt;[@&lt;version&gt;][/&lt;path&gt;]</code>. When citing dandisets in publications it is best to use DOIs.</p> <p>To try this out, run</p> <pre>dandi ls --metadata all https://api.dandiarchive.org/api/assets/834a2598-927c-4d56-91c6-92eeb9ef005c/download/\n</pre> <p>you can also try out some other files (AKA assets) in the archive.</p> In\u00a0[3]: Copied! <pre>!dandi download --help\n</pre> !dandi download --help <pre>Usage: dandi download [OPTIONS] [URL]...\n\n  Download a file or entire folder from DANDI\n\nOptions:\n  -o, --output-dir DIRECTORY      Directory where to download to (directory\n                                  must exist). Files will be downloaded with\n                                  paths relative to that directory.\n  -e, --existing [error|skip|overwrite|overwrite-different|refresh]\n                                  What to do if a file found existing locally.\n                                  'refresh': verify that according to the size\n                                  and mtime, it is the same file, if not -\n                                  download and overwrite.  [default: error]\n  -f, --format [pyout|debug]      Choose the format/frontend for output. TODO:\n                                  support all of the ls\n  -J, --jobs INTEGER              Number of parallel download jobs.  [default:\n                                  6]\n  --download [assets,dandiset.yaml,all]\n                                  Comma-separated list of elements to download\n                                  [default: all]\n  --sync                          Delete local assets that do not exist on the\n                                  server\n  -i, --dandi-instance [dandi|dandi-api-local-docker-tests|dandi-devel|dandi-staging]\n                                  DANDI instance to use\n  --help                          Show this message and exit.\n</pre> <p>Please review the options. And although it does not say it yet, the <code>download</code> command supports all those URL patterns which you saw listed by <code>dandi ls</code>. <code>dandi download</code> will provide you with them if you enter some unrecognized URL, e.g.:</p> In\u00a0[4]: Copied! <pre>!dandi download from-the-ether\n</pre> !dandi download from-the-ether <pre>2021-11-01 15:06:46,026 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20211101150645Z-554.log\nError: We do not know how to map URL from-the-ether to our servers.\nPatterns for known setups:\n - DANDI:&lt;dandiset id&gt;\n - https://dandiarchive.org/...\n - https://identifiers.org/DANDI:&lt;dandiset id&gt;\n - https://&lt;server&gt;[/api]/[#/]dandiset/&lt;dandiset id&gt;[/&lt;version&gt;][/files[?location=&lt;path&gt;]]\n - https://*dandiarchive-org.netflify.app/...\n - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;[/versions[/&lt;version&gt;]]\n - https://&lt;server&gt;[/api]/assets/&lt;asset id&gt;[/download]\n - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/&lt;asset id&gt;[/download]\n - https://&lt;server&gt;[/api]/dandisets/&lt;dandiset id&gt;/versions/&lt;version&gt;/assets/?path=&lt;path&gt;\n - dandi://&lt;instance name&gt;/&lt;dandiset id&gt;[@&lt;version&gt;][/&lt;path&gt;]\n - https://&lt;server&gt;/...\n</pre> <p>so you can download an entire dandiset, with all the assets if you provide a resource identifier (e.g., <code>DANDI:000027</code>) or a URL to that dandiset as you copy paste it from the browser (e.g. https://gui.dandiarchive.org/#/dandiset/000027/).</p> <p>If a URL points to the staging DANDI archive, <code>dandi download</code> will interact with that server.</p> <p>Keeping in mind the options and URL patterns listed above, and try the following out in a Terminal:</p> <ol> <li>Download the entire 000027 dandiset (from the main archive).</li> </ol> <p>Note: if a dandiset was already published, and the version is not contained in the URL, <code>download</code> will download the most recent release, and not the \"draft\" version</p> <ol> <li><p>Download a <code>draft</code> version of 000027 into some other folder.</p> </li> <li><p>(\"optional\"- bonus point) What is different between draft and published (0.210831.2033) version of the dandiset?</p> <p>Hint: <code>diff -Naur folder1/ folder2/</code> could be used in the Terminal to find an answer.</p> </li> <li><p>Download the <code>sub-anm369962/</code> folder from <code>000006</code> dandiset.</p> </li> </ol> In\u00a0[5]: Copied! <pre>!dandi validate --help\n</pre> !dandi validate --help <pre>Usage: dandi validate [OPTIONS] [PATHS]...\n\n  Validate files for NWB (and DANDI) compliance.\n\n  Exits with non-0 exit code if any file is not compliant.\n\nOptions:\n  --help  Show this message and exit.\n</pre> In\u00a0[7]: Copied! <pre>!dandi validate ../data/ecephys_example.nwb  # &lt;-- put path to the .nwb files you want to validate here\n</pre> !dandi validate ../data/ecephys_example.nwb  # &lt;-- put path to the .nwb files you want to validate here <pre>2021-11-01 15:08:01,558 [    INFO] Note: detected 72 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n2021-11-01 15:08:01,559 [    INFO] Note: NumExpr detected 72 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n2021-11-01 15:08:01,559 [    INFO] NumExpr defaulting to 8 threads.\n2021-11-01 15:08:03,211 [    INFO] ../data/ecephys_example.nwb: ok\nSummary: No validation errors among 1 file(s)\n2021-11-01 15:08:03,211 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20211101150800Z-724.log\n</pre> In\u00a0[8]: Copied! <pre># enter your Python code here\n</pre> # enter your Python code here <ol> <li>Following the original example and/or documentation, get an object representing the dandiset of interest to you (e.g. <code>000006</code>) and download one of the assets using Python code:</li> </ol> In\u00a0[9]: Copied! <pre># enter your Python code here\n</pre> # enter your Python code here <p>All components (such as the https://gui.dandiarchive.org web interfiace) and tools (such as the <code>dandi</code> CLI and Python library) interact with the archive via the DANDI API (Application Programming Interface), which is provided by the <code>https://api.dandiarchive.org</code> server. This API server is the \"Heart\" of the archive, which manages all dandisets and assets in the archive and orchestrates access and deposition of data to AWS S3 bucket.</p> <p>All code of the DANDI API server is available openly from https://github.com/dandi/dandi-api/ . A number of convenient interfaces are available to help you learn about what features it provides, and how to interact with the server.</p> <p>Both production and staging servers have a Swagger interface, which you can reach by going to https://api.dandiarchive.org/swagger/ for production (and https://api-staging.dandiarchive.org/swagger/ for staging, which is where your test datasets are):</p> <p></p> <p>Read-only interaction with the archive (such as listing dandisets, their assets, etc.) does not even require authentication, and in the following brief exercises we will interact with the API server directly in the Terminal, but such interactions could be coded virtually in any programming language.</p> <p>Note: For the production server we also have https://dandi.readme.io , which provides even better UI and code snippets in wide range of languages.</p> <ol> <li>List dandisets known to the archive</li> </ol> <ul> <li>Find \"GET <code>/dandisets/</code>\" end point in swagger interface</li> <li>Click on that row to expand it down and reveal options for that call</li> <li>Click on \"Try it out\" button, possibly scroll down after the options, until you see \"Execute\" button</li> <li>Click on \"Execute\"</li> </ul> <p>After a short while you will see the response in its entirety below, alongside a curl invocation which you can copy and paste into your Terminal to execute, and to obtain a similar result.</p> <ol> <li>List assets \"under a path\"</li> </ol> <ul> <li>Choose a dandiset ID (<code>versions__dandiset__pk</code> within Swagger interface for <code>/assets</code> end points), e.g. <code>000006</code> you would like to list assets for</li> <li>Find end point for function <code>dandisets_versions_assets_list</code></li> <li>If you do not enter any value any value for the <code>path</code> option -- all assets of the dandiset will be listed. If you enter some path -- all assets under that path (including immediately in that directory or any subdirectory below) will be listed.</li> <li>For dandiset version (<code>versions__version</code> within Swagger interface) you can enter specific existing version of the dandiset (if was published) or \"draft\" version</li> <li>Click on \"Execute\"</li> </ul> <p>Although these exercises are simplistic, and typically you would not interact with the archive through the API (but rather use the <code>dandi</code> CLI or Python library), we hope understanding that all operations could also be programmed in any language of your choice can encourage you to interface with the DANDI archive across all platforms, software or web applications you work on.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#part-ii-interacting-with-the-archive-in-different-ways","title":"Part II, Interacting with the archive \"in different ways\"\u00b6","text":"<p>This is part 4 of the DANDI User Training on Nov 1, 2021.</p> <p>In the previous parts you have already briefly interacted with the archive via the Web UI, the <code>dandi</code> command line interface (CLI), and have even used the <code>dandi</code> Python library to access sample files directly from the Python interpreter.</p> <p>Note: Both the CLI and the Python library are provided by the dandi package on PyPI (and Conda), with the underlying codebase being available on GitHub as https://github.com/dandi/dandi-cli/.</p> <p>In this part we will explore more of the <code>dandi</code> CLI/Python library functionality, and introduce you to the DANDI acrhive API server, which allows you to interact with the archive directly from the CLI or programming language of your choice (e.g., MATLAB).</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-command-line-interface","title":"dandi Command Line interface\u00b6","text":"<p>The DANDI Handbook provides a brief introduction to various functions of the <code>dandi</code> CLI, which we will practice using in this section.</p> <p>As with any sane command line tool, <code>dandi</code> provides brief documentation on its usage if you add <code>--help</code> to its invocation:</p> <p>Note: in the code cell below you see commands prefixed with <code>!</code>. This annotation instructs the Jupyter notebook to run that command in bash. You could achieve the same effect by running that command in the Terminal, without typing leading <code>!</code>.  In the exercises below please run these commands in the Terminal.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-download-download-dandisetsfoldersfiles-from-the-archive","title":"dandi download - Download dandisets/folders/files from the archive\u00b6","text":"<p><code>dandi download</code> is probably the most frequently used command by a \"data consumer\" user interested primarily in data download.</p> <p>Let's first see which options it provides:</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-validate-validate-nwb-files-before-upload","title":"dandi validate - Validate NWB files before upload\u00b6","text":"<p>is a very useful command for any \"data producer\". As we have mentioned in the previous portion of the User Guide, all data uploaded to the DANDI archive must follow established standards such as NWB and BIDS. To ensure standard compliance <code>dandi upload</code> validates all files intended for upload and skips (by default) any file which fails validation.</p> <p><code>dandi validate</code> should be used before the upload of data to ensure that <code>.nwb</code> files do not have any internal NWB inconsistencies, and contain metadata required by DANDI archive. At the moment, the interface of the <code>validate</code> command is very trivial:</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-python-library","title":"dandi Python library\u00b6","text":"<p>The <code>dandi</code> command line interface we have practiced with above is a part of the <code>dandi</code> Python package, which also provides Python interfaces to interact with any instance of the DANDI archive (hint: the <code>dandi instances</code> command will list known instances of the archive).</p> <p>In the previous section you already used the library in the following Python code snippet:</p> <pre>from dandi.dandiapi import DandiAPIClient\n\nwith DandiAPIClient() as client:\n    asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\n    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n</pre> <p>https://dandi.readthedocs.io provides documentation not only on CLI, but also on Python interfaces.</p> <p>dandi.dandiapi module used in the snippet above provides high level interfaces which you can use in your scripts or applications.</p> <ol> <li>There is an example at the start of the the dandi.dandiapi documentation. Copy/paste and edit, or enter it in the cell below, and modify it to just list all dandisets in the main instance of the archive:</li> </ol> <p>Hint: You can \"run\" the code by pressing \"Shift-Enter\" or clicking on \"Run\" button in the menu.</p>"},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#dandi-api-server","title":"DANDI API Server\u00b6","text":""},{"location":"tutorials/DANDI%20User%20Guide%2C%20Part%20II/#datalad-dandisets","title":"DataLad dandisets\u00b6","text":"<p>If you like <code>git</code> and/or are interested in using a data management system to take control over your code, data, and computing environment, DataLad provides a solution. A recent DataLad paper in JOSS provides a concise introduction and overview of the features, the DataLad Handbook is a trove of knowledge about DataLad, and the DataLad YouTube channel has many informational videos and tutorials.</p> <p>In a nutshell: All dandisets in the main DANDI archive (not staging) are also made available as DataLad datasets from GitHub: https://github.com/dandisets .</p> <p>Note: they are updated regularly but not in real time, so it takes up to a day for the most recent changes to be propagated.</p> <p>One of the very convenient features of DataLad is the ability to provide a \"view\" of the entire \"tree\" of files in the dandiset (locally, and without downloading all of its content at once). All files under DataLad control are unambiguously version controlled (it is all git and git-annex underneath). But even beyond the reliability advantages of version control, DataLad makes it easy to <code>get</code>, or later <code>drop</code>, any needed content without figuring out where any particular file needs to be downloaded from.</p> <p>For a brief example, let's quickly \"install\" the 000026 dandiset, the size of which is over 11TB if downloaded in full, and <code>get</code> and <code>drop</code> some files.</p> <ol> <li>In the Terminal run</li> </ol> <pre>datalad install https://github.com/dandisets/000026\ncd 000026\n</pre> <ol> <li><p>Use regular <code>cd</code> and <code>ls</code> commands to navigate through the dataset.</p> </li> <li><p>Use <code>datalad get PATHs</code> to get content for file(s) or folder(s) of interest.</p> </li> <li><p>Use <code>datalad drop PATHs</code> to drop the content and reclaim local disk space.</p> </li> </ol> <p>A complete example on a tiny test dandiset.  You are welcome to try on other dandisets:</p> <pre>datalad install https://github.com/dandisets/000027\ncd 000027\ndatalad get sub-RAT123/\n# now you have access to the files under that directory - can use `dandi ls` etc\n# and after you are done working with that data, you are ready to drop the content\ndatalad drop sub-RAT123/\n</pre>"},{"location":"tutorials/NWBWidget-demo/","title":"NWB Widget Demo","text":"In\u00a0[\u00a0]: Copied! <pre>import pynwb\nfrom pynwb import NWBHDF5IO\nfrom nwbwidgets import nwb2widget\nimport requests\n\ndef _search_assets(url, filepath):\n    response = requests.request(\"GET\", url, headers={\"Accept\": \"application/json\"}).json() \n    \n    for asset in response[\"results\"]:\n        if filepath == asset[\"path\"]:\n            return asset[\"asset_id\"]\n    \n    if response.get(\"next\", None):\n        return _search_assets(response[\"next\"], filepath)\n    \n    raise ValueError(f'path {filepath} not found in dandiset {dandiset_id}.')\n\n\ndef get_asset_id(dandiset_id, filepath):\n    url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/\"\n    return _search_assets(url, filepath)\n\ndef get_s3_url(dandiset_id, filepath):\n    \"\"\"Get the s3 location for any NWB file on DANDI\"\"\"\n\n    asset_id = get_asset_id(dandiset_id, filepath)\n    url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\"\n    \n    s3_url = requests.request(url=url, method='head').url\n    if '?' in s3_url:\n        return s3_url[:s3_url.index('?')]\n    return s3_url\n</pre> import pynwb from pynwb import NWBHDF5IO from nwbwidgets import nwb2widget import requests  def _search_assets(url, filepath):     response = requests.request(\"GET\", url, headers={\"Accept\": \"application/json\"}).json()           for asset in response[\"results\"]:         if filepath == asset[\"path\"]:             return asset[\"asset_id\"]          if response.get(\"next\", None):         return _search_assets(response[\"next\"], filepath)          raise ValueError(f'path {filepath} not found in dandiset {dandiset_id}.')   def get_asset_id(dandiset_id, filepath):     url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/\"     return _search_assets(url, filepath)  def get_s3_url(dandiset_id, filepath):     \"\"\"Get the s3 location for any NWB file on DANDI\"\"\"      asset_id = get_asset_id(dandiset_id, filepath)     url = f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\"          s3_url = requests.request(url=url, method='head').url     if '?' in s3_url:         return s3_url[:s3_url.index('?')]     return s3_url In\u00a0[\u00a0]: Copied! <pre># calcium imaging, Giocomo Lab (30 GB)\ndandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"\n\n# neuropixel, Giocomo Lab (46 GB)\n#dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"\n\ns3_path = get_s3_url(dandiset_id, filepath)\n</pre> # calcium imaging, Giocomo Lab (30 GB) dandiset_id, filepath = \"000054\", \"sub-F2/sub-F2_ses-20190407T210000_behavior+ophys.nwb\"  # neuropixel, Giocomo Lab (46 GB) #dandiset_id, filepath = \"000053\", \"sub-npI1/sub-npI1_ses-20190415_behavior+ecephys.nwb\"  s3_path = get_s3_url(dandiset_id, filepath) <p>Note that these NWB files are quite large. In fact, we have chosen NWB files that contain raw data to demonstrate how data streaming can efficiently deal with these large files. Streaming works efficiently with NWB Widgets so that only the data necessary to create each view is read from DANDI. As a result, data transfer is minimized and data can be explored efficiently.</p> In\u00a0[\u00a0]: Copied! <pre># use the \"Read Only S3\" (ros3) driver to stream data directly from DANDI (or any other S3 location)\nio = NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3')\n\nnwb = io.read()\nnwb2widget(nwb)\n</pre> # use the \"Read Only S3\" (ros3) driver to stream data directly from DANDI (or any other S3 location) io = NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3')  nwb = io.read() nwb2widget(nwb) In\u00a0[\u00a0]: Copied! <pre># icephys, optogenetics, and behavior, Svoboda Lab (632 MB)\ndandiset_id, filepath = \"000005\", \"sub-anm324650/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\"\nasset_id = get_asset_id(dandiset_id, filepath)\nprint(f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\")\n</pre> # icephys, optogenetics, and behavior, Svoboda Lab (632 MB) dandiset_id, filepath = \"000005\", \"sub-anm324650/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\" asset_id = get_asset_id(dandiset_id, filepath) print(f\"https://api.dandiarchive.org/api/dandisets/{dandiset_id}/versions/draft/assets/{asset_id}/download/\") <p>click the link that is output above ^^</p> In\u00a0[\u00a0]: Copied! <pre>import os.path as op\nio = NWBHDF5IO(op.expanduser(\"~/Downloads/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\"), mode='r', load_namespaces=True)\n\nnwb = io.read()\nnwb2widget(nwb)\n</pre> import os.path as op io = NWBHDF5IO(op.expanduser(\"~/Downloads/sub-anm324650_ses-20160422_behavior+icephys+ogen.nwb\"), mode='r', load_namespaces=True)  nwb = io.read() nwb2widget(nwb) <p>When running with local data, NWBWidgets still only reads the data that is necessary from disk.</p>"},{"location":"tutorials/NWBWidget-demo/#nwb-widgets","title":"NWB Widgets\u00b6","text":"<p>NWB Widgets is a python package for automatic, interactive, performant exploration of data in NWB files. This notebook demonstrates how to use NWB Widgets to explore data, and how to stream data from the DANDI archive directly into NWB Widgets.</p> <p>NWB Widgets uses the metadata of the NWB file to understand the contents and infer what visualizations make sense for this data. The code is exactly the same for visualizing each different NWB filea. We demonstrate this here with one calcium imaging NWB file and one with Neuropixel extracellular electrophysiology.</p> <p>While this notebook can be run on a properly configured environment anywhere, it will be particularly easy to use and performant using the \"NWBstream\" environment deployed for free on DANDI Hub.</p>"},{"location":"tutorials/NWBWidget-demo/#running-nwbwidgets-locally","title":"Running NWBWidgets locally\u00b6","text":"<p>You can also download NWB files and run NWB Widgets locally by passing the local path (and omitting the ros3 driver flag). Here we will demonstrate this with a smaller intracellular electrophysiology dataset.</p>"},{"location":"tutorials/NWB_tutorial_2019/","title":"NWB tutorial 2019","text":"In\u00a0[1]: Copied! <pre>from pynwb import NWBFile\nfrom datetime import datetime\nfrom dateutil import tz\n\nstart_time = datetime(2018, 4, 25, 2, 30, 3, tzinfo=tz.gettz('US/Pacific'))\n\nnwbfile = NWBFile(identifier='Mouse5_Day3',\n                 session_description='mouse in open exploration and theta maze',  # required\n                 session_start_time=start_time,  # required\n                 experimenter='My Name',  # optional\n                 session_id='session_id',  # optional\n                 institution='University of My Institution',  # optional\n                 lab='My Lab Name',  # optional\n                 related_publications='DOI:10.1016/j.neuron.2016.12.011')  # optional\n</pre> from pynwb import NWBFile from datetime import datetime from dateutil import tz  start_time = datetime(2018, 4, 25, 2, 30, 3, tzinfo=tz.gettz('US/Pacific'))  nwbfile = NWBFile(identifier='Mouse5_Day3',                  session_description='mouse in open exploration and theta maze',  # required                  session_start_time=start_time,  # required                  experimenter='My Name',  # optional                  session_id='session_id',  # optional                  institution='University of My Institution',  # optional                  lab='My Lab Name',  # optional                  related_publications='DOI:10.1016/j.neuron.2016.12.011')  # optional In\u00a0[2]: Copied! <pre>from pynwb.file import Subject\n\nnwbfile.subject = Subject(age='9 months', description='mouse 5',\n                          species='Mus musculus', sex='M')\n</pre> from pynwb.file import Subject  nwbfile.subject = Subject(age='9 months', description='mouse 5',                           species='Mus musculus', sex='M') In\u00a0[3]: Copied! <pre>import numpy as np\nfrom pynwb.behavior import SpatialSeries, Position\n\nposition_data = np.array([\n    np.linspace(0, 10, 100),\n    np.linspace(1, 8, 100)]).T\nspatial_series_object = SpatialSeries(\n   name='position', data=position_data,\n   reference_frame='unknown',\n   conversion=1.0, resolution=np.nan,\n   timestamps=np.linspace(0, 100) / 200)\n</pre> import numpy as np from pynwb.behavior import SpatialSeries, Position  position_data = np.array([     np.linspace(0, 10, 100),     np.linspace(1, 8, 100)]).T spatial_series_object = SpatialSeries(    name='position', data=position_data,    reference_frame='unknown',    conversion=1.0, resolution=np.nan,    timestamps=np.linspace(0, 100) / 200) In\u00a0[4]: Copied! <pre>pos_obj = Position(spatial_series=spatial_series_object)\nbehavior_module = nwbfile.create_processing_module(\n    name='behavior',\n    description='data relevant to behavior')\n\nbehavior_module.add_data_interface(pos_obj)\n</pre> pos_obj = Position(spatial_series=spatial_series_object) behavior_module = nwbfile.create_processing_module(     name='behavior',     description='data relevant to behavior')  behavior_module.add_data_interface(pos_obj) In\u00a0[5]: Copied! <pre>from pynwb import NWBHDF5IO\n\nwith NWBHDF5IO('test_ephys.nwb', 'w') as io:\n    io.write(nwbfile)\n</pre> from pynwb import NWBHDF5IO  with NWBHDF5IO('test_ephys.nwb', 'w') as io:     io.write(nwbfile) In\u00a0[\u00a0]: Copied! <pre>nwbfile.add_trial_column('correct', description='correct trial')\nnwbfile.add_trial(start_time=1.0, stop_time=5.0, correct=True)\nnwbfile.add_trial(start_time=6.0, stop_time=10.0, correct=False)\n</pre> nwbfile.add_trial_column('correct', description='correct trial') nwbfile.add_trial(start_time=1.0, stop_time=5.0, correct=True) nwbfile.add_trial(start_time=6.0, stop_time=10.0, correct=False) In\u00a0[6]: Copied! <pre>nwbfile.add_electrode_column('label', 'label of electrode')\nshank_channels = [4, 3]\n\nelectrode_counter = 0\ndevice = nwbfile.create_device('implant')\nfor shankn, nelecs in enumerate(shank_channels):\n    electrode_group = nwbfile.create_electrode_group(\n       name='shank{}'.format(shankn),\n       description='electrode group for shank {}'.format(shankn),\n       device=device,\n       location='brain area')\n    for ielec in range(nelecs):\n        nwbfile.add_electrode(\n           x=5.3, y=1.5, z=8.5, imp=np.nan,\n           location='unknown', filtering='unknown',\n           group=electrode_group,\n           label='shank{}elec{}'.format(shankn, ielec))\n        electrode_counter += 1\n\nall_table_region = nwbfile.create_electrode_table_region(\n  list(range(electrode_counter)), 'all electrodes')\n</pre> nwbfile.add_electrode_column('label', 'label of electrode') shank_channels = [4, 3]  electrode_counter = 0 device = nwbfile.create_device('implant') for shankn, nelecs in enumerate(shank_channels):     electrode_group = nwbfile.create_electrode_group(        name='shank{}'.format(shankn),        description='electrode group for shank {}'.format(shankn),        device=device,        location='brain area')     for ielec in range(nelecs):         nwbfile.add_electrode(            x=5.3, y=1.5, z=8.5, imp=np.nan,            location='unknown', filtering='unknown',            group=electrode_group,            label='shank{}elec{}'.format(shankn, ielec))         electrode_counter += 1  all_table_region = nwbfile.create_electrode_table_region(   list(range(electrode_counter)), 'all electrodes') In\u00a0[7]: Copied! <pre>from pynwb.ecephys import ElectricalSeries, LFP\nlfp_data = np.random.randn(100, 7)\necephys_module = nwbfile.create_processing_module(\n    name='ecephys',\n    description='extracellular electrophysiology data')\necephys_module.add_data_interface(\nLFP(ElectricalSeries('lfp', lfp_data, all_table_region, \nrate=1000., resolution=.001, conversion=1.)))\n</pre> from pynwb.ecephys import ElectricalSeries, LFP lfp_data = np.random.randn(100, 7) ecephys_module = nwbfile.create_processing_module(     name='ecephys',     description='extracellular electrophysiology data') ecephys_module.add_data_interface( LFP(ElectricalSeries('lfp', lfp_data, all_table_region,  rate=1000., resolution=.001, conversion=1.))) In\u00a0[8]: Copied! <pre>for shankn, channels in enumerate(shank_channels):\n    for n_units_per_shank in range(np.random.poisson(lam=5)):\n        n_spikes = np.random.poisson(lam=10)\n        spike_times = np.abs(np.random.randn(n_spikes))\n        nwbfile.add_unit(spike_times=spike_times)\n</pre> for shankn, channels in enumerate(shank_channels):     for n_units_per_shank in range(np.random.poisson(lam=5)):         n_spikes = np.random.poisson(lam=10)         spike_times = np.abs(np.random.randn(n_spikes))         nwbfile.add_unit(spike_times=spike_times) In\u00a0[10]: Copied! <pre>from pynwb import NWBHDF5IO\n\nwith NWBHDF5IO('test_ephys.nwb', 'w') as io:\n    io.write(nwbfile)\n\nwith NWBHDF5IO('test_ephys.nwb', 'r') as io:\n    nwbfile2 = io.read()\n\n    print(nwbfile2.modules['ecephys']['LFP'].electrical_series['lfp'].data[:])\n</pre> from pynwb import NWBHDF5IO  with NWBHDF5IO('test_ephys.nwb', 'w') as io:     io.write(nwbfile)  with NWBHDF5IO('test_ephys.nwb', 'r') as io:     nwbfile2 = io.read()      print(nwbfile2.modules['ecephys']['LFP'].electrical_series['lfp'].data[:]) <pre>[[-1.18748994e+00 -1.02725896e+00  6.80845832e-01  1.41010819e+00\n   9.93847913e-01 -1.07257974e+00 -1.76200432e+00]\n [-1.91578338e+00 -9.00207419e-01  5.91149951e-01 -2.58572290e+00\n  -1.73342216e+00  1.15397889e+00 -7.66208021e-01]\n [-2.28052884e-01 -2.08688104e-01 -1.40442801e-01  2.51521720e-01\n   6.04263260e-02 -9.98354434e-02  1.08457621e+00]\n [-8.30306082e-01  2.78702874e-02 -1.15557359e+00 -1.53361656e+00\n  -8.10439648e-01  8.64130938e-02 -1.01399851e+00]\n [ 9.88800813e-01  8.60196534e-01 -1.13070345e+00  4.21478483e-01\n  -1.39917114e+00 -3.02864657e-01  1.43830828e+00]\n [ 4.58407372e-02  1.55692901e-01 -9.87254173e-01  9.20414330e-01\n  -1.05719026e+00 -1.58959302e+00  1.01406388e+00]\n [ 1.02244064e+00  4.42150202e-01  1.57886662e-01 -1.47574098e+00\n  -5.73636339e-01  1.72859034e+00  3.17727796e-01]\n [ 4.40830850e-03  2.42902334e-02  3.14347080e-02  8.60116748e-01\n   5.63744801e-02  8.70175600e-01 -6.54398389e-01]\n [-2.70055784e-01  2.80836658e-01  2.96627813e-01 -1.22255795e+00\n  -2.49816636e+00 -4.82817221e-02 -4.00840451e-01]\n [-2.19238143e+00  1.46674518e+00 -4.90982106e-01 -1.34011046e-01\n  -2.15160877e+00  9.38157025e-03 -8.11425686e-01]\n [ 9.81328704e-01 -1.29386216e+00 -7.35925200e-01  1.26504829e+00\n   9.87240018e-02 -8.19934149e-01 -3.35969836e-01]\n [ 1.28830762e+00 -5.80049978e-01 -3.44140053e-01 -3.81949669e-01\n  -4.54005934e-01 -1.54497124e+00 -5.60514802e-01]\n [-2.68059443e+00 -4.07394278e-01 -3.94990661e-02  1.25714288e+00\n   1.23175096e+00 -2.71591471e-01 -2.33823536e-01]\n [ 1.10671551e+00 -4.38783932e-02 -1.22989185e+00  1.12088640e-01\n   3.63958385e-01  3.69725778e-01  1.96383551e+00]\n [-2.41984542e-01 -8.91094788e-01  1.39050773e-01 -7.98820566e-01\n  -6.63442724e-01  7.56823903e-01  1.99890326e+00]\n [-1.50274797e-02 -2.02535992e-01 -4.41450873e-01 -4.33170027e-01\n   9.23106273e-01  4.58931781e-01 -2.01494556e-01]\n [ 5.84692885e-01  2.04735579e-01  3.89215698e-01 -8.24351688e-01\n   1.83065380e+00 -5.60068294e-01 -7.01577057e-02]\n [-1.29157039e+00 -8.55217753e-01 -3.98036620e-01  1.09171463e+00\n  -2.44956986e-01 -6.04394060e-01  1.98579829e+00]\n [ 2.96311438e-01 -6.34662294e-01  2.43557800e-01  5.93518421e-01\n  -8.24353574e-01  5.36343216e-01 -5.69038750e-01]\n [-1.11878047e+00 -1.94953909e+00 -4.24810015e-01 -2.03326443e-01\n   1.85859323e+00 -1.04235373e+00 -1.22781075e-01]\n [ 2.72725615e+00 -9.86193006e-01 -1.19115821e+00 -3.95999155e-01\n   7.66215517e-01 -8.88780830e-01  2.07273881e+00]\n [ 6.80629517e-01 -2.63289906e-01  1.51923562e+00 -6.82652166e-01\n  -2.71914736e+00 -5.91616203e-02 -3.16512724e-01]\n [ 6.95005961e-01  9.64127058e-02 -1.00403919e-01  1.04433528e+00\n  -2.10251372e+00 -2.17102462e+00  4.06807408e-01]\n [ 1.50969184e+00  2.33795229e+00  4.29520271e-01  1.80824153e-01\n   2.93642146e-01 -1.13829512e-01 -6.48352093e-01]\n [-7.62302259e-01 -1.01544742e+00 -5.19415054e-01  8.44815890e-01\n   1.41934533e+00  2.71454515e-01  1.45849630e+00]\n [ 1.44505914e-01 -5.10629318e-01  3.83669700e-02  1.54697516e+00\n   1.40837203e+00  2.08492345e-01 -8.73304205e-01]\n [ 1.49990906e+00  1.24406923e+00  1.05819609e+00  1.16437822e+00\n  -8.03464916e-01 -8.06748742e-01  2.64751175e+00]\n [-2.32164620e+00  1.09455673e+00  5.03041949e-01  4.90362560e-01\n   1.52736986e+00 -8.75323863e-01 -8.08701931e-01]\n [ 2.46010466e+00 -6.28149557e-01 -1.55027275e+00 -7.84111933e-01\n   1.98456705e-01  7.99703255e-02  6.94940401e-01]\n [ 1.55652994e-01  3.97529007e-01 -9.96233045e-01 -1.15956675e+00\n   1.21275026e+00 -3.01587387e-01 -6.58867025e-01]\n [ 6.30184223e-01  6.89381516e-01  5.74533635e-01  3.13899335e-01\n  -8.34312999e-01 -6.61055148e-01 -3.13372050e-01]\n [ 1.36608140e+00 -8.25118854e-01 -4.11666255e-01  8.93650469e-01\n  -8.24404328e-01 -1.44133699e+00 -6.13745988e-01]\n [-1.50935090e+00 -1.13415164e+00 -2.94557711e-01 -2.10649405e+00\n   1.27866758e+00 -5.34412000e-01  2.89330697e-01]\n [-2.86913686e-01 -3.97462329e-01 -2.00936188e+00 -1.85376533e-01\n  -1.45524927e-01  5.41252594e-02  1.12917936e+00]\n [-1.51094968e-01 -3.04399292e-01 -9.60257141e-02  1.53870593e+00\n   6.16500428e-01 -4.09719152e-01 -7.21885664e-01]\n [-4.85816081e-01 -9.17702324e-01 -2.73060994e-01  6.08077564e-01\n  -7.99883167e-01 -1.05061638e-01 -5.19187267e-01]\n [-9.60144327e-01  9.45533252e-01  4.50004418e-01  6.68421520e-01\n  -6.93007926e-01  6.48474204e-01 -1.45560302e-01]\n [-1.43256999e+00  1.17943155e+00 -6.86717916e-01 -6.94511497e-01\n  -1.88775803e-01  2.75456605e-01  2.77992170e-01]\n [-2.31509992e+00 -2.52475709e+00  7.97132690e-02 -2.17034498e-01\n   4.99036967e-01 -6.38572817e-01  2.33850301e+00]\n [ 1.09654347e+00  6.06501018e-01  4.82001983e-01 -1.07237248e-01\n  -4.02978912e-01  2.31735746e-01  2.76476205e+00]\n [ 1.21941274e+00  1.05425459e+00  2.95042588e+00  1.27201603e+00\n   6.59158327e-01 -2.99011991e-01  1.62729276e+00]\n [-1.27314716e+00 -4.88921272e-01  2.23201844e-02  3.19027471e+00\n  -4.63816812e-01 -3.72539160e-01  4.41649266e-02]\n [-5.52971468e-02  1.07432845e+00  1.45762681e+00 -1.05918324e-01\n  -5.54770370e-01 -9.34760270e-02  1.14818082e+00]\n [ 3.62114580e-01  2.63219749e-01 -1.96986799e+00  8.76457903e-01\n   8.80513222e-01  1.19148296e-01 -1.71551776e+00]\n [ 5.31979490e-01  4.84119010e-01 -7.45025313e-01  6.06832230e-02\n  -8.01603972e-01  9.57140801e-01  5.66790529e-01]\n [-8.37861932e-01  1.24547080e+00  1.60318746e+00 -1.06870564e+00\n   8.21820403e-01  7.52851300e-01  1.31627214e+00]\n [ 7.58907372e-02 -8.59963617e-01  8.43531803e-01 -1.25002800e+00\n   4.46155793e-01  9.02542074e-01  2.32400599e+00]\n [ 1.31159974e+00 -1.90874526e+00  1.08507394e+00  1.20880367e-01\n   2.97884604e-01 -1.09779774e+00  1.16002806e+00]\n [-3.58224546e-02 -3.50351344e-01  4.50891552e-01 -6.59579205e-01\n   1.01003878e+00 -9.43268252e-02 -1.92049504e+00]\n [-3.40041512e-01 -1.06969039e-01 -2.24165414e-01 -1.67530458e-01\n  -3.32667384e-01  1.60576695e-01  1.14995776e-01]\n [-2.56144030e-01 -9.09981425e-01  9.49139608e-02 -3.61340523e-01\n   3.26964879e+00 -2.86368287e-01 -1.58534436e+00]\n [ 1.15741086e+00  1.23711985e+00 -1.77311513e+00  1.25067079e+00\n  -1.20660066e+00  7.15197360e-01  1.24823989e+00]\n [-3.41305278e-01 -1.68146343e+00  1.79042569e+00  5.22205193e-01\n  -9.95876573e-01 -1.37503243e-02  4.40375895e-01]\n [ 1.74218104e+00  2.09138385e+00  2.13972915e-01  1.34873360e+00\n  -1.34726846e+00 -4.63554693e-01  3.24714886e-01]\n [ 6.97965559e-01  1.05256239e+00  1.71586942e+00  2.89975155e-01\n  -9.97333725e-01 -4.43730524e-01 -3.67160451e-01]\n [ 1.62850282e+00 -1.26366783e-01  7.90218234e-01  9.44285828e-01\n   7.62774908e-01 -1.18329169e-01  4.40311162e-01]\n [ 1.48186930e-01 -4.44411400e-01  1.39305352e+00 -4.57474908e-02\n  -4.78397334e-01 -2.75629232e-01  4.29182297e-01]\n [ 4.76453976e-01  8.02793709e-01 -1.29793768e+00 -1.04504413e+00\n  -6.51768527e-02  6.14264028e-01  1.29102876e+00]\n [-8.59961109e-01  1.92807528e+00 -6.64917520e-01  9.03926262e-02\n   2.24885379e-01  1.66362874e+00  2.77179326e-01]\n [-2.72080868e-03  4.36780823e-01  3.28857315e-01  1.72016889e+00\n  -2.21412917e+00 -1.21010524e+00  8.65723366e-01]\n [ 1.28944256e-01 -4.09203553e-02  5.47602868e-01  3.18683311e-01\n  -1.75769344e-01 -3.83820378e-01 -1.16119385e-02]\n [ 2.22713927e-01  2.34995597e-01 -1.28451583e+00 -1.68438496e+00\n   2.56605637e-01 -1.13058960e+00  1.44622546e+00]\n [-1.99264437e-01  4.65391546e-01  1.17370770e+00 -1.07715153e+00\n   1.02608032e+00 -2.20254023e+00  2.30410952e-01]\n [ 1.01286444e-01  2.40210880e-01  1.61401147e+00  6.68020319e-01\n   1.17628451e-01  3.29585645e+00  9.62768330e-01]\n [-4.40127949e-01  1.83738437e-01 -8.11417234e-01 -1.13548319e+00\n   4.22937942e-01 -8.72006665e-01  4.17154273e-02]\n [-3.27163340e-01  6.19525645e-01  8.99483646e-01 -6.68073666e-01\n  -3.49243372e-01  1.11621561e+00  6.83032877e-01]\n [ 2.11479253e+00 -2.35822663e+00  5.08137763e-01  3.55316562e-01\n   7.05986669e-02 -7.53233938e-01  1.30403028e+00]\n [ 1.17570296e+00  1.81997977e+00 -2.51725150e-01 -1.24142586e-02\n  -1.43674984e+00 -7.97610589e-01  5.47986947e-02]\n [-4.19331431e-01  8.29896157e-01  5.66608309e-01  6.44690056e-01\n   1.25447187e+00  2.40038561e+00  3.78089153e-01]\n [-3.87787806e-01  1.42998314e+00 -1.03891792e+00  5.64142231e-01\n  -3.37645954e-01 -1.42767826e+00  3.03586411e-01]\n [ 2.85093778e-01 -3.66645752e-01 -1.32138284e+00  5.80284871e-01\n   4.67662774e-01 -1.20888892e-01 -5.45225670e-01]\n [-1.37128449e-01 -1.32194119e-02 -4.28939705e-01  1.19928963e+00\n  -1.01006749e+00  4.90427944e-01 -1.42065616e+00]\n [-7.14273224e-01  2.11710396e+00  8.71893954e-01  1.11977773e+00\n  -1.49333195e+00 -6.80964002e-03  1.74571573e+00]\n [-2.51465126e-01  1.01257499e+00  1.07359104e+00 -6.45424818e-01\n  -4.75588984e-01 -1.05723186e+00  1.00495011e+00]\n [ 6.31768915e-02  5.24912910e-01 -8.23352060e-01  2.63676151e-01\n  -3.57439429e-01  3.65714473e-01 -1.15561583e+00]\n [ 9.96296959e-01  1.30161095e+00  9.37877450e-01  9.30125829e-01\n  -8.66156733e-02  6.44895406e-01  6.87308695e-01]\n [-4.52842466e-01  3.97138833e-01  1.31345137e+00 -6.05219881e-01\n   6.01250641e-01 -5.43927913e-01  2.00657510e+00]\n [-5.38169344e-02  4.82490184e-01  6.66230704e-01  1.00294851e-01\n  -3.38718499e-01  2.77086494e-02  1.25208477e+00]\n [-3.60913995e-01 -8.24707266e-01 -7.84579744e-02  7.01313452e-01\n   5.32318356e-01 -3.70442047e-01 -1.85716977e-02]\n [ 8.10470506e-01  4.57949184e-01 -9.16955951e-01 -3.03886851e-01\n  -1.18461611e-01  2.12101453e-01  4.51568607e-01]\n [-3.64411562e-01 -5.30299781e-01 -5.56503122e-01  6.09433909e-01\n  -1.10582313e+00 -1.20623365e-01  3.80503040e-01]\n [ 9.76977481e-02 -1.31596423e-01 -1.16248855e+00  1.06515018e+00\n   1.93726894e+00  3.96196223e-01 -3.51224786e-01]\n [-2.41152865e+00  2.27960146e+00 -3.12264269e-01  2.12230426e+00\n   2.61270494e+00 -1.12406369e+00  1.69568932e-01]\n [ 7.05403028e-01  2.43763403e-02  6.11465452e-01 -4.20674880e-01\n  -2.70165923e-01  2.28940223e-01 -4.94622575e-01]\n [-5.34448727e-01 -6.94247716e-01  1.68006625e+00 -6.31529971e-01\n  -1.04118049e+00  1.22408385e+00  1.44704451e-01]\n [-1.66676445e-01  1.04282407e+00  7.36520918e-01 -9.12225176e-02\n  -1.50546075e+00  1.92709390e-01  3.65071418e-01]\n [ 7.55831867e-01 -8.84432464e-01 -4.06725469e-01 -5.97991564e-01\n  -9.27942522e-01  4.14561830e-01  2.63237827e-01]\n [ 1.79833343e-01 -2.14479778e+00  1.01735536e+00 -1.35919145e-01\n   1.31229509e+00 -2.76824916e-01 -7.86429181e-01]\n [ 1.06184407e+00  8.48674312e-01 -5.08767267e-01 -9.17141673e-02\n   9.37021898e-01  4.56787101e-01  1.08755135e-01]\n [ 5.76516427e-01  1.66480012e+00 -4.94398685e-01  1.34443338e+00\n  -6.11066019e-01 -3.63626026e-01  8.38789639e-01]\n [ 3.33531363e-01  8.25205151e-01  7.16614574e-01  9.46510036e-01\n   3.07328812e-01  1.13143229e+00  3.49383238e-01]\n [-1.86433745e+00 -6.32593870e-02  1.15123259e+00  2.00954721e-01\n  -1.00174025e+00  1.13009992e+00  5.57538720e-01]\n [-1.19300816e-01  5.83972330e-01  1.71070522e-01 -3.73109066e-01\n  -5.49542248e-01 -1.77958020e+00  1.18576246e+00]\n [ 5.67529630e-01  5.07921678e-01 -1.37682694e-01 -2.36910783e-01\n   9.42149772e-01  1.80540334e-01  7.43259069e-01]\n [-2.25878329e-01  9.27680571e-01  6.36430391e-01  9.84531019e-01\n   1.47808284e+00 -1.47209138e+00 -9.10209903e-01]\n [-2.02913254e-01 -4.63895451e-01 -1.45765241e+00  8.11577979e-01\n   1.15398684e+00  1.91283778e+00  1.10979068e-01]\n [-1.50011625e+00 -9.44171222e-02  7.09493866e-01 -4.09783830e-01\n   3.61918961e-01 -4.33555247e-01 -4.16999539e-01]\n [ 5.34734787e-01 -3.31290086e-01  7.57103336e-01 -3.93971967e-01\n  -2.70027654e-01 -5.76638174e-01 -7.28264273e-01]\n [-9.24925110e-02 -1.04481807e+00  1.60892231e+00  1.98912207e-01\n  -8.20713792e-01 -6.72565813e-01 -8.64606330e-01]\n [ 1.29874520e+00 -2.88805695e-01 -1.41308850e-01 -1.25091036e-01\n   1.25742789e-01 -2.82622541e-01 -6.17634611e-01]]\n</pre> In\u00a0[11]: Copied! <pre>io = NWBHDF5IO('test_ephys.nwb', 'r')\nnwbfile2 = io.read()\n\nprint('section of lfp:')\nprint(nwbfile2.modules['ecephys']['LFP'].electrical_series['lfp'].data[:10,:5])\nprint('')\nprint('')\nprint('spike times from first unit:')\nprint(nwbfile2.units['spike_times'][0])\nio.close()\n</pre> io = NWBHDF5IO('test_ephys.nwb', 'r') nwbfile2 = io.read()  print('section of lfp:') print(nwbfile2.modules['ecephys']['LFP'].electrical_series['lfp'].data[:10,:5]) print('') print('') print('spike times from first unit:') print(nwbfile2.units['spike_times'][0]) io.close() <pre>section of lfp:\n[[-1.18748994 -1.02725896  0.68084583  1.41010819  0.99384791]\n [-1.91578338 -0.90020742  0.59114995 -2.5857229  -1.73342216]\n [-0.22805288 -0.2086881  -0.1404428   0.25152172  0.06042633]\n [-0.83030608  0.02787029 -1.15557359 -1.53361656 -0.81043965]\n [ 0.98880081  0.86019653 -1.13070345  0.42147848 -1.39917114]\n [ 0.04584074  0.1556929  -0.98725417  0.92041433 -1.05719026]\n [ 1.02244064  0.4421502   0.15788666 -1.47574098 -0.57363634]\n [ 0.00440831  0.02429023  0.03143471  0.86011675  0.05637448]\n [-0.27005578  0.28083666  0.29662781 -1.22255795 -2.49816636]\n [-2.19238143  1.46674518 -0.49098211 -0.13401105 -2.15160877]]\n\n\nspike times from first unit:\n[0.02647075 0.07243559 1.50147784 0.18226922 1.16359468 1.4911805\n 0.25810137 0.58790677 0.5408381 ]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/NWB_tutorial_2019/#cosyne-2019-nwbn-tutorial-extracellular-electrophysiology","title":"Cosyne 2019 NWB:N Tutorial - Extracellular Electrophysiology\u00b6","text":""},{"location":"tutorials/NWB_tutorial_2019/#set-up-nwb-file","title":"Set up NWB file\u00b6","text":"<p>NWB files require a session start time to be entered with a timezone field.</p>"},{"location":"tutorials/NWB_tutorial_2019/#subject-info","title":"Subject info\u00b6","text":""},{"location":"tutorials/NWB_tutorial_2019/#position","title":"Position\u00b6","text":"<p>The <code>Position</code> object is a <code>MultiContainerInterface</code> that holds one or more <code>SpatialSeries</code> objects, which are a subclass of <code>TimeSeries</code>. Here, we put a <code>SpatialSeries</code> object called <code>'position'</code> in a <code>Position</code> object, and put that in a <code>ProcessingModule</code> named <code>'behavior'</code>. </p>"},{"location":"tutorials/NWB_tutorial_2019/#write-to-file","title":"Write to file\u00b6","text":""},{"location":"tutorials/NWB_tutorial_2019/#trials","title":"Trials\u00b6","text":"<p>Trials is another <code>DynamicTable</code> that lives an <code>/intervals/trials</code>.</p>"},{"location":"tutorials/NWB_tutorial_2019/#electrodes-table","title":"Electrodes table\u00b6","text":"<p>Extracellular electrodes are stored in a <code>electrodes</code>, which is a <code>DynamicTable</code>. <code>electrodes</code> has several required fields: x, y, z, impedance, location, filtering, and electrode_group. Here, we also demonstrate how to add optional columns to a table by adding the <code>'label'</code> column.</p>"},{"location":"tutorials/NWB_tutorial_2019/#lfp","title":"LFP\u00b6","text":"<p><code>LFP</code> is another <code>MultiContainerInterface</code>. It holds one or more <code>ElectricalSeries</code> objects, which are <code>TimeSeries</code>. Here, we put an <code>ElectricalSeries</code> named <code>'lfp'</code> in an <code>LFP</code> object, in a <code>ProcessingModule</code> named <code>'ecephys'</code>. </p>"},{"location":"tutorials/NWB_tutorial_2019/#spike-times","title":"Spike Times\u00b6","text":"<p>Spike times are stored in another <code>DynamicTable</code> of subtype <code>Units</code>. The main <code>Units</code> table is at <code>/units</code> in the HDF5 file. You can add columns to the <code>Units</code> table just like you did for <code>electrodes</code>.</p>"},{"location":"tutorials/NWB_tutorial_2019/#write-and-read","title":"Write and read\u00b6","text":"<p>Data arrays are read passively from the file. That means <code>TimeSeries.data</code> does not read the entire data object, but presents an h5py object that can be indexed to read data. Index this array just like a numpy array to read only a specific section of the array, or use the <code>[:]</code> operator to read the entire thing.</p>"},{"location":"tutorials/NWB_tutorial_2019/#accessing-data-regions","title":"Accessing data regions\u00b6","text":"<p>You can easily read subsections of datasets</p>"},{"location":"tutorials/advanced_asset_search/","title":"Advanced Asset Search","text":"In\u00a0[1]: Copied! <pre>import json\nimport numpy as np\nfrom dandi.dandiapi import DandiAPIClient\nfrom tqdm.notebook import tqdm\nfrom isodate import parse_duration, Duration\nfrom datetime import datetime\n</pre> import json import numpy as np from dandi.dandiapi import DandiAPIClient from tqdm.notebook import tqdm from isodate import parse_duration, Duration from datetime import datetime In\u00a0[2]: Copied! <pre>client = DandiAPIClient()\ndandisets = list(client.get_dandisets())\n</pre> client = DandiAPIClient() dandisets = list(client.get_dandisets()) In\u00a0[3]: Copied! <pre>nwb_dandisets = []\n\nfor dandiset in tqdm(dandisets):\n    raw_metadata = dandiset.get_raw_metadata()\n\n    if any(\n        data_standard['identifier'] == \"RRID:SCR_015242\"  # this is the RRID for NWB\n        for data_standard in raw_metadata['assetsSummary'].get('dataStandard', [])\n    ):\n        nwb_dandisets.append(dandiset)\nprint(f\"There are currently {len(nwb_dandisets)} NWB datasets on DANDI!\")\n</pre> nwb_dandisets = []  for dandiset in tqdm(dandisets):     raw_metadata = dandiset.get_raw_metadata()      if any(         data_standard['identifier'] == \"RRID:SCR_015242\"  # this is the RRID for NWB         for data_standard in raw_metadata['assetsSummary'].get('dataStandard', [])     ):         nwb_dandisets.append(dandiset) print(f\"There are currently {len(nwb_dandisets)} NWB datasets on DANDI!\") <pre>  0%|          | 0/223 [00:00&lt;?, ?it/s]</pre> <pre>There are currently 128 NWB datasets on DANDI!\n</pre> In\u00a0[4]: Copied! <pre>def iso_to_fractional_days(age_iso: str, experiment_date: str) -&gt; float:\n    \"\"\"\n    Defining a helper function which parses the ISO 8601 age and returns it in float-valued seconds.\n    \n    This is because a dattetime.timedelta can only return either its `.days` (integer, rounded down) or\n    its `total_seconds()`.\n    \n    This helper also resolves some complications that can arise in other datasets when the age is measured in years,\n    or if the the age is a range.\n    \"\"\"\n    if \"/\" in age_iso:  # Some ages can be have upper and lower ranges due to uncertainty\n        return  # Skip\n\n    age_duration = parse_duration(datestring=age_iso)\n\n    if isinstance(age_duration, Duration):\n        experiment_datetime = datetime.fromisoformat(experiment_date)\n        time_delta = age_duration.totimedelta(end=experiment_datetime)\n    else:\n        time_delta = age_duration\n\n    return time_delta.total_seconds() / (  # Evaluate using the total number of seconds\n        60 *  # 60 seconds per minute\n        60 *  # 60 minutes per hour\n        24  # 24 hours per day (ignoring daylight savings time)\n    )\n\n\nall_subject_ages_in_days = []\n\ndandiset = client.get_dandiset(\"000398\")\nassets = list(dandiset.get_assets())\nfor asset in tqdm(assets):\n    raw_metadata = asset.get_raw_metadata()\n    subjects = raw_metadata[\"wasAttributedTo\"]\n\n    for subject_metadata in subjects:\n        if \"age\" in subject_metadata:\n            age_in_days = iso_to_fractional_days(\n                age_iso=subject_metadata[\"age\"][\"value\"],\n                experiment_date=raw_metadata[\"wasGeneratedBy\"][0][\"startDate\"]\n            )\n\n            if age_in_days:  # Skip if the age is null\n                all_subject_ages_in_days.append(age_in_days)\nprint(f\"The average age of the subjects in dandiset #398 is: {np.mean(all_subject_ages_in_days)} days\")\n</pre> def iso_to_fractional_days(age_iso: str, experiment_date: str) -&gt; float:     \"\"\"     Defining a helper function which parses the ISO 8601 age and returns it in float-valued seconds.          This is because a dattetime.timedelta can only return either its `.days` (integer, rounded down) or     its `total_seconds()`.          This helper also resolves some complications that can arise in other datasets when the age is measured in years,     or if the the age is a range.     \"\"\"     if \"/\" in age_iso:  # Some ages can be have upper and lower ranges due to uncertainty         return  # Skip      age_duration = parse_duration(datestring=age_iso)      if isinstance(age_duration, Duration):         experiment_datetime = datetime.fromisoformat(experiment_date)         time_delta = age_duration.totimedelta(end=experiment_datetime)     else:         time_delta = age_duration      return time_delta.total_seconds() / (  # Evaluate using the total number of seconds         60 *  # 60 seconds per minute         60 *  # 60 minutes per hour         24  # 24 hours per day (ignoring daylight savings time)     )   all_subject_ages_in_days = []  dandiset = client.get_dandiset(\"000398\") assets = list(dandiset.get_assets()) for asset in tqdm(assets):     raw_metadata = asset.get_raw_metadata()     subjects = raw_metadata[\"wasAttributedTo\"]      for subject_metadata in subjects:         if \"age\" in subject_metadata:             age_in_days = iso_to_fractional_days(                 age_iso=subject_metadata[\"age\"][\"value\"],                 experiment_date=raw_metadata[\"wasGeneratedBy\"][0][\"startDate\"]             )              if age_in_days:  # Skip if the age is null                 all_subject_ages_in_days.append(age_in_days) print(f\"The average age of the subjects in dandiset #398 is: {np.mean(all_subject_ages_in_days)} days\") <pre>  0%|          | 0/42 [00:00&lt;?, ?it/s]</pre> <pre>The average age of the subjects in dandiset #398 is: 170.74276620370375 days\n</pre> In\u00a0[5]: Copied! <pre>from warnings import simplefilter\nsimplefilter(\"ignore\")  # Suppress namespace warnings from reading older NWB files\n\nfrom nwbinspector.tools import get_s3_urls_and_dandi_paths\nfrom pynwb import NWBHDF5IO\n</pre> from warnings import simplefilter simplefilter(\"ignore\")  # Suppress namespace warnings from reading older NWB files  from nwbinspector.tools import get_s3_urls_and_dandi_paths from pynwb import NWBHDF5IO In\u00a0[6]: Copied! <pre>s3_urls = get_s3_urls_and_dandi_paths(dandiset_id=\"000059\")\n\nnum_units_per_asset = dict()\nfor s3_url in tqdm(s3_urls):\n    io = NWBHDF5IO(path=s3_url, load_namespaces=True, driver=\"ros3\")\n    nwbfile = io.read()\n    \n    if nwbfile.units:\n        num_units_per_asset.update({s3_url: len(nwbfile.units)})\n</pre> s3_urls = get_s3_urls_and_dandi_paths(dandiset_id=\"000059\")  num_units_per_asset = dict() for s3_url in tqdm(s3_urls):     io = NWBHDF5IO(path=s3_url, load_namespaces=True, driver=\"ros3\")     nwbfile = io.read()          if nwbfile.units:         num_units_per_asset.update({s3_url: len(nwbfile.units)}) <pre>  0%|          | 0/54 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre>num_units_per_asset\n</pre> num_units_per_asset Out[7]: <pre>{'https://dandiarchive.s3.amazonaws.com/blobs/a8e/fd7/a8efd760-6dd4-485e-9d7f-6df493ae29e5': 395,\n 'https://dandiarchive.s3.amazonaws.com/blobs/4d1/98e/4d198e63-c6c2-4697-86ae-98637b57b45e': 381,\n 'https://dandiarchive.s3.amazonaws.com/blobs/ad2/bcf/ad2bcf13-79c2-4f05-a002-e08ff6fe3654': 365,\n 'https://dandiarchive.s3.amazonaws.com/blobs/a7e/22f/a7e22f26-0d53-4264-b9ae-dfab2ffa4864': 361,\n 'https://dandiarchive.s3.amazonaws.com/blobs/b0a/94a/b0a94a39-0460-4e84-a053-abe8b9ab12ac': 344,\n 'https://dandiarchive.s3.amazonaws.com/blobs/7ae/56a/7ae56abd-66db-4e61-b75a-8e815580f3ed': 409,\n 'https://dandiarchive.s3.amazonaws.com/blobs/5ce/588/5ce58887-9750-45a9-bfc8-7a8445acb49c': 915,\n 'https://dandiarchive.s3.amazonaws.com/blobs/6d2/1df/6d21df2b-b391-42e6-89ba-5b9113a80c86': 1114,\n 'https://dandiarchive.s3.amazonaws.com/blobs/b17/bfc/b17bfcce-2b08-40e1-bd73-0a603c750066': 468,\n 'https://dandiarchive.s3.amazonaws.com/blobs/034/6b4/0346b4b8-5ade-441c-9088-8fd9f9392d74': 1014,\n 'https://dandiarchive.s3.amazonaws.com/blobs/86a/36b/86a36bf9-dd7a-4e25-a3a5-cf6f4f3b821d': 584,\n 'https://dandiarchive.s3.amazonaws.com/blobs/1a5/403/1a540330-e889-436a-93dd-fdf5befc2905': 928,\n 'https://dandiarchive.s3.amazonaws.com/blobs/df6/633/df663372-04f6-4e6d-94db-7f09218bca4d': 854,\n 'https://dandiarchive.s3.amazonaws.com/blobs/d42/9cd/d429cd65-2630-4ddb-85ff-e1547a47c9b0': 1024,\n 'https://dandiarchive.s3.amazonaws.com/blobs/04f/c85/04fc8532-6df0-42f3-8e93-0260f4bee758': 1730,\n 'https://dandiarchive.s3.amazonaws.com/blobs/261/48c/26148caa-f3e7-4991-ad0b-9b5033f8d9b2': 1827,\n 'https://dandiarchive.s3.amazonaws.com/blobs/e41/7bd/e417bd22-effb-4f2d-8571-e7426f0c9bcd': 1384,\n 'https://dandiarchive.s3.amazonaws.com/blobs/62b/204/62b20403-6377-42d7-909e-5c6346e59572': 855,\n 'https://dandiarchive.s3.amazonaws.com/blobs/343/097/3430974d-387d-414f-b341-a108e2b793cf': 476,\n 'https://dandiarchive.s3.amazonaws.com/blobs/627/99c/62799cfd-e2f9-4f86-8924-b0b5b1c32cb0': 1509,\n 'https://dandiarchive.s3.amazonaws.com/blobs/eeb/bf6/eebbf65f-dc8e-4ebd-b4dc-b5d2b1ec88de': 1191,\n 'https://dandiarchive.s3.amazonaws.com/blobs/7ae/cb7/7aecb7dd-06d5-450e-b5ee-fd756b7c2371': 1183,\n 'https://dandiarchive.s3.amazonaws.com/blobs/f6d/638/f6d6384b-5a46-4a92-a30b-d5740c8aa36a': 1664,\n 'https://dandiarchive.s3.amazonaws.com/blobs/9c0/8a8/9c08a84a-7deb-4732-9338-92d7f1d30b52': 1344}</pre> In\u00a0[8]: Copied! <pre>print(f\"Dandiset #59 has a total of {sum(num_units_per_asset.values())} identified spiking units!\")\n</pre> print(f\"Dandiset #59 has a total of {sum(num_units_per_asset.values())} identified spiking units!\") <pre>Dandiset #59 has a total of 22319 identified spiking units!\n</pre> In\u00a0[10]: Copied! <pre>print(json.dumps(assets[0].get_raw_metadata(), indent=4))\n</pre> print(json.dumps(assets[0].get_raw_metadata(), indent=4)) <pre>{\n    \"id\": \"dandiasset:11c25674-6eff-43a8-8dba-7dea2e8c76c4\",\n    \"path\": \"sub-San4/sub-San4_ses-20200302T142114_ecephys.nwb\",\n    \"access\": [\n        {\n            \"status\": \"dandi:OpenAccess\",\n            \"schemaKey\": \"AccessRequirements\"\n        }\n    ],\n    \"digest\": {\n        \"dandi:sha2-256\": \"b770e3ac3f75f40618de2ba2a81e996429d5fc01dd530e9d826acc7a1ad0853c\",\n        \"dandi:dandi-etag\": \"4c907ae8685aea1bfbe57316942b881f-4\"\n    },\n    \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.3/context.json\",\n    \"approach\": [\n        {\n            \"name\": \"electrophysiological approach\",\n            \"schemaKey\": \"ApproachType\"\n        }\n    ],\n    \"schemaKey\": \"Asset\",\n    \"contentUrl\": [\n        \"https://api.dandiarchive.org/api/assets/11c25674-6eff-43a8-8dba-7dea2e8c76c4/download/\",\n        \"https://dandiarchive.s3.amazonaws.com/blobs/429/baa/429baaad-a057-411d-8957-8460947aef73\"\n    ],\n    \"identifier\": \"11c25674-6eff-43a8-8dba-7dea2e8c76c4\",\n    \"contentSize\": 237267068,\n    \"publishedBy\": {\n        \"id\": \"urn:uuid:f9fc60e7-3126-4912-8222-4de0c7d2cd7a\",\n        \"name\": \"DANDI publish\",\n        \"endDate\": \"2022-12-08T18:03:14.228709+00:00\",\n        \"schemaKey\": \"PublishActivity\",\n        \"startDate\": \"2022-12-08T18:03:14.228709+00:00\",\n        \"wasAssociatedWith\": [\n            {\n                \"id\": \"urn:uuid:2a080a04-80f8-4d25-835b-77aa87eb4b81\",\n                \"name\": \"DANDI API\",\n                \"version\": \"0.1.0\",\n                \"schemaKey\": \"Software\",\n                \"identifier\": \"RRID:SCR_017571\"\n            }\n        ]\n    },\n    \"dateModified\": \"2022-12-02T20:15:00.997018-08:00\",\n    \"datePublished\": \"2022-12-08T18:03:14.228709+00:00\",\n    \"schemaVersion\": \"0.6.3\",\n    \"encodingFormat\": \"application/x-nwb\",\n    \"wasGeneratedBy\": [\n        {\n            \"name\": \"Acquisition session\",\n            \"schemaKey\": \"Session\",\n            \"startDate\": \"2020-03-02T14:21:14-08:00\",\n            \"description\": \"Fig 3i, S10\"\n        },\n        {\n            \"id\": \"urn:uuid:a698c09b-ca57-4cf3-a523-dfbb00a8f524\",\n            \"name\": \"Metadata generation\",\n            \"schemaKey\": \"Activity\",\n            \"description\": \"Metadata generated by DANDI cli\",\n            \"wasAssociatedWith\": [\n                {\n                    \"url\": \"https://github.com/dandi/dandi-cli\",\n                    \"name\": \"DANDI Command Line Interface\",\n                    \"version\": \"0.46.6\",\n                    \"schemaKey\": \"Software\",\n                    \"identifier\": \"RRID:SCR_019009\"\n                }\n            ]\n        }\n    ],\n    \"wasAttributedTo\": [\n        {\n            \"age\": {\n                \"value\": \"P209DT55274S\",\n                \"unitText\": \"ISO-8601 duration\",\n                \"schemaKey\": \"PropertyValue\",\n                \"valueReference\": {\n                    \"value\": \"dandi:BirthReference\",\n                    \"schemaKey\": \"PropertyValue\"\n                }\n            },\n            \"sex\": {\n                \"name\": \"Male\",\n                \"schemaKey\": \"SexType\",\n                \"identifier\": \"http://purl.obolibrary.org/obo/PATO_0000384\"\n            },\n            \"species\": {\n                \"name\": \"Mus musculus - House mouse\",\n                \"schemaKey\": \"SpeciesType\",\n                \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n            },\n            \"genotype\": \"Emx1-Cre[tg/wt];Ai32[tg/wt]\",\n            \"schemaKey\": \"Participant\",\n            \"identifier\": \"San4\"\n        }\n    ],\n    \"blobDateModified\": \"2022-12-02T17:21:54.708718-08:00\",\n    \"variableMeasured\": [\n        {\n            \"value\": \"ElectrodeGroup\",\n            \"schemaKey\": \"PropertyValue\"\n        },\n        {\n            \"value\": \"ElectricalSeries\",\n            \"schemaKey\": \"PropertyValue\"\n        }\n    ],\n    \"measurementTechnique\": [\n        {\n            \"name\": \"multi electrode extracellular electrophysiology recording technique\",\n            \"schemaKey\": \"MeasurementTechniqueType\"\n        },\n        {\n            \"name\": \"surgical technique\",\n            \"schemaKey\": \"MeasurementTechniqueType\"\n        }\n    ]\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/advanced_asset_search/#more-specific-identification-of-nwb-dandisets","title":"More specific identification of NWB dandisets\u00b6","text":"<p>The simpler tutorial only tested if the phrase \"NWB\" was in the name of any of the data standards for a dandiset.</p> <p>The more official and precise method is to use the specific RRID of NWB, which is <code>\"RRID:SCR_015242\"</code>.</p>"},{"location":"tutorials/advanced_asset_search/#average-age-of-subjects-used-in-a-dandiset","title":"Average age of subjects used in a dandiset\u00b6","text":"<p>Let's consider a more advanced calculation - finding the average age of all the subjects used in a particular dandiset.</p> <p>For this we will be directly accessing the asset level fields <code>wasAttributedTo</code> as a key of the <code>asset_metadata</code>, instead of as an attribute.</p> <p>We will also have to do some manual data manipulation to parse the form of the ISO 8601.</p>"},{"location":"tutorials/advanced_asset_search/#count-the-number-of-spiking-units-across-all-sessions-in-an-experiment","title":"Count the number of spiking units across all sessions in an experiment\u00b6","text":"<p>The number of units identified from spike sorting is not something that DANDI extracts automatically during upload...</p> <p>But we can calculate it ourselves without downloading an entire dandiset!</p> <p>We do this by streaming directly from the archive, which requires us to retrieve the asset path on the S3 backend of the DANDI archive and then set the <code>driver</code> argument to <code>ros3</code> (Read-Only S3).</p> <p>There are several ways to retrieve the S3 path, but the easiest is to use the NWB Inspector helper function <code>nwbinspector.tools.get_s3_urls_and_dandi_paths</code>, which will format the path in the way <code>ros3</code> expects.</p>"},{"location":"tutorials/advanced_asset_search/#going-beyond","title":"Going beyond\u00b6","text":"<p>These examples show a few types of queries, but since the metadata structures are quite rich on both the dandiset and asset levels, they enable many complex queries beyond the examples here.</p> <p>These metadata structures are also expanding over time as DANDI becomes more strict about what counts as essential metadata.</p> <p>The <code>.get_raw_metadata</code> method of both <code>client.get_dandiset(...)</code> and <code>client.get_dandiset(...).get_assets()</code> provides a nice view into the available fields.</p> <p>Note: for any attribute, it is recommended to first check that it is not <code>None</code> before checking for its value.</p>"},{"location":"tutorials/analysis-demo/","title":"Streaming and interacting with NWB data","text":"<p>First, let's import a few classes. If you are not running this notebook on DANDI Hub, you will need to install these packages using <code>pip</code> or your favorite Python package manager. For example:</p> <pre><code>pip install dandi pynwb fsspec requests aiohttp matplotlib pynapple seaborn\n</code></pre> In\u00a0[1]: Copied! <pre>from dandi.dandiapi import DandiAPIClient\nimport fsspec\nfrom fsspec.implementations.cached import CachingFileSystem\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pynwb\n</pre> from dandi.dandiapi import DandiAPIClient import fsspec from fsspec.implementations.cached import CachingFileSystem import h5py import matplotlib.pyplot as plt import numpy as np import pynwb <p>The data used in this tutorial were used in this publication: Sargolini, et al. \"Conjunctive representation of position, direction, and velocity in entorhinal cortex.\" Science 312.5774 (2006): 758-762. The data can be found on the DANDI Archive in Dandiset 000582.</p> In\u00a0[2]: Copied! <pre>dandiset_id = \"000582\"\nfilepath = \"sub-10073/sub-10073_ses-17010302_behavior+ecephys.nwb\"\nwith DandiAPIClient() as client:\n    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)\n    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n</pre> dandiset_id = \"000582\" filepath = \"sub-10073/sub-10073_ses-17010302_behavior+ecephys.nwb\" with DandiAPIClient() as client:     asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)     s3_url = asset.get_content_url(follow_redirects=1, strip_query=True) In\u00a0[3]: Copied! <pre>s3_url\n</pre> s3_url Out[3]: <pre>'https://dandiarchive.s3.amazonaws.com/blobs/26a/22c/26a22c31-09bc-43a4-9187-edc7394ed12c'</pre> <p>There are multiple methods to stream NWB files. We currently recommend fsspec, but additional options are described in the pynwb streaming tutorial.</p> In\u00a0[4]: Copied! <pre># create a virtual http filesystem with local caching\nfs = CachingFileSystem(\n    fs=fsspec.filesystem(\"http\"),\n    cache_storage=\"nwb-cache\",  # local folder for the cache\n)\n</pre> # create a virtual http filesystem with local caching fs = CachingFileSystem(     fs=fsspec.filesystem(\"http\"),     cache_storage=\"nwb-cache\",  # local folder for the cache ) In\u00a0[5]: Copied! <pre># open the file using the S3 URL\nf = fs.open(s3_url, \"rb\")\nfile = h5py.File(f)\nio = pynwb.NWBHDF5IO(file=file)\nnwbfile = io.read()\n</pre> # open the file using the S3 URL f = fs.open(s3_url, \"rb\") file = h5py.File(f) io = pynwb.NWBHDF5IO(file=file) nwbfile = io.read() <p>You can print the <code>NWBFile</code> object in a Jupyter notebook to get a simplified, interactive representation of the contents of the NWB file.</p> In\u00a0[6]: Copied! <pre>nwbfile\n</pre> nwbfile Out[6]: root (NWBFile)session_description: This session includes spike and position times for recorded cells from a Long Evans rat that was running in a 1 x 1 meter enclosure. The cells were recorded in the dorsocaudal 25% portion of the medial entorhinal cortex (MEC).Position is given for two LEDs to enable calculation of head direction.identifier: 294b7de1-a624-44d8-b1a1-28028dd2cf0csession_start_time1900-01-01 00:00:00+01:00timestamps_reference_time1900-01-01 00:00:00+01:00file_create_date02023-09-16 15:50:09.775622+02:00experimenter('Sargolini, Francesca',)related_publications('https://doi.org/10.1126/science.1125572',)acquisitionElectricalSeriesstarting_time: 0.0rate: 4800.0resolution: -1.0comments: no commentsdescription: The EEG signals from one electrode amplified 8000-10000 times, lowpass-filtered at 500 Hz (single pole), and stored at 4800 Hz (16 bits/sample).conversion: 1e-06offset: 0.0unit: voltsdatastarting_time_unit: secondselectrodesdescription: all electrodestabledescription: metadata about extracellular electrodestable location group group_name id 0 MEC ElectrodeGroup pynwb.ecephys.ElectrodeGroup at 0x5893589184\\nFields:\\n  description: The name of the ElectrodeGroup this electrode is a part of.\\n  device: EEG pynwb.device.Device at 0x5895127888\\nFields:\\n  description: The device used to record EEG signals.\\n\\n  location: MEC\\n ElectrodeGroup keywordsprocessingbehaviordescription: Processed behavioral data.data_interfacesPositionspatial_seriesSpatialSeriesLED1resolution: -1.0comments: no commentsdescription: Position (x, y) for the first tracking LED.conversion: 0.01offset: 0.0unit: metersdatatimestampstimestamps_unit: secondsinterval: 1reference_frame: (0,0) is not known.ecephysdescription: Processed electrical series data.data_interfacesLFPelectrical_seriesElectricalSeriesLFPstarting_time: 0.0rate: 250.0resolution: -1.0comments: no commentsdescription: The EEG signals from one electrode stored at 250 Hz.conversion: 1e-06offset: 0.0unit: voltsdatastarting_time_unit: secondselectrodesdescription: all electrodestabledescription: metadata about extracellular electrodestable location group group_name id 0 MEC ElectrodeGroup pynwb.ecephys.ElectrodeGroup at 0x5893589184\\nFields:\\n  description: The name of the ElectrodeGroup this electrode is a part of.\\n  device: EEG pynwb.device.Device at 0x5895127888\\nFields:\\n  description: The device used to record EEG signals.\\n\\n  location: MEC\\n ElectrodeGroup epoch_tagsset()electrodesdescription: metadata about extracellular electrodestable location group group_name id 0 MEC ElectrodeGroup pynwb.ecephys.ElectrodeGroup at 0x5893589184\\nFields:\\n  description: The name of the ElectrodeGroup this electrode is a part of.\\n  device: EEG pynwb.device.Device at 0x5895127888\\nFields:\\n  description: The device used to record EEG signals.\\n\\n  location: MEC\\n ElectrodeGroup electrode_groupsElectrodeGroupdescription: The name of the ElectrodeGroup this electrode is a part of.location: MECdevicedescription: The device used to record EEG signals.devicesEEGdescription: The device used to record EEG signals.subjectage: P3M/P5Mage__reference: birthdescription: A Long Evans rat.sex: Mspecies: Rattus norvegicussubject_id: 10073weight: 0.35/0.45unitsdescription: Autogenerated by NWBFilewaveform_unit: voltstable unit_name spike_times histology hemisphere depth id 0 t1c1 [0.7903958333333333, 0.794, 0.8111666666666667, 0.8313541666666666, 0.9217708333333333, 1.0205208333333333, 1.3573020833333334, 1.6583229166666666, 1.6768645833333333, 2.7457708333333333, 4.008697916666667, 4.01678125, 4.402270833333334, 4.522583333333333, 4.527708333333333, 5.598760416666667, 5.61415625, 5.617927083333333, 5.68934375, 5.701510416666666, 5.714885416666666, 5.71740625, 5.723197916666667, 5.806802083333333, 5.8149375, 5.8207708333333334, 6.1641875, 6.201979166666667, 6.2260625, 6.2363125, 6.3546875, 6.363916666666666, 6.480145833333333, 6.48803125, 6.899927083333333, 6.992489583333334, 6.995885416666667, 7.033135416666667, 7.098052083333333, 7.10146875, 7.105677083333333, 7.245541666666667, 11.069739583333334, 11.499979166666666, 11.5111875, 11.522375, 11.6165, 11.702270833333333, 11.714625, 11.7245, 11.920979166666667, 11.983385416666666, 11.98634375, 11.995645833333333, 12.08903125, 12.105385416666667, 12.148697916666666, 12.157114583333334, 12.165864583333333, 13.123729166666667, 13.206645833333333, 13.224875, 13.232479166666666, 13.362927083333334, 13.40890625, 13.522697916666667, 13.530177083333333, 13.54115625, 13.623927083333333, 13.685302083333333, 13.738260416666666, 13.757302083333334, 13.812510416666667, 13.855427083333334, 13.971072916666667, 13.98659375, 13.992822916666666, 14.082979166666666, 14.162229166666666, 14.165541666666666, 14.394229166666667, 14.397375, 14.488708333333333, 14.492708333333333, 14.500604166666667, 14.547833333333333, 14.563979166666666, 14.5875, 14.590729166666666, 14.59375, 14.601708333333333, 14.608729166666667, 14.620083333333334, 14.647083333333333, 14.699260416666666, 14.70190625, 14.741635416666666, 14.801010416666667, 14.80403125, 14.88940625, ...] MEC LII 0.0024 1 t2c1 [1.0451354166666667, 1.7003854166666668, 2.3154375, 11.046822916666667, 14.239729166666667, 14.822927083333333, 14.837010416666667, 19.281322916666667, 19.585395833333333, 19.603958333333335, 19.719083333333334, 19.722625, 19.819458333333333, 19.822979166666666, 19.8256875, 19.829520833333333, 28.67203125, 29.932614583333333, 30.8508125, 30.951416666666667, 31.050583333333332, 31.106822916666665, 31.410760416666665, 36.44560416666667, 46.90858333333333, 47.34158333333333, 47.397375, 47.4190625, 47.47598958333333, 48.156, 59.66520833333333, 59.66970833333333, 59.78719791666666, 60.00934375, 60.12197916666667, 60.12890625, 60.458333333333336, 60.8745625, 61.55807291666667, 62.76680208333333, 62.94321875, 63.27045833333333, 63.4875, 64.47296875, 67.56085416666667, 67.57622916666666, 67.72179166666666, 67.72525, 67.80897916666666, 67.814625, 67.8276875, 67.91458333333334, 67.94144791666666, 67.95988541666667, 68.02155208333333, 68.13059375, 68.20140625, 68.33521875, 68.40710416666667, 68.57409375, 68.58878125, 68.681375, 68.70475, 68.75991666666667, 68.832625, 68.9400625, 68.94379166666667, 71.02867708333333, 71.25821875, 71.41845833333333, 72.28534375, 78.7965, 79.7308125, 79.76083333333334, 79.84297916666667, 79.91891666666666, 79.97095833333333, 80.08979166666667, 83.240875, 88.275625, 91.66017708333334, 91.67665625, 92.069125, 92.10191666666667, 92.56378125, 92.56932291666666, 92.57228125, 92.58971875, 92.59544791666667, 92.67121875, 92.67442708333333, 92.81330208333334, 92.84265625, 93.16694791666667, 93.28475, 93.47997916666667, 93.49264583333333, 93.51422916666667, 93.51847916666667, 93.52220833333334, ...] MEC LII 0.0024 2 t2c3 [0.18273958333333334, 0.5340729166666667, 0.5707291666666666, 0.7023333333333334, 0.7242604166666666, 0.7670520833333333, 0.88084375, 1.06084375, 1.1815, 1.2828333333333333, 1.2922916666666666, 1.4162291666666667, 1.5238333333333334, 1.5430416666666666, 1.5909479166666667, 1.6242291666666666, 1.6451041666666666, 1.7132604166666667, 3.0415729166666665, 5.295635416666666, 5.516947916666667, 6.2452604166666665, 6.336885416666667, 6.6401666666666666, 7.0116875, 7.089395833333334, 7.20128125, 8.1189375, 8.1546875, 8.251333333333333, 8.605614583333333, 10.502302083333333, 10.848864583333333, 10.931302083333334, 11.852802083333334, 11.86840625, 11.877864583333333, 11.963645833333333, 13.12328125, 13.53028125, 13.688145833333333, 13.751333333333333, 14.159802083333334, 16.810760416666668, 16.91994791666667, 17.323583333333332, 17.454822916666668, 19.2395, 19.306625, 19.73028125, 21.200885416666665, 21.439739583333335, 22.102395833333333, 22.926447916666667, 23.722489583333335, 23.85609375, 25.524104166666667, 25.860020833333333, 26.32103125, 26.6210625, 26.66503125, 26.753708333333332, 26.846958333333333, 26.8845, 26.959229166666667, 27.08540625, 27.1913125, 27.271979166666668, 27.31240625, 27.537260416666665, 27.791447916666666, 27.805697916666666, 27.817010416666665, 27.90071875, 27.917572916666668, 28.02446875, 28.13378125, 28.15521875, 28.32228125, 28.363958333333333, 28.38533333333333, 28.608604166666666, 28.6311875, 29.11065625, 30.226875, 33.932625, 34.056666666666665, 34.590354166666664, 36.890760416666666, 38.448458333333335, 38.4585, 38.78067708333333, 39.00878125, 39.022072916666666, 39.23472916666667, 39.24640625, 39.25477083333333, 39.60910416666667, 40.26836458333333, 40.29759375, ...] MEC LII 0.0024 3 t3c1 [1.0358229166666666, 1.04803125, 1.6964270833333333, 1.7780416666666667, 1.7842083333333334, 1.8619166666666667, 1.8659375, 1.879125, 1.8885416666666666, 1.9763333333333333, 2.0486666666666666, 2.1341875, 2.1698541666666666, 2.2325416666666666, 2.245875, 2.251375, 2.2590625, 2.2775833333333333, 2.282395833333333, 2.2996041666666667, 2.3334375, 2.3404375, 2.3472708333333334, 2.3584375, 2.380541666666667, 2.3935104166666665, 2.40184375, 2.4201979166666665, 2.44096875, 2.45184375, 2.4599270833333335, 2.4681770833333334, 2.4746770833333334, 2.4855729166666665, 2.5209479166666666, 2.55503125, 2.5716979166666665, 2.5834895833333333, 2.591552083333333, 2.6127604166666667, 2.6709895833333333, 2.88928125, 3.134916666666667, 3.2006875, 3.3154375, 3.4335625, 4.24340625, 4.2707395833333335, 4.34684375, 14.862104166666667, 15.2978125, 15.543479166666666, 16.012020833333334, 16.19559375, 16.322864583333335, 16.855166666666666, 16.8641875, 19.730333333333334, 19.756604166666666, 19.76535416666667, 19.8773125, 19.9308125, 20.047833333333333, 20.227822916666668, 20.318989583333334, 20.336260416666665, 20.47409375, 20.584364583333333, 20.78321875, 20.79134375, 20.810072916666666, 20.824833333333334, 20.8931875, 20.906208333333332, 20.918354166666667, 20.9286875, 21.020104166666666, 21.109541666666665, 21.1209375, 21.129208333333334, 21.139520833333332, 28.955604166666667, 28.969427083333333, 29.12302083333333, 29.227, 29.93634375, 30.745760416666666, 30.862416666666668, 33.691541666666666, 33.70033333333333, 33.70483333333333, 45.592125, 48.75952083333333, 52.425875, 52.44083333333333, 53.031697916666666, 53.03701041666667, 57.72391666666667, 57.736291666666666, 57.74427083333333, ...] MEC LII 0.0024 <p>... and 4 more rows.</p>experiment_description: The sample includes conjunctive cells and head direction cells from layers III and V of medial entorhinal cortex and have been published in Sargolini et al. (Science, 2006).session_id: 17010302lab: Moserinstitution: Centre for the Biology of Memory, Norwegian University of Science and Technology <p>Access <code>nwbfile.subject</code> to get information about the subject used in this experiment, including their age, sex, species, and ID. Age uses the ISO 8601 standard for time durations - <code>P3M</code> corresponds to 3 months old, and <code>P3M/P5M</code> means the subject was between 3-5 months old.</p> In\u00a0[7]: Copied! <pre>nwbfile.subject\n</pre> nwbfile.subject Out[7]: subject (Subject)age: P3M/P5Mage__reference: birthdescription: A Long Evans rat.sex: Mspecies: Rattus norvegicussubject_id: 10073weight: 0.35/0.45 <p>Now let's access the position of the animal, which is stored in a <code>SpatialSeries</code> object at this particular path within the NWB file.</p> In\u00a0[8]: Copied! <pre>position = nwbfile.processing[\"behavior\"][\"Position\"][\"SpatialSeriesLED1\"]\nposition\n</pre> position = nwbfile.processing[\"behavior\"][\"Position\"][\"SpatialSeriesLED1\"] position Out[8]: SpatialSeriesLED1 (SpatialSeries)resolution: -1.0comments: no commentsdescription: Position (x, y) for the first tracking LED.conversion: 0.01offset: 0.0unit: metersdatatimestampstimestamps_unit: secondsinterval: 1reference_frame: (0,0) is not known. In\u00a0[9]: Copied! <pre>position.data\n</pre> position.data Out[9]: <pre>&lt;HDF5 dataset \"data\": shape (30000, 2), type \"&lt;f8\"&gt;</pre> In\u00a0[10]: Copied! <pre>pos_data = position.data[:]\npos_data\n</pre> pos_data = position.data[:] pos_data Out[10]: <pre>array([[-0.4427946 , 17.10393108],\n       [-0.4427946 , 17.10393108],\n       [ 1.26778258, 16.85956291],\n       ...,\n       [ 2.00088708,  6.84046804],\n       [ 2.00088708,  6.84046804],\n       [ 0.77904624,  4.64115453]])</pre> In\u00a0[11]: Copied! <pre>x = position.data[:,0]\nx\n</pre> x = position.data[:,0] x Out[11]: <pre>array([-0.4427946 , -0.4427946 ,  1.26778258, ...,  2.00088708,\n        2.00088708,  0.77904624])</pre> In\u00a0[12]: Copied! <pre>ts = position.timestamps[:]\nts\n</pre> ts = position.timestamps[:] ts Out[12]: <pre>array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 5.9994e+02, 5.9996e+02,\n       5.9998e+02])</pre> <p>Now let's use those values to plot the X position of the subject over time. All times in NWB are stored in seconds relative to the session start time.</p> In\u00a0[13]: Copied! <pre>plt.plot(ts, x)\nplt.xlabel(\"Time (seconds)\")\nplt.ylabel(\"X coordinate of the subject\");\n</pre> plt.plot(ts, x) plt.xlabel(\"Time (seconds)\") plt.ylabel(\"X coordinate of the subject\"); <p>We can also slice in the time dimension. Let's plot the (x, y) position of the subject for the first 100 time points.</p> In\u00a0[14]: Copied! <pre>time_indices = slice(0, 100)\nfig, ax = plt.subplots()\nax.scatter(position.data[time_indices,0], position.data[time_indices,1], c=position.timestamps[time_indices])\nax.plot(position.data[time_indices,0], position.data[time_indices,1], color='k', zorder=0)\nax.set(xlabel=\"X coordinate\", ylabel=\"Y coordinate\")\n</pre> time_indices = slice(0, 100) fig, ax = plt.subplots() ax.scatter(position.data[time_indices,0], position.data[time_indices,1], c=position.timestamps[time_indices]) ax.plot(position.data[time_indices,0], position.data[time_indices,1], color='k', zorder=0) ax.set(xlabel=\"X coordinate\", ylabel=\"Y coordinate\") Out[14]: <pre>[Text(0.5, 0, 'X coordinate'), Text(0, 0.5, 'Y coordinate')]</pre> In\u00a0[15]: Copied! <pre>nwbfile.units\n</pre> nwbfile.units Out[15]: units (Units)description: Autogenerated by NWBFilewaveform_unit: voltstable unit_name spike_times histology hemisphere depth id 0 t1c1 [0.7903958333333333, 0.794, 0.8111666666666667, 0.8313541666666666, 0.9217708333333333, 1.0205208333333333, 1.3573020833333334, 1.6583229166666666, 1.6768645833333333, 2.7457708333333333, 4.008697916666667, 4.01678125, 4.402270833333334, 4.522583333333333, 4.527708333333333, 5.598760416666667, 5.61415625, 5.617927083333333, 5.68934375, 5.701510416666666, 5.714885416666666, 5.71740625, 5.723197916666667, 5.806802083333333, 5.8149375, 5.8207708333333334, 6.1641875, 6.201979166666667, 6.2260625, 6.2363125, 6.3546875, 6.363916666666666, 6.480145833333333, 6.48803125, 6.899927083333333, 6.992489583333334, 6.995885416666667, 7.033135416666667, 7.098052083333333, 7.10146875, 7.105677083333333, 7.245541666666667, 11.069739583333334, 11.499979166666666, 11.5111875, 11.522375, 11.6165, 11.702270833333333, 11.714625, 11.7245, 11.920979166666667, 11.983385416666666, 11.98634375, 11.995645833333333, 12.08903125, 12.105385416666667, 12.148697916666666, 12.157114583333334, 12.165864583333333, 13.123729166666667, 13.206645833333333, 13.224875, 13.232479166666666, 13.362927083333334, 13.40890625, 13.522697916666667, 13.530177083333333, 13.54115625, 13.623927083333333, 13.685302083333333, 13.738260416666666, 13.757302083333334, 13.812510416666667, 13.855427083333334, 13.971072916666667, 13.98659375, 13.992822916666666, 14.082979166666666, 14.162229166666666, 14.165541666666666, 14.394229166666667, 14.397375, 14.488708333333333, 14.492708333333333, 14.500604166666667, 14.547833333333333, 14.563979166666666, 14.5875, 14.590729166666666, 14.59375, 14.601708333333333, 14.608729166666667, 14.620083333333334, 14.647083333333333, 14.699260416666666, 14.70190625, 14.741635416666666, 14.801010416666667, 14.80403125, 14.88940625, ...] MEC LII 0.0024 1 t2c1 [1.0451354166666667, 1.7003854166666668, 2.3154375, 11.046822916666667, 14.239729166666667, 14.822927083333333, 14.837010416666667, 19.281322916666667, 19.585395833333333, 19.603958333333335, 19.719083333333334, 19.722625, 19.819458333333333, 19.822979166666666, 19.8256875, 19.829520833333333, 28.67203125, 29.932614583333333, 30.8508125, 30.951416666666667, 31.050583333333332, 31.106822916666665, 31.410760416666665, 36.44560416666667, 46.90858333333333, 47.34158333333333, 47.397375, 47.4190625, 47.47598958333333, 48.156, 59.66520833333333, 59.66970833333333, 59.78719791666666, 60.00934375, 60.12197916666667, 60.12890625, 60.458333333333336, 60.8745625, 61.55807291666667, 62.76680208333333, 62.94321875, 63.27045833333333, 63.4875, 64.47296875, 67.56085416666667, 67.57622916666666, 67.72179166666666, 67.72525, 67.80897916666666, 67.814625, 67.8276875, 67.91458333333334, 67.94144791666666, 67.95988541666667, 68.02155208333333, 68.13059375, 68.20140625, 68.33521875, 68.40710416666667, 68.57409375, 68.58878125, 68.681375, 68.70475, 68.75991666666667, 68.832625, 68.9400625, 68.94379166666667, 71.02867708333333, 71.25821875, 71.41845833333333, 72.28534375, 78.7965, 79.7308125, 79.76083333333334, 79.84297916666667, 79.91891666666666, 79.97095833333333, 80.08979166666667, 83.240875, 88.275625, 91.66017708333334, 91.67665625, 92.069125, 92.10191666666667, 92.56378125, 92.56932291666666, 92.57228125, 92.58971875, 92.59544791666667, 92.67121875, 92.67442708333333, 92.81330208333334, 92.84265625, 93.16694791666667, 93.28475, 93.47997916666667, 93.49264583333333, 93.51422916666667, 93.51847916666667, 93.52220833333334, ...] MEC LII 0.0024 2 t2c3 [0.18273958333333334, 0.5340729166666667, 0.5707291666666666, 0.7023333333333334, 0.7242604166666666, 0.7670520833333333, 0.88084375, 1.06084375, 1.1815, 1.2828333333333333, 1.2922916666666666, 1.4162291666666667, 1.5238333333333334, 1.5430416666666666, 1.5909479166666667, 1.6242291666666666, 1.6451041666666666, 1.7132604166666667, 3.0415729166666665, 5.295635416666666, 5.516947916666667, 6.2452604166666665, 6.336885416666667, 6.6401666666666666, 7.0116875, 7.089395833333334, 7.20128125, 8.1189375, 8.1546875, 8.251333333333333, 8.605614583333333, 10.502302083333333, 10.848864583333333, 10.931302083333334, 11.852802083333334, 11.86840625, 11.877864583333333, 11.963645833333333, 13.12328125, 13.53028125, 13.688145833333333, 13.751333333333333, 14.159802083333334, 16.810760416666668, 16.91994791666667, 17.323583333333332, 17.454822916666668, 19.2395, 19.306625, 19.73028125, 21.200885416666665, 21.439739583333335, 22.102395833333333, 22.926447916666667, 23.722489583333335, 23.85609375, 25.524104166666667, 25.860020833333333, 26.32103125, 26.6210625, 26.66503125, 26.753708333333332, 26.846958333333333, 26.8845, 26.959229166666667, 27.08540625, 27.1913125, 27.271979166666668, 27.31240625, 27.537260416666665, 27.791447916666666, 27.805697916666666, 27.817010416666665, 27.90071875, 27.917572916666668, 28.02446875, 28.13378125, 28.15521875, 28.32228125, 28.363958333333333, 28.38533333333333, 28.608604166666666, 28.6311875, 29.11065625, 30.226875, 33.932625, 34.056666666666665, 34.590354166666664, 36.890760416666666, 38.448458333333335, 38.4585, 38.78067708333333, 39.00878125, 39.022072916666666, 39.23472916666667, 39.24640625, 39.25477083333333, 39.60910416666667, 40.26836458333333, 40.29759375, ...] MEC LII 0.0024 3 t3c1 [1.0358229166666666, 1.04803125, 1.6964270833333333, 1.7780416666666667, 1.7842083333333334, 1.8619166666666667, 1.8659375, 1.879125, 1.8885416666666666, 1.9763333333333333, 2.0486666666666666, 2.1341875, 2.1698541666666666, 2.2325416666666666, 2.245875, 2.251375, 2.2590625, 2.2775833333333333, 2.282395833333333, 2.2996041666666667, 2.3334375, 2.3404375, 2.3472708333333334, 2.3584375, 2.380541666666667, 2.3935104166666665, 2.40184375, 2.4201979166666665, 2.44096875, 2.45184375, 2.4599270833333335, 2.4681770833333334, 2.4746770833333334, 2.4855729166666665, 2.5209479166666666, 2.55503125, 2.5716979166666665, 2.5834895833333333, 2.591552083333333, 2.6127604166666667, 2.6709895833333333, 2.88928125, 3.134916666666667, 3.2006875, 3.3154375, 3.4335625, 4.24340625, 4.2707395833333335, 4.34684375, 14.862104166666667, 15.2978125, 15.543479166666666, 16.012020833333334, 16.19559375, 16.322864583333335, 16.855166666666666, 16.8641875, 19.730333333333334, 19.756604166666666, 19.76535416666667, 19.8773125, 19.9308125, 20.047833333333333, 20.227822916666668, 20.318989583333334, 20.336260416666665, 20.47409375, 20.584364583333333, 20.78321875, 20.79134375, 20.810072916666666, 20.824833333333334, 20.8931875, 20.906208333333332, 20.918354166666667, 20.9286875, 21.020104166666666, 21.109541666666665, 21.1209375, 21.129208333333334, 21.139520833333332, 28.955604166666667, 28.969427083333333, 29.12302083333333, 29.227, 29.93634375, 30.745760416666666, 30.862416666666668, 33.691541666666666, 33.70033333333333, 33.70483333333333, 45.592125, 48.75952083333333, 52.425875, 52.44083333333333, 53.031697916666666, 53.03701041666667, 57.72391666666667, 57.736291666666666, 57.74427083333333, ...] MEC LII 0.0024 <p>... and 4 more rows.</p> <p>We can view the single unit data as a pandas <code>DataFrame</code>.</p> In\u00a0[16]: Copied! <pre>units_df = nwbfile.units.to_dataframe()\nunits_df\n</pre> units_df = nwbfile.units.to_dataframe() units_df Out[16]: unit_name spike_times histology hemisphere depth id 0 t1c1 [0.7903958333333333, 0.794, 0.8111666666666667... MEC LII 0.0024 1 t2c1 [1.0451354166666667, 1.7003854166666668, 2.315... MEC LII 0.0024 2 t2c3 [0.18273958333333334, 0.5340729166666667, 0.57... MEC LII 0.0024 3 t3c1 [1.0358229166666666, 1.04803125, 1.69642708333... MEC LII 0.0024 4 t3c2 [2.43025, 2.4398333333333335, 3.17965625, 3.39... MEC LII 0.0024 5 t3c3 [2.1157708333333334, 2.425427083333333, 3.3630... MEC LII 0.0024 6 t3c4 [0.07945833333333334, 2.244947916666667, 3.173... MEC LII 0.0024 7 t4c1 [2.4301666666666666, 2.439770833333333, 3.1795... MEC LII 0.0024 <p>To access the spike times of the first single unit, index this pandas dataframe with the column name, \u201cspike_times\u201d, and the row index, 0. All times in NWB are stored in seconds relative to the session start time.</p> In\u00a0[17]: Copied! <pre>units_df[\"spike_times\"][0]\n</pre> units_df[\"spike_times\"][0] Out[17]: <pre>array([  0.79039583,   0.794     ,   0.81116667, ..., 595.28703125,\n       595.53125   , 599.68578125])</pre> <p>We can use these spike times to generate raster plots of single unit activity over time.</p> In\u00a0[18]: Copied! <pre>fig, ax = plt.subplots()\nfor i, st in enumerate(units_df['spike_times']):\n    ax.plot(st, np.ones_like(st) + i, '|', markersize=20)\n    ax.set(xlabel='Time (s)', ylabel='Unit number')\n</pre> fig, ax = plt.subplots() for i, st in enumerate(units_df['spike_times']):     ax.plot(st, np.ones_like(st) + i, '|', markersize=20)     ax.set(xlabel='Time (s)', ylabel='Unit number') <p>We can also inspect the columns of the <code>Units</code> table for useful metadata such as descriptions of the values of each column.</p> In\u00a0[19]: Copied! <pre>nwbfile.units[\"depth\"].description\n</pre> nwbfile.units[\"depth\"].description Out[19]: <pre>'Indicates the depth of the inserted electrodes in meters.'</pre> <p>Pynapple is a light-weight python library for neurophysiological data analysis that accepts NWB files as input.</p> <p>Let's import pynapple and seaborn, set some seaborn plotting parameters, and load the streamed NWB file into Pynapple by creating a <code>nap.NWBFile</code> object using the <code>pynwb.NWBFile</code> object that we created earlier from <code>io.read()</code>.</p> In\u00a0[20]: Copied! <pre>import pynapple as nap\nimport seaborn as sns\n\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\n\nnwb = nap.NWBFile(nwbfile)\n\nnwb\n</pre> import pynapple as nap import seaborn as sns  custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False} sns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)  nwb = nap.NWBFile(nwbfile)  nwb Out[20]: <pre>17010302\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\n\u2502 Keys                \u2502 Type     \u2502\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\u2502 units               \u2502 TsGroup  \u2502\n\u2502 ElectricalSeriesLFP \u2502 Tsd      \u2502\n\u2502 SpatialSeriesLED1   \u2502 TsdFrame \u2502\n\u2502 ElectricalSeries    \u2502 Tsd      \u2502\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519</pre> <p>We can load the sorted units as a pynapple <code>TsGroup</code> for inspection.</p> In\u00a0[21]: Copied! <pre>units = nwb[\"units\"]\nunits\n</pre> units = nwb[\"units\"] units Out[21]: <pre>  Index    rate  unit_name    histology    hemisphere      depth\n-------  ------  -----------  -----------  ------------  -------\n      0    2.93  t1c1         MEC LII                     0.0024\n      1    1.5   t2c1         MEC LII                     0.0024\n      2    2.58  t2c3         MEC LII                     0.0024\n      3    1.13  t3c1         MEC LII                     0.0024\n      4    1.29  t3c2         MEC LII                     0.0024\n      5    1.36  t3c3         MEC LII                     0.0024\n      6    2.89  t3c4         MEC LII                     0.0024\n      7    1.47  t4c1         MEC LII                     0.0024</pre> <p>We can also load the position data, which is a pynapple <code>TsdFrame</code>.</p> In\u00a0[22]: Copied! <pre>position = nwb[\"SpatialSeriesLED1\"]\nposition\n</pre> position = nwb[\"SpatialSeriesLED1\"] position Out[22]: <pre>Time (s)            x         y\n----------  ---------  --------\n0.0         -0.442795  17.1039\n0.02        -0.442795  17.1039\n0.04         1.26778   16.8596\n0.06         1.26778   16.8596\n0.08        -0.198426  17.1039\n...\n599.9        2.24526    4.88552\n599.92       2.24526    4.88552\n599.94       2.00089    6.84047\n599.96       2.00089    6.84047\n599.98       0.779046   4.64115\ndtype: float64, shape: (30000, 2)</pre> <p>Next, let's compute the 2d tuning curves and plot them.</p> In\u00a0[23]: Copied! <pre>tc, binsxy = nap.compute_2d_tuning_curves(units, position, 20)\n\nextent = (\n    np.min(position[\"x\"]),\n    np.max(position[\"x\"]),\n    np.min(position[\"y\"]),\n    np.max(position[\"y\"]),\n)\n\nplt.figure(figsize=(15, 7))\nfor i in tc.keys():\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(tc[i], origin=\"lower\", extent=extent, aspect=\"auto\")\n    plt.title(\"Unit {}\".format(i))\nplt.tight_layout()\nplt.show()\n</pre> tc, binsxy = nap.compute_2d_tuning_curves(units, position, 20)  extent = (     np.min(position[\"x\"]),     np.max(position[\"x\"]),     np.min(position[\"y\"]),     np.max(position[\"y\"]), )  plt.figure(figsize=(15, 7)) for i in tc.keys():     plt.subplot(2, 4, i + 1)     plt.imshow(tc[i], origin=\"lower\", extent=extent, aspect=\"auto\")     plt.title(\"Unit {}\".format(i)) plt.tight_layout() plt.show() <pre>/Users/smprince/anaconda3/envs/test2/lib/python3.12/site-packages/pynapple/process/tuning_curves.py:223: RuntimeWarning: invalid value encountered in divide\n  count = count / occupancy\n</pre> <p>Finally, let's plot the spikes of unit 1, which has a nice grid. Let's use the function <code>value_from</code> to assign to each spike the closest position in time.</p> In\u00a0[24]: Copied! <pre>plt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.imshow(tc[1], origin=\"lower\", extent=extent, aspect=\"auto\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.subplot(122)\nplt.plot(position[\"y\"], position[\"x\"], color=\"grey\")\nspk_pos = units[1].value_from(position)\nplt.plot(spk_pos[\"y\"], spk_pos[\"x\"], \"o\", color=\"red\", markersize=5, alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(15, 6)) plt.subplot(121) plt.imshow(tc[1], origin=\"lower\", extent=extent, aspect=\"auto\") plt.xlabel(\"x\") plt.ylabel(\"y\")  plt.subplot(122) plt.plot(position[\"y\"], position[\"x\"], color=\"grey\") spk_pos = units[1].value_from(position) plt.plot(spk_pos[\"y\"], spk_pos[\"x\"], \"o\", color=\"red\", markersize=5, alpha=0.5) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.tight_layout() plt.show() <p>It is good practice to close any open file and IO objects when you are done working with them so that they can be modified by other processes.</p> In\u00a0[25]: Copied! <pre>io.close()\nfile.close()\nf.close()\n</pre> io.close() file.close() f.close() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/analysis-demo/#streaming-and-interacting-with-nwb-data-from-dandi","title":"Streaming and interacting with NWB data from DANDI\u00b6","text":""},{"location":"tutorials/analysis-demo/#lazy-loading-of-datasets","title":"Lazy loading of datasets\u00b6","text":"<p>Data arrays are read passively from the NWB file. Accessing the <code>data</code> attribute of the <code>SpatialSeries</code> object does not read the data values, but presents an <code>h5py.Dataset</code> object that can be indexed to read data. You can use the <code>[:]</code> operator to read the entire data array into memory.</p>"},{"location":"tutorials/analysis-demo/#slicing-datasets","title":"Slicing datasets\u00b6","text":"<p>Especially with very large datasets, it is often preferable to read only a portion of the data. To do this, index or slice into the <code>data</code> attribute just like if you were indexing or slicing a numpy array.</p> <p>Let's get the X coordinates of the subject for all timestamps and get the timestamps.</p>"},{"location":"tutorials/analysis-demo/#access-single-unit-data","title":"Access single unit data\u00b6","text":"<p>Data and metadata about sorted single units are stored in a <code>Units</code> object. It stores metadata about each single unit in a tabular form, where each row represents a unit with spike times and additional metadata.</p>"},{"location":"tutorials/analysis-demo/#using-pynapple-for-data-analysis","title":"Using Pynapple for data analysis\u00b6","text":""},{"location":"tutorials/simple_dandiset_search/","title":"Simple Dandiset Search","text":"In\u00a0[1]: Copied! <pre>import json\nfrom dandi.dandiapi import DandiAPIClient\nfrom tqdm.notebook import tqdm\n</pre> import json from dandi.dandiapi import DandiAPIClient from tqdm.notebook import tqdm In\u00a0[2]: Copied! <pre>client = DandiAPIClient()\ndandisets = list(client.get_dandisets())\n</pre> client = DandiAPIClient() dandisets = list(client.get_dandisets()) In\u00a0[3]: Copied! <pre>nwb_dandisets = []\n\nfor dandiset in tqdm(dandisets):\n    raw_metadata = dandiset.get_raw_metadata()\n\n    if any(\n        \"NWB\" in data_standard[\"name\"]\n        for data_standard in raw_metadata[\"assetsSummary\"].get(\"dataStandard\", [])\n    ):\n        nwb_dandisets.append(dandiset)\nprint(f\"There are currently {len(nwb_dandisets)} NWB datasets on DANDI!\")\n</pre> nwb_dandisets = []  for dandiset in tqdm(dandisets):     raw_metadata = dandiset.get_raw_metadata()      if any(         \"NWB\" in data_standard[\"name\"]         for data_standard in raw_metadata[\"assetsSummary\"].get(\"dataStandard\", [])     ):         nwb_dandisets.append(dandiset) print(f\"There are currently {len(nwb_dandisets)} NWB datasets on DANDI!\") <pre>  0%|          | 0/223 [00:00&lt;?, ?it/s]</pre> <pre>There are currently 128 NWB datasets on DANDI!\n</pre> In\u00a0[4]: Copied! <pre>mouse_nwb_dandisets = []\n\nfor dandiset in tqdm(nwb_dandisets):\n    raw_metadata = dandiset.get_raw_metadata()\n\n    if any(\n        \"mouse\" in species[\"name\"]\n        for species in raw_metadata[\"assetsSummary\"].get(\"species\", [])\n    ):\n        mouse_nwb_dandisets.append(dandiset)\nprint(f\"There are currently {len(mouse_nwb_dandisets)} NWB datasets on DANDI that use mice!\")\n</pre> mouse_nwb_dandisets = []  for dandiset in tqdm(nwb_dandisets):     raw_metadata = dandiset.get_raw_metadata()      if any(         \"mouse\" in species[\"name\"]         for species in raw_metadata[\"assetsSummary\"].get(\"species\", [])     ):         mouse_nwb_dandisets.append(dandiset) print(f\"There are currently {len(mouse_nwb_dandisets)} NWB datasets on DANDI that use mice!\") <pre>  0%|          | 0/128 [00:00&lt;?, ?it/s]</pre> <pre>There are currently 61 NWB datasets on DANDI that use mice!\n</pre> In\u00a0[5]: Copied! <pre>dandiset = client.get_dandiset(\"000005\")\nfemale_mouse_nwb_sessions = []\n\nassets = list(dandiset.get_assets())\nfor asset in tqdm(assets):\n    asset_metadata = asset.get_metadata()\n    subjects = asset_metadata.wasAttributedTo\n\n    if any(\n        subject.species and \"mouse\" in subject.species.name.lower()\n        and subject.sex and subject.sex.name == \"Female\"\n        for subject in subjects\n    ):\n        female_mouse_nwb_sessions.append(asset)\nprint(f\"Dandiset #5 has {len(female_mouse_nwb_sessions)} out of {len(assets)} files that use female mice!\")\n</pre> dandiset = client.get_dandiset(\"000005\") female_mouse_nwb_sessions = []  assets = list(dandiset.get_assets()) for asset in tqdm(assets):     asset_metadata = asset.get_metadata()     subjects = asset_metadata.wasAttributedTo      if any(         subject.species and \"mouse\" in subject.species.name.lower()         and subject.sex and subject.sex.name == \"Female\"         for subject in subjects     ):         female_mouse_nwb_sessions.append(asset) print(f\"Dandiset #5 has {len(female_mouse_nwb_sessions)} out of {len(assets)} files that use female mice!\") <pre>  0%|          | 0/148 [00:00&lt;?, ?it/s]</pre> <pre>Dandiset #5 has 69 out of 148 files that use female mice!\n</pre> In\u00a0[6]: Copied! <pre>print(json.dumps(dandisets[0].get_raw_metadata(), indent=4))\n</pre> print(json.dumps(dandisets[0].get_raw_metadata(), indent=4)) <pre>{\n    \"id\": \"DANDI:000003/0.210812.1448\",\n    \"doi\": \"10.48324/dandi.000003/0.210812.1448\",\n    \"url\": \"https://dandiarchive.org/dandiset/000003/0.210812.1448\",\n    \"name\": \"Physiological Properties and Behavioral Correlates of Hippocampal Granule Cells and Mossy Cells\",\n    \"about\": [\n        {\n            \"name\": \"hippocampus\",\n            \"schemaKey\": \"Anatomy\",\n            \"identifier\": \"UBERON:0002421\"\n        }\n    ],\n    \"access\": [\n        {\n            \"status\": \"dandi:OpenAccess\",\n            \"schemaKey\": \"AccessRequirements\",\n            \"contactPoint\": {\n                \"email\": \"petersen.peter@gmail.com\",\n                \"schemaKey\": \"ContactPoint\"\n            }\n        }\n    ],\n    \"license\": [\n        \"spdx:CC-BY-4.0\"\n    ],\n    \"version\": \"0.210812.1448\",\n    \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.4.4/context.json\",\n    \"citation\": \"Senzai, Yuta; Fernandez-Ruiz, Antonio; Buzs\\u00e1ki, Gy\\u00f6rgy (2021) Physiological Properties and Behavioral Correlates of Hippocampal Granule Cells and Mossy Cells (Version 0.210812.1448) [Data set]. DANDI archive. https://doi.org/10.48324/dandi.000003/0.210812.1448\",\n    \"keywords\": [\n        \"cell types\",\n        \"current source density\",\n        \"laminar recordings\",\n        \"oscillations\",\n        \"mossy cells\",\n        \"granule cells\",\n        \"optogenetics\"\n    ],\n    \"protocol\": [],\n    \"schemaKey\": \"Dandiset\",\n    \"identifier\": \"DANDI:000003\",\n    \"repository\": \"https://dandiarchive.org/\",\n    \"contributor\": [\n        {\n            \"name\": \"Senzai, Yuta\",\n            \"roleName\": [\n                \"dcite:Author\",\n                \"dcite:ContactPerson\",\n                \"dcite:DataCollector\",\n                \"dcite:FormalAnalysis\"\n            ],\n            \"schemaKey\": \"Person\",\n            \"affiliation\": [],\n            \"includeInCitation\": true\n        },\n        {\n            \"name\": \"Fernandez-Ruiz, Antonio\",\n            \"roleName\": [\n                \"dcite:Author\",\n                \"dcite:FormalAnalysis\"\n            ],\n            \"schemaKey\": \"Person\",\n            \"identifier\": \"0000-0001-8481-0796\",\n            \"affiliation\": [],\n            \"includeInCitation\": true\n        },\n        {\n            \"name\": \"Buzs\\u00e1ki, Gy\\u00f6rgy\",\n            \"roleName\": [\n                \"dcite:Author\"\n            ],\n            \"schemaKey\": \"Person\",\n            \"identifier\": \"0000-0002-3100-4800\",\n            \"affiliation\": [\n                {\n                    \"name\": \"New York University Langone Medical Center\",\n                    \"schemaKey\": \"Affiliation\",\n                    \"identifier\": \"https://ror.org/005dvqh91\"\n                }\n            ],\n            \"includeInCitation\": true\n        }\n    ],\n    \"description\": \"Data from \\\"Physiological Properties and Behavioral Correlates of Hippocampal Granule Cells and Mossy Cells\\\" Senzai, Buzsaki, Neuron 2017. Electrophysiology recordings of hippocampus during theta maze exploration.\",\n    \"publishedBy\": {\n        \"id\": \"urn:uuid:6dfd1ae5-c5ea-443a-ad11-d4ac423261af\",\n        \"name\": \"DANDI publish\",\n        \"endDate\": \"2021-08-12T14:48:43.199600\",\n        \"schemaKey\": \"PublishActivity\",\n        \"startDate\": \"2021-08-12T14:48:43.199600\",\n        \"wasAssociatedWith\": [\n            {\n                \"id\": \"urn:uuid:53ec67cd-4e87-42f9-9c2c-962b46392963\",\n                \"name\": \"DANDI API\",\n                \"version\": \"0.1.0\",\n                \"schemaKey\": \"Software\",\n                \"identifier\": \"RRID:SCR_017571\"\n            }\n        ]\n    },\n    \"studyTarget\": [],\n    \"assetsSummary\": {\n        \"species\": [\n            {\n                \"name\": \"House mouse\",\n                \"schemaKey\": \"SpeciesType\",\n                \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n            }\n        ],\n        \"approach\": [\n            {\n                \"name\": \"electrophysiological approach\",\n                \"schemaKey\": \"ApproachType\"\n            },\n            {\n                \"name\": \"behavioral approach\",\n                \"schemaKey\": \"ApproachType\"\n            }\n        ],\n        \"schemaKey\": \"AssetsSummary\",\n        \"dataStandard\": [\n            {\n                \"name\": \"Neurodata Without Borders (NWB)\",\n                \"schemaKey\": \"StandardsType\",\n                \"identifier\": \"RRID:SCR_015242\"\n            }\n        ],\n        \"numberOfBytes\": 2559248010229,\n        \"numberOfFiles\": 101,\n        \"numberOfSubjects\": 16,\n        \"variableMeasured\": [\n            \"DecompositionSeries\",\n            \"LFP\",\n            \"Units\",\n            \"Position\",\n            \"ElectricalSeries\"\n        ],\n        \"measurementTechnique\": [\n            {\n                \"name\": \"signal filtering technique\",\n                \"schemaKey\": \"MeasurementTechniqueType\"\n            },\n            {\n                \"name\": \"fourier analysis technique\",\n                \"schemaKey\": \"MeasurementTechniqueType\"\n            },\n            {\n                \"name\": \"spike sorting technique\",\n                \"schemaKey\": \"MeasurementTechniqueType\"\n            },\n            {\n                \"name\": \"behavioral technique\",\n                \"schemaKey\": \"MeasurementTechniqueType\"\n            },\n            {\n                \"name\": \"multi electrode extracellular electrophysiology recording technique\",\n                \"schemaKey\": \"MeasurementTechniqueType\"\n            }\n        ]\n    },\n    \"datePublished\": \"2021-08-12T14:48:43.199600\",\n    \"schemaVersion\": \"0.4.4\",\n    \"ethicsApproval\": [],\n    \"wasGeneratedBy\": [],\n    \"relatedResource\": [\n        {\n            \"url\": \"https://doi.org/10.1016/j.neuron.2016.12.011\",\n            \"relation\": \"dcite:IsDescribedBy\",\n            \"schemaKey\": \"Resource\",\n            \"identifier\": \"doi:10.1016/j.neuron.2016.12.011\"\n        }\n    ],\n    \"manifestLocation\": [\n        \"https://dandiarchive.s3.amazonaws.com/dandisets/000003/0.210812.1448/assets.yaml\"\n    ]\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/simple_dandiset_search/#identify-nwb-dandisets","title":"Identify NWB dandisets\u00b6","text":"<p>Most dandisets hold NWB-formatted data, but DANDI also hold data of other formats.</p> <p>Let's start by filtering down to only the dandisets that contain at least one NWB file.</p> <p>We can do this by querying the metadata of each dandiset, which tells us the data formats within in <code>raw_metadata[\"assetsSummary\"][\"dataStandard\"]</code>.</p> <p>If no data has been uploaded to that dandiset, the \"dataStandard\" field is not present.</p> <p>We handle this by using the <code>.get</code> method to iterate over an empty list.</p>"},{"location":"tutorials/simple_dandiset_search/#filtering-dandisets-species","title":"Filtering dandisets: species\u00b6","text":"<p>Let's use the <code>nwb_dandisets</code> list from the previous recipe and see which of them used mice in their study.</p> <p>You can find this information in <code>raw_metadata[\"assetsSummary\"][\"species\"]</code>.</p> <p>We'll use the same <code>.get</code> trick as above for if no data has been uploaded.</p>"},{"location":"tutorials/simple_dandiset_search/#filtering-by-session-species-and-sex","title":"Filtering by session: species and sex\u00b6","text":"<p>Let's say you have identified a dandiset of interest, \"000005\", and you want to identify all of the sessions on female mice.</p> <p>You can do this by querying asset-level metadata.</p> <p>Assets correspond to individual NWB files, and contain metadata extracted from those files.</p> <p>The metadata of each asset contains a <code>.wasAttributedTo</code> attribute, which is a list of <code>Participant</code> objects corresponding to the subjects for that session.</p> <p>We do that by first testing that attribute exists (is not <code>None</code> - some older dandisets may not have included it) and then checking the value of its <code>name</code> parameter.</p>"},{"location":"tutorials/simple_dandiset_search/#going-beyond","title":"Going beyond\u00b6","text":"<p>These examples show a few types of queries, but since the metadata structures are quite rich on both the dandiset and asset levels, they enable many complex queries beyond the examples here.</p> <p>These metadata structures are also expanding over time as DANDI becomes more strict about what counts as essential metadata.</p> <p>The <code>.get_raw_metadata</code> method of both <code>client.get_dandiset(...)</code> and <code>client.get_dandiset(...).get_assets()</code> provides a nice view into the available fields.</p> <p>Note: for any attribute, it is recommended to first check that it is not <code>None</code> before checking for its value.</p>"}]}